{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS-467 Machine Learning for Physisicst\n",
    "### Assignment 3: Predicting Atomization Energies with Neural Nets\n",
    "\n",
    "\n",
    "#### Guilherme Simplício"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All important imports for the whole code\n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "The dataset is stored as a dictionary in the file dataset.pt and is already divided into train,\n",
    "validation, and test sets, with keys X train, y train, X val, y val, and X test, y test respectively.\n",
    "The tensors X’s contain the upper triangular parts of the normalized Coulomb matrices, and the\n",
    "tensors y’s the atomization energies of the molecules in units of kcal/mol. Open the dataset using\n",
    "torch.load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train': tensor([[ 5.1027,  1.4530,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  1.7468,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  3.7440,  3.4268,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         ...,\n",
       "         [ 7.1501,  1.2805,  7.1501,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  3.1458,  3.4268,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  1.3803,  5.1027,  ..., -0.3168, -0.3168, -0.3168]]),\n",
       " 'y_train': tensor([-1655.6700, -1332.6200, -1717.9399,  ..., -1239.8700, -1713.2700,\n",
       "         -1447.5900]),\n",
       " 'X_val': tensor([[ 7.1501,  1.3131,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  2.1739,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  1.1284,  7.1501,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         ...,\n",
       "         [ 7.1501,  1.5263,  7.1501,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 5.1027,  2.6138,  3.4268,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  1.5531,  5.1027,  ..., -0.3168, -0.3168, -0.3168]]),\n",
       " 'y_val': tensor([-1717.3400, -1593.6300, -1241.2500, -1863.5900, -1536.0800, -1483.8199,\n",
       "         -1097.3600, -1699.4900, -1349.7200, -1929.6000, -1644.2200, -1461.7000,\n",
       "         -1446.8199, -1185.6200, -1541.6700, -2043.0000, -1787.1100, -1438.1100,\n",
       "         -1251.7100, -2042.3800, -1328.1200, -1600.7700, -1539.8700, -1850.4000,\n",
       "         -1445.5900, -1576.9800, -2042.7000, -1618.6500, -1459.5200, -1781.4301,\n",
       "         -1104.6801, -1231.8900, -1550.7900, -1448.7000, -1549.2400, -1712.0400,\n",
       "         -1578.7300, -1719.1700, -1710.1600, -1710.2100, -1615.6801, -1540.1500,\n",
       "         -1575.5500, -1491.1000, -1763.7300, -1689.0000, -1567.6700, -1413.0200,\n",
       "         -1785.5500, -1501.3700, -1540.6400, -1364.0100, -1018.4200, -1626.1000,\n",
       "         -1322.4301, -1601.1000, -1277.1500, -1329.9700, -1756.3800, -1268.3900,\n",
       "         -1498.1801, -1644.4900, -1567.1400, -2044.2000, -1461.2700, -1659.4600,\n",
       "         -1457.7000, -1424.1899, -1842.1400, -1633.7500, -1219.6000, -1453.4900,\n",
       "         -1305.7700, -1641.9700, -1354.3900, -1560.5699, -1113.7400, -1428.9200,\n",
       "         -1648.8500, -1928.1000, -1918.2100, -1677.7400, -1647.6400, -1531.0699,\n",
       "         -1610.3800, -2060.5400, -1564.6600, -1550.3900, -1360.7900, -1159.6200,\n",
       "         -1911.5900, -1782.4500, -1544.0300, -1545.1400, -1523.8500, -1357.3101,\n",
       "         -1728.1400, -1511.0000, -2054.1201, -1410.8800]),\n",
       " 'X_test': tensor([[ 5.1027,  1.8598,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 5.1027,  1.7783,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 7.1501,  0.7623,  5.1027,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         ...,\n",
       "         [39.0940,  8.7827,  7.1501,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 3.4268,  2.1162,  3.4268,  ..., -0.3168, -0.3168, -0.3168],\n",
       "         [ 5.1027,  1.7734,  5.1027,  ..., -0.3168, -0.3168, -0.3168]]),\n",
       " 'y_test': tensor([-1666.5300, -1501.6899, -1309.7500,  ..., -1236.0500, -1750.3500,\n",
       "         -1359.6100])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check if the data has been correctly stored, one can simply run this cell:\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "To inspect the labels y’s, make a histogram of the atomization energies of the molecules contained\n",
    "in the training set. Then normalize the train, validation, and test labels by subtracting the mean and\n",
    "dividing by the standard deviation of the train labels. This procedure helps to speed up the convergence\n",
    "of the optimization dynamics and to avoid numerical instabilities. Keep track of the mean and the\n",
    "standard deviation that you used to normalize in order to be able to unnormalize the energies once\n",
    "the training is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of y_train\n",
    "\n",
    "plt.hist(dataset[\"y_train\"],facecolor = \"g\")\n",
    "\n",
    "plt.xlabel('Atomization energies kcal/mol')\n",
    "#plt.ylabel('')\n",
    "plt.title('Histogram of the atomization energies of the molecules contained in the training set')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the train, validation, and test labels (subtract the mean and divide by the standard deviation of the train labels)\n",
    "# Keep track of the mean and the standard deviation that you used to normalize\n",
    "\n",
    "mean_y_train = torch.mean(dataset[\"y_train\"])\n",
    "std_y_train = torch.std(dataset[\"y_train\"])\n",
    "\n",
    "N_y_train = (dataset[\"y_train\"] - mean_y_train) / std_y_train\n",
    "N_y_val   = (dataset[\"y_val\"] - mean_y_train) / std_y_train  \n",
    "N_y_test  = (dataset[\"y_test\"] - mean_y_train) / std_y_train\n",
    "\n",
    "# to check results one simple histogram of N_y_{train,val,test} can be done, as showed in the cell above\n",
    "# this was omitted in order to keep the notebook clean and concise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Use torch.utils.data.TensorDataset to generate the train, validation, and test PyTorch\n",
    "datasets. Then, build the train, validation, and test Pytorch dataloaders, setting the batch size to 100\n",
    "and activating reshuffling at each epoch for the train data by setting shuffle=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch datasets:\n",
    "train_dataset = torch.utils.data.TensorDataset(dataset[\"X_train\"], N_y_train)\n",
    "val_dataset   = torch.utils.data.TensorDataset(dataset[\"X_val\"], N_y_val)\n",
    "test_dataset  = torch.utils.data.TensorDataset(dataset[\"X_test\"], N_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch dataloaders:\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= 100, shuffle= True)\n",
    "val_dataloader   = torch.utils.data.DataLoader(val_dataset, batch_size= 100, shuffle= True) \n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset, batch_size= 100, shuffle= True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Define a nn.Module class for a scalar valued one-hidden-layer fully-connected network fNN (x)\n",
    "with the appropriate input dimension d, p hidden neurons, and ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports for neural networks\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"Defines a fully connected network with the appropriate input dimension d, p hidden neurons, and ReLU activation function.\"\n",
    "     \n",
    "    def __init__(self,d,p):\n",
    "        \n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(d, p)\n",
    "        self.fc2 = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Train the neural network setting p = 1000 for 1000 epochs, using the MSE loss (nn.MSELoss)\n",
    "and the Adam optimizer (torch.optim.Adam). Choose from the set {10−5 , 10−4 , 10−3 , 10−2 } the\n",
    "learning rate that results in the best generalization performance on the validation set. Averaging over\n",
    "2-3 different random initializations of the network leads to a better picture and avoids choices based\n",
    "on a poor random initialization, but it is not mandatory. All the following requests refer to the model\n",
    "trained with the chosen learning rate. First, plot and analyze the behavior of the train and validation\n",
    "losses during training . Finally, evaluate the performance of this model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, epochs, learning_rate, datatype):\n",
    "  \"\"\"\n",
    "  This function trains a neural network with SGD and its variants.\n",
    "  Input: train_dataloader = iterable over the training set;\n",
    "         model = neural network architecture with weights and biases;\n",
    "         learning_rate = size of gradient updates;\n",
    "  Output: trained neural network model and metrics stored in *.csv file. \n",
    "  \"\"\"\n",
    "  l = nn.MSELoss()\n",
    "  o = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # extract the size of the training set\n",
    "  size = len(train_dataloader.dataset)\n",
    "\n",
    "  # initialize an empty dictionary\n",
    "  res = {'epoch': [], datatype +'_loss': []}\n",
    "\n",
    "  # loop over epochs\n",
    "  for e in range(epochs):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "\n",
    "    # initialize the variable for train loss  \n",
    "    running_loss = 0\n",
    "    \n",
    "    # loop over all batches and over all input-output pair within a batch\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "\n",
    "        y = torch.reshape(y,(y.shape[0],1)) \n",
    "      \n",
    "        o.zero_grad() # setting gradient to zeros\n",
    "        pred = model(X) # get predictions through forward pass\n",
    "        loss = l(pred, y) # compute the loss\n",
    "\n",
    "        loss.backward() # backward propagation        \n",
    "        \n",
    "        o.step() # update the gradient to new gradients\n",
    "\n",
    "    # compute the metrics of interest\n",
    "        running_loss += loss.item()\n",
    "          \n",
    "        # check how the training is evolving accross the batches\n",
    "        if batch % 10 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            print(f\"Processing batch n. {batch+1} ----> running loss: {running_loss/(batch + 1):>7f}[{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    running_loss = running_loss/(batch + 1)\n",
    "    print(\"\")\n",
    "    print(f\"running {datatype} loss =  \", running_loss)\n",
    "    print(\"\")\n",
    "\n",
    "    res['epoch'].append(e) # populate the dictionary of results\n",
    "    res[datatype +'_loss'].append(running_loss)\n",
    "\n",
    "  print(\"Done!\")\n",
    "\n",
    "  res = pd.DataFrame.from_dict(res) # translate the dictionary into a pandas dataframe\n",
    "  res.to_csv(\"./metrics_over_epochs.csv\", mode = 'w', index = False) # store the results into a *.csv file\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss as a function of the number of epochs\n",
    "def plot_loss(learning_rate,data_type):\n",
    "    fig, ax = plt.subplots(figsize=(10,7)) # initialize an empty figure and axes object\n",
    "    res = pd.read_csv(\"./metrics_over_epochs.csv\") # read the content of the *.csv file\n",
    "\n",
    "    res.plot(x = 'epoch',\n",
    "             y = data_type+ '_loss',\n",
    "             color = 'blue',\n",
    "             lw = 3, # linewidth\n",
    "             ax=ax) # plot the results with matplotlib through pandas\n",
    "\n",
    "    ax.set_title(f\"{data_type} loss function versus number of epochs for learning rate: {learning_rate}\") # set the plot title\n",
    "    ax.set_ylabel(f\"{data_type} loss\") # set the name of the y axis\n",
    "    ax.set_xlabel(\"epoch\") # set the name of the x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1.037600[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 1.045165[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 1.058913[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 1.022914[ 3100/ 4000]\n",
      "\n",
      "running train loss =   1.0202871471643449\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1.096200[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.991659[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.958999[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.937147[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.9210105299949646\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.795580[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.842788[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.853489[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.845388[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.8387090370059014\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.831730[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.811411[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.813674[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.775337[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.7673425033688546\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.711947[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.676326[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.676310[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.706700[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.7039611294865609\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.774544[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.646598[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.637704[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.646201[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.6472139805555344\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.631391[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.587995[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.602730[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.595018[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.5958261221647263\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.614124[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.545274[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.555838[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.559250[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.5485728904604912\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.535982[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.508778[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.512193[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.511434[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.5052018694579601\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.336101[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.441221[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.449679[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.453935[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.4650216154754162\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.362864[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.457643[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.453883[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.437415[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.42777420580387115\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.359492[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.406339[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.389409[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.378447[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.39313687533140185\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.336856[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.378711[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.368173[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.359527[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.36101220920681953\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.420257[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.341054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.331587[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.334980[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.3311319831758738\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.280954[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.320345[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.305085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.298501[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.3034575507044792\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.319807[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.287660[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.288440[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.284383[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.27780770920217035\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.358482[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.269984[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.248515[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.252926[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.25380156449973584\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.247366[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.246064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.229156[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.233530[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.23136678636074065\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.182994[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.221067[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.222583[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.213049[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.21136071644723414\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.180271[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.190474[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.199182[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.198236[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.19369056522846223\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.187536[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.178668[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.182998[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.180329[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.1770264832302928\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.204114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.158622[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.168256[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.169516[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.162443757802248\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.146348[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.147688[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.144064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.143773[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.14935624301433564\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.247302[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.154140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.143867[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.140934[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.137854609079659\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.119931[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.117932[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.120952[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.124892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.12732272464782\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.138994[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.118787[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.122129[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.119332[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.1179084038361907\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.131361[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.107111[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.107505[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.105428[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.10978415235877037\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.123788[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.114529[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.103700[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.102766[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.10256165619939565\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.120376[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.101024[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.101982[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.098091[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.09631785620003938\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.067363[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.094593[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.094536[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.093713[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.09073833506554366\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.076271[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.081611[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.076452[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.085771[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.08578026294708252\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.095367[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.085219[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.084346[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.081385[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.08144340934231878\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.071017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.073741[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.080211[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.075513[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.07760891085490584\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.128063[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.073510[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.074964[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.072645[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.07430279506370426\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.064783[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.070730[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.074978[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.073852[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.07138778576627373\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.057100[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.070100[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.067780[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.067473[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.06876604342833162\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.093160[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.069767[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.066180[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.064684[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.06647379640489817\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.058947[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.061062[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.063684[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.066547[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0648719236254692\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.064570[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.068858[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.063116[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.063312[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.06250524781644344\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.067947[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.056590[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.060886[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.062218[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.06075440319254995\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.074873[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.063251[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.060861[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.059144[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.059200594667345285\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.061442[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.060464[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.059245[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.059487[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.057688044104725125\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.054651[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.060175[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.057364[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.056094[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.056304367166012524\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.054317[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.049982[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.057195[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.057425[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.05495215468108654\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.054867[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.052183[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.048942[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.051075[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.05376486200839281\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.044020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.058019[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.054426[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.055509[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.05255904607474804\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.054707[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.046630[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.051136[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.052979[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.051490486692637207\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.053315[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.049396[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.048835[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.051285[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.050483134016394614\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.043969[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.057214[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.050887[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.051375[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04932088232599199\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.050003[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.061955[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.052597[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.050292[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0484039360191673\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034209[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.038047[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.046224[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.049052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04750320776365698\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.044075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.048484[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.044809[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.044753[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04653688929975033\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034887[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.045161[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.043903[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.045053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04576553856022656\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.050890[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.043055[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.041551[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.043145[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0449142943136394\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.032738[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.043238[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.042927[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.043711[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04414134626276791\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.056533[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.041099[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.043219[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.043177[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04341742666438222\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.039826[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.039915[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.040433[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.040433[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04271547915413976\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.054488[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.042047[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.041879[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.040760[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.042031475622206924\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.056229[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.043133[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.039603[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.042161[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.041652772761881354\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.044746[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.041309[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.043502[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.042062[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04073813995346427\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.035228[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.041462[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.038931[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.039088[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.040275686280801894\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.032441[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.040716[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.041285[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.039777[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03945481115952134\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.030075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.035568[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.039388[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.039309[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03889380246400833\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.045317[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.037970[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.037518[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.036116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.038360698195174334\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.037589[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.041760[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.038696[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.038536[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03783032409846783\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.028567[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033597[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033347[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.034388[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03734181132167578\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.039146[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.034043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.034451[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.035898[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.036790674878284337\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.027617[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.034444[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.034857[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.034119[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03632374238222837\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.033383[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.036572[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.035517[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.035499[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03595291208475828\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026939[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.032898[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.032467[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.031892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03541481546126306\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.028499[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031464[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.032790[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.035284[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03494942579418421\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022903[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.038006[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.035963[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.035316[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.034608533373102546\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025527[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.034789[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.034903[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.034061[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.034208421036601065\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.027516[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.029642[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.032054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.032399[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03361434666439891\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.041810[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031456[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.035013[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.035170[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03341754041612148\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033720[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.031253[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.031728[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0330161205958575\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.039705[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.032141[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033930[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.033208[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03256347314454615\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026984[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033747[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.031588[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.033367[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03219362595118582\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.024581[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033401[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030210[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.029565[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0318536723498255\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023893[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030147[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.032409[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03153647445142269\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021671[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030730[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030844[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.032495[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.031200267514213918\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.039404[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030716[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029240[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030491[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030866455053910613\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.027947[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033299[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033712[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.032274[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030681359954178332\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022289[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026449[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030849[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.029944[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030459510860964656\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026180[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031455[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030447[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030078[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030006776051595806\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.037740[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.028211[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029696[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.029086[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.029733321350067853\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025241[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026762[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029999[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.029290[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.029509206116199494\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.030265[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031098[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029282[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030193[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02920521516352892\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.028266[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027950[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027304[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.028807614278048278\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025456[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026365[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.027863[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027280[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02864485690370202\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.037239[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026237[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.026363[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027349[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.028390901768580078\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.028542[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031587[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029598[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02824115054681897\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.036452[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027347[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027762[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.027932243468239904\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020590[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.025792[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.026638[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026575[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02768854391761124\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.036081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.032271[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030283[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.028392[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02737505049444735\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.032332[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.024921[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024607[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.025802[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.027298903604969383\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025341[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031519[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.029268[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027953[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.027105121966451408\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.029705[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026496[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024173[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026117[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.026813717419281603\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034144[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030359[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.027054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.027050[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.026606369856745006\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022473[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.024877[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025815[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.025585[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.026325790816918014\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020864[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.034240[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033095[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.031192[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.029671682370826603\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022609[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.025418[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.023344[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.025077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02520983456633985\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020520[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027105[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025953[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.025010[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.024847565707750617\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020885[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.025543[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025104[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023988[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023069138498976828\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022324[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020433[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.022786[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.022278[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.022200294001959265\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.019262[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024305[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.022450[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02222757861018181\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.024633[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021206[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020686[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.020820[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02062711005564779\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013194[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018900[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019789[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019739[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.019509191042743623\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022501[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018716[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019405[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019153[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01892187858466059\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013482[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.017061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017040[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019194[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018942010379396378\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017537[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020176[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018411[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018054[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017897807504050435\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022503[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019352[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018279[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017561440588906407\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011697[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015288[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016150[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015690[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.016210641036741434\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016139[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015858[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015736[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015460[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015618776762858033\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015364[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013152[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013357[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014213[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015019861282780766\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016873[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012737[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013519[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013879[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015005808998830616\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025208[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014487[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014514[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014126[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014066918264143169\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010471[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013346[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013019[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012857[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0134099745657295\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013296[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013548[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012761[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013314[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013110399711877108\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012591[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013997[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013079[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012727[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012431736011058092\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012338[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011202[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011377[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012389[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012237187451682984\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011734[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011836[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011721[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011818425660021604\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008805[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010451[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011278[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011360[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011595175578258931\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009134[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010374[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010605[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010711[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011004157818388195\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011920[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011482[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010639[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010585[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010721372067928314\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006319[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011131[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010823[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010400[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010477903112769128\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010174[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009829[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009334[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009933[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009824472805485129\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009563[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010002[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009690[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009595[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00947573664598167\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012280[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008933[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009847[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009645[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009252524015028029\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006489[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008324[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009327[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009220199845731258\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009941[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008579[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008801[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008765[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008829673496074974\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008368[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009604[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009094[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008682[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008697928942274303\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009585[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009182[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008436[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008738[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008791670377831906\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014548[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008851[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008333[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008138[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00802956452826038\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008791[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007724[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007756[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008007[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007937346072867512\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010717[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008248[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008126[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007842[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007721158105414361\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007712[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007714[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007390[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007421[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007562513544689864\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006135[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007956[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007373[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007493[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007387631083838641\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005857[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006838[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007128[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00718255516840145\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006198[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007315[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006975[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006675[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007083812402561307\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006969[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007699[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008484[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007700119388755411\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005777[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006847[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006589[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006460[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006530633720103651\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005470[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005963[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006252[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006412[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006504004052840173\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008080[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007338[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007202[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007034[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00668099713511765\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005504[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005419[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005779[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006257[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006327820644946769\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006407[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006103[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006207[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006152604194357991\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004896[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005442[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005773[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005903[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006017169973347336\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005877[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006214[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006516[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005948562640696764\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005266[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005553[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005615[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005687[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005625736498041078\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004776[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005637[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005305[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00538119730190374\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004577[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005520[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005176[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005275[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005419033410726115\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004864[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006246[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005690[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005621[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005483050929615274\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006339[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004892[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004876[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005162[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005233400000724942\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004902[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004977[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005371[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0052729810064192865\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006787[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005876[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005343[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005290[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005105748539790511\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004195[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004903[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004994[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00481520636822097\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004041[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004387[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004666[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004953263961942867\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005024[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004663[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004527[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004670[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004681707010604441\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005819[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005238[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004863[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004737414774717763\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003781[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004597[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004924[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004947[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004785891895880923\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003889[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003944[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004274[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004484[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004447839135536924\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007009[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004870[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004627[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004425[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004485479847062379\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004419[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004666[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004198[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004255[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0044863329268991945\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002984[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004789[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004670[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004520943597890436\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002701[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004380[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004458[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004535[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0043913261266425255\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003844[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003833[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004620[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004865[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004908915603300557\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003504[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004423[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004326[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004374[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0043786938127595935\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004126[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004335[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004213[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004062[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00393903196672909\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003989[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003755[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003708[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003739[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0037869386898819355\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003529[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003686[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003812[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003717[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003733981552068144\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003277[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003521[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003460[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003630[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003631082799984142\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003290[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003483[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003460[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003515[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003694802481913939\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003839[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003954[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003696[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003660[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003722317627398297\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006024[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004393[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003983[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003939975041430443\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004794[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003592[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003530[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003523[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0035392919846344737\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003441[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003183[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003260[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0034190672391559928\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004701[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003070[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003538[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003472[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0034052471863105895\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003084[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003615[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003492[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003429[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0034226001764182\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004640[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003936[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003596[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003384[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003365791362011805\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003538[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002955[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003146[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0033081877103541047\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002848[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002980[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003109[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003144974328461103\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002236[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002854[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002895[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030941943463403732\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003288[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003406[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003435[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003462[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003443326085107401\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003545[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003332[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003278[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003238[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003156368451891467\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004579[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003294[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003115[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030593764153309167\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002589[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003270[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003074[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002978[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030744593008421362\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004054[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003588[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003326[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003208[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0031384711764985695\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002656[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003434[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003210[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003097[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0031629496079403907\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004152[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003526[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003662[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003418[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0032378918665926905\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001903[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002420[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002556[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002782[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002826414903393015\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003285[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002880[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002734[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002800[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00277717097196728\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003153[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002648[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002708[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002832[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028152794955531137\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002308[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003400[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003282[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0029838335292879494\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002554[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003136[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003187[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003289[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0032359823060687633\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003606[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002794[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002695[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002624[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002665147272637114\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004482[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002855[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002855[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002817[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028206397371832283\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002455[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002601[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002453[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002544[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002604411172796972\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002636[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002719[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002614[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0025981873739510776\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003981[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002742[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002906[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002804[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026624105550581588\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002286[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002407[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002377[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024138009699527173\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002337[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.716718[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.455985[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.333248[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.2673752603470348\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026404[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019319[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015830[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014263[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013014438841491938\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010083[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008404[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007835[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007433[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007227302365936339\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006354[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005978[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005858[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005606[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005471241724444553\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004813[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004597[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004847[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004727[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004683623253367841\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004541[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004415[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004200[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004112[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004093428375199437\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004008[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003868[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003990[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004005[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004053406516322866\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005489[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004471[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004005[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003803[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0038130943779833616\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003287[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003170[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003212[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00327397322980687\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002764[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003389[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003173[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0032539049803745\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004223[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003392[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003067[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030143383191898465\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002680[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002674[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002876[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003192[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003232862876029685\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003284[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002916[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002913[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0029076292063109576\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002821[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002769[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002804[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002749[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002755445038201287\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003099[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002460[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002496[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002599[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0027626134426100178\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003235[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002680[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002630[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002639[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026509608287597075\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002210[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002490[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002621[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002667[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026489347219467163\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002558[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002831[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002751[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002631[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002570789848687127\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001727[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002183[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002453[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002583[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026857650256715713\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001924[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002435[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002583[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003040[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003106942347949371\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002837[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002134[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002574[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002599[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0025455447525018827\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002528[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002360[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002247[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002496[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024704081821255385\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002037[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002293[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002609[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002801[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0027203149162232875\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002875[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002442[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002559[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002606[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028341586934402586\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002104[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003627[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003626[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003481[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0034604458254761995\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003045[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003305[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002888[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002880[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0029559614893514665\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002227[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002855[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002821[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003120[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00335219468397554\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004441[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004735[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004517[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004004[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003554751837509684\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002519[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002441[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002866[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002645[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026472892495803535\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001973[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002249[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002150[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002269[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0022877605428220705\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002468[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002531[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002420[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002637[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0033528416679473594\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006818[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004676[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003965[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003492[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00312844505533576\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001699[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002328[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002550[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002522[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002435000435798429\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001802[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001680[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001835[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001889[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002066991792526096\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001483[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003897[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003151[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002849[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002845615096157417\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002266[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002124[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002824[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003269[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030871379422023892\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002161[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002756[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002825[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003080[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030916356190573424\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001235[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002616[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002591[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002521[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024200505868066102\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001380[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002912[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003310[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002814[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0027700282604200765\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005253[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004048[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003715[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005063[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005122769894660451\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004469[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003496[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003452554446994327\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002062[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003459[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003611[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003996[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0037769557966385037\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001554[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002133[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002861[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002963717869715765\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003928[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002976[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002477[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002515[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00290065482549835\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008179[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007459[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007044[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006599[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0070543083944357935\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002790[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004693[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004617[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004714665369829163\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011574[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005114[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004946[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004831[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004390324937412515\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001691[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002018[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002174[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004195[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004920954932458699\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009132[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004566[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003850[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003273[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030635286384494975\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002285[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002173[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002228[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0025599659071303903\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002088[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004342[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003267[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003244[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003202326863538474\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007166[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003790[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004346[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004467[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00426320266269613\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004215[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008317[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006409[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005369[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004863283224403858\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001855[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002422[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002633[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002482[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024841955833835526\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003327[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004719[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005231[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005222[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005400874311453663\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001950[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004362[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004676[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004166[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0037185794179094957\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002245[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002560[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002573[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002549[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002410275928559713\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000911[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001875[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002483[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002468[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002511645141930785\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001787[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006127175968140363\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023544[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008610[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007321[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006711[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006560196209466085\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009323[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005172[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006348[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006070941005600616\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003720[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003558[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003255[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003157[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028894948452943938\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003700[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002346[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002922[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003757[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003920600036508404\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002339[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002082[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019791571859968824\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001644[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002122[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002404[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002835[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002948191005270928\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002092[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004720[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004321[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004040[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003951411560410634\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006707[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003556[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003004[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002636[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00297226304828655\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004933[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003581[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003616[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003174991221749224\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001946[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003453[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003308[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003180469069047831\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002881[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004020[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003080[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002642[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002365011826623231\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002069[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001896[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019788994977716357\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001789[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001956[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001776[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002456[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0025162011239444836\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002472[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002728[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002782[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002541[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024386572942603378\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002012[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002197[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001907[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001785[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018803399667376651\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001539[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001857[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002198[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002371[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002206636921619065\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001834[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001663[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001804[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001769[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016780213714810087\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002319[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002717[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002923[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002634[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0031090002827113493\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001866[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005331[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004932[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005050[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005168834823416546\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004296[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002745[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002491[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002275[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023251561942743137\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001357[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002871[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002365[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002355[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024154882004950194\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001934[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001811[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001832[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001635[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00259988726611482\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005785[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004657[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003658[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003187[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00283474619500339\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001977[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002662[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002264[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002126[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00205284213880077\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002333[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002431[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002207[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001883[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0017634605115745216\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000740[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001486[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001664[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001499[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016214114963077008\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000593[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001421[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001495[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001362[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013908079723478294\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002224[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001381[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003040[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003355[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00437560810969444\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008559[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008471[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008258[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006795[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006072796307853423\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002183[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004010[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003189[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002968[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026921723096165807\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001440[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001604[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001638[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001614[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015860740022617392\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001710[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001305[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001240[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001482[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014200448305928149\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001909[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001250[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001428[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001642[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002283099001215305\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007277[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005846[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006738[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005938[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005543880758341402\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008138[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003739[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003179[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002764[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002737444800732192\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001623[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001379[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001349[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001228[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013148327052476816\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002885[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002398[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001912[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002011[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0025972256757086144\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002886[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002939[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002644[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002275[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0022337199392495678\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001548[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001191[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001132[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012199010510812514\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001109[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001336[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001142[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001221[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001172059071541298\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000918[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001357[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001430[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001671[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015327110391808673\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000958[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 29.157990[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 15.719652[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 10.905810[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.610300147760427\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.815297[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.570732[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.516996[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.465084[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.4233440551906824\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.159530[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.173852[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.144979[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.121379[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.1084519618190825\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.059565[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.052541[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.047394[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.045393[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.046027407189831136\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.037611[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033913[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033131[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.033945[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03455118299461901\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.019072[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030920[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030765[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030339[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.029980677552521227\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.027742[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.029587[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030269[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030717[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030422819778323174\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.033906[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.024506[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.027656[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026503[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02692453837953508\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.031038[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030567[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030179[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.030630[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.029756148206070067\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025494[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.025984[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.027782[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.026160267926752568\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034968[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.023447[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.023275[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.022234[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.021700265165418387\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021784[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020216[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019886[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.020642[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.022018934111110867\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023213[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027310[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.027140[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.028787[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.028189452202059328\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.031737[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.023649[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.023088[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021949[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.021493725455366076\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022831[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021824[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.021260[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021627[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.021173865883611143\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.024723[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019784[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.021017[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.020466[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.019902542047202588\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.019851[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019939[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019251[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019324[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01920263187494129\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017612[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018338[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018770[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018799[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.019171402719803155\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020417[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021424[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020847[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.022026[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.022998102498240768\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.044216[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.028876[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.026279[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023847[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023288251971825956\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025357[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018016[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018889[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018865[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01917965095490217\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017010[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020059[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018908[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018854[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0194941864348948\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.024729[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.022576[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.021656[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021625[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.021761574666015804\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.042136[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020961[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020078[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019850[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.019774556253105403\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.042211[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.030125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.026259[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023754[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023093896592035888\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034265[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020287[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024387[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.024089[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023042697203345598\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021076[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019513[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019584[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018934[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020740325679071248\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.034197[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.022138[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020150[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021001[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020701503823511304\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018565[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.017323[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018472[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018585[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018323665275238453\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013786[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.017017[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017516[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017697[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017615893739275634\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015589[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016640[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019769[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.020265[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020710102375596763\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016103[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018569[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019764[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020056267315521837\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016608[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016670[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016898[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017451[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017345271655358374\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017840[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016267[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015557[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016806[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017715243832208218\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.025843[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019123[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017327[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017142[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.016973169753327966\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013410[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.017569[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017267[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016726[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.016352646122686564\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015765[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015419[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015177[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016264[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01748438363429159\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018335[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020891[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019528[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018597869225777684\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014248[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018426[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017365[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017476[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017097288742661476\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015410[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018283[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024160[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023883[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02386981714516878\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015575[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.024571[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.023552[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023233[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.021275420766323806\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015937[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021754[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.022096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021802[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02002415372990072\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012328[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014711[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015989[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016936[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017059842078015208\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014235[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014255[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013523[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013256[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013445023400709032\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006468[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012700[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013522[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013022[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012888958328403533\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012845[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012598[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015707[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015936282486654817\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013958[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014836[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013698[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013301[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013191771041601897\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012855[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013478[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014853[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017772[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018532909406349062\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011636[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016281[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016013[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014681588974781334\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012707[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013327[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013354[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012985816330183298\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008463[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010318[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010460[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011299[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0118502979981713\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014126[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012819[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011926[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011286[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010847864078823478\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011623[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015033[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.022550[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021580[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020348204695619643\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022363[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011671[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011063[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011279[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01127268368145451\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008599[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010713[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012109[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011413[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011767949361819774\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014323[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011694[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010744[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010764[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010701883083675057\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010058[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011585[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012525[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011946[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011266239127144218\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013561[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009285[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009615[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010458[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010769617452751845\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010695[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013284[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011937[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010881[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010399776580743491\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009657[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009682[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010444[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010254977352451533\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008538[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013601[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014173[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015338[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015611420175991953\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010102[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020178[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019479[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019639[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018708279018756\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015957[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018583[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016682[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016094[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015741976769641043\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.020438[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014672[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015310[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014240[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013577757030725479\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007894[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011001[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011484[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011044926603790372\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009736[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013517[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013654[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014387[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014597074361518025\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008725[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018215[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016256[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015250[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014092440612148493\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009775[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013253[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011650[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011563[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011819827207364141\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009884[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009362[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010047[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01044777495553717\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021464[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009841[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011820[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013649[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01382581606740132\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015668[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016151[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017227[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01627585944952443\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012801[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011179[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010782[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010493062029127032\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008728[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010955[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010362[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010241[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010281767754349858\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016718[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011185[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011544[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011339[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011282580718398095\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013676[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009575[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012160[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015853[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014936708845198154\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012406[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010647[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010519[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010975[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011534557223785668\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012992[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009766[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010626[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010644[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010377758217509836\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012771[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008562[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009912[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010593[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01265523275360465\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017116[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013361[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011881[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012922[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012921953096520156\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008647[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011521[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010827[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010848[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010569323354866355\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008790[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012115[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016526[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014781[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01458414044464007\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.030230[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.019743[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018782[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017441[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.016664363583549856\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009053[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014673[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012675[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011821[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01165260614361614\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017288[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011195[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009963[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010559[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01062233782140538\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012764[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011491[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010347[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010359[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010129447223152965\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010295[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008732[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008436[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008591[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009224060969427229\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007698[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012558[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013732[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012482[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0117436368833296\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010092[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009351[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009228[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009158[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009807293675839901\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015468[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011132[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009972[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009576[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009440819139126689\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008914[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008100[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007544[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00821898995200172\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007518[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011026[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012039[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012347[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011565325502306222\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009021[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010637[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010261[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010892[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010930330748669803\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009475[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009932[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010499[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01035122264875099\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008820[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009516362729482353\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010380[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008743[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008665[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008570[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008642551174852997\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009933[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008355[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008899[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008573[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00869369040010497\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006249[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010834[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009942[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010215[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01134724960429594\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005827[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011517[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011763[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011701976240146905\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015685[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011560[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012820[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011337[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011215993005316704\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011315[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010480[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009899[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010045[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009826910158153623\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extra to determine the learning rate:\n",
    "\n",
    "# define the neural network model\n",
    "d = dataset[\"X_train\"].shape[1]\n",
    "p = 1000 \n",
    "\n",
    "model = NeuralNetwork(d,p)\n",
    "\n",
    "\n",
    "# define the hyper-parameters of the network training\n",
    "#epochs = 1000\n",
    "epochs = 100\n",
    "\n",
    "learnings_rate = [1e-5,1e-4,1e-3,1e-2]\n",
    "\n",
    "# train the network for each learning rate\n",
    "for learning_rate in learnings_rate:\n",
    "    train_loop(train_dataloader, model, epochs = epochs, learning_rate = learning_rate, datatype= \"train\")\n",
    "    plot_loss(learning_rate,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.785703[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.950040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.843162[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.781814[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.7164029784500598\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.424779[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.445286[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.397505[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.363727[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.3381321784108877\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.312588[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.202205[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.180975[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.165889[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.15306923743337392\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.075094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.101862[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.093893[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.085761[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.08259751321747899\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.055426[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.060276[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.061548[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.060040[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.06199608687311411\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.060882[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.064915[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.057073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.052254[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.051983009977266195\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.040076[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.044008[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.044789[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.045482[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04508752389810979\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.067366[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.041611[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.043934[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.042120[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.04079445591196418\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.042248[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.032006[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.033026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.034932[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03679467104375363\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.032329[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.031565[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.031659[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.031081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.03307754783891141\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.039924[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.033454[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.030911[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.031677[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.030687667429447174\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.026924[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.023810[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025120[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.028780[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.028707599430345\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.019175[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027319[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.026429[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026524[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.027331804670393468\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.027570[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025639[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.026556[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.026180099695920944\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023360[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021060[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.022350[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.024373[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.024488597037270667\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021758[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021628[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.022265[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.022468[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023804713529534637\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023374[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.026068[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.025706[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023262[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.023241708427667616\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018937[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.024921[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.024114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.023287[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.022534324135631324\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018316[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.021544[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.023562[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.021706[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.02155201465357095\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017780[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018749[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.019598[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.020876[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.020247003342956305\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018443[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.020375[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.020039[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019807[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.019665009900927545\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016466[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.018421[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018666[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.019413[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018930250965058803\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.021550[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.017028[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.018908[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.018940[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.018269914761185647\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.022982[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015679[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016456[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017605[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.017388397268950938\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.014573[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016359[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.017583[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.017039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.016643610410392286\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.016116[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.015154[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015444[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016286[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01736891604959965\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011903[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014929[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016446[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015775[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01623561338055879\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015415[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016207[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.015382[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.015357[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015352048492059112\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015549[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.016151[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.016522[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.016486[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.015811052010394632\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.013818[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014329[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.013894[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014633[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.014441920490935446\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010433[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014393[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014247[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014322[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013971354579553008\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.023861[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.013048[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012884[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.014107[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013528348738327622\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.017768[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014810[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014072[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013712[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013477995712310075\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012795[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012933[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012890[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01304754321463406\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011964[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012774[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.014346[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.013685[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.013446734589524566\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011729[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.014426[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012498[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.012208583264146\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009214[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012982[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012859[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012304[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011993104917928576\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008739[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.012456[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012880[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.012365[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011973105429206043\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012508[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.011030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.011023[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011270[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.01127529809018597\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009500[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010672[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.012114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.011527[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.011226596776396036\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011888[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010808[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010283[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010784[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010757594113238155\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010582[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010463[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010103[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010179[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010317222482990473\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010975[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010465[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010706[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.010477[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0104575838544406\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007578[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010758[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.010394[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009830[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009986704110633581\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008862[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009320[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009338[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009834[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.010028458025772125\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008399[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009559[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009445[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009769[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009592660597991198\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011764[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.010398[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009684[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009783[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009827477321960033\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007848[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008956[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008986[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009085[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009150747838430107\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006655[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009500[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009481[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009339[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.009496827679686248\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.010538[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007944[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008866[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008814[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008520834788214415\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.015018[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.009955[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.009166[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.009097[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00870703865075484\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.011743[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008025[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007978[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008144[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008187860844191163\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008486[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.008998[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008356[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007914[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007940238458104432\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.009915[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007592[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007662[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008022[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007849308068398387\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005410[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007179[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007194[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007404[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007649008615408093\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008170[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007821[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008394[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008208[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.008052209299057722\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004964[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007813[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.008228[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.008485[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00848337026545778\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007396[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007138[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006991[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007194[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007370086770970374\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012387[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007530[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007307[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007207705127075315\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006264[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007276[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007046[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007205752283334732\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012565[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.007033[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007338[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007092[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006991529150400311\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007816[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006822[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.007482[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.007134[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.007071968179661781\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006321[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006873[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006870[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006717[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006451406527776271\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005528[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006331[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006174[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006397[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006450097914785147\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005827[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006183[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006169[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006302[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006344027619343251\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004683[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005467[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005754[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006258[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00624052647035569\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008002[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006178[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006196[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006019[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006062270904658362\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005594[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006005[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005906[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005787[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005820912274066359\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008788[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006056[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005788[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005775[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005688663164619356\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006381[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005429[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005581[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005834[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0058284454280510545\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005297[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.006524[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.006396[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.006113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.006159493094310164\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006758[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005895[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005628[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005511[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0054886418976821\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005080[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005539[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005288[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005256[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005618225899524987\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005459[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005319[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005249[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005413[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0054194475698750464\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006197[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005335[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005027[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005306[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005444197054021061\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005663[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005600[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005371[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005344[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0052896999404765666\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006468[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005152[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004989[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004881[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00509973926236853\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004957[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004631[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005058[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005144694191403687\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006068[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005003[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005295[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005219[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005081930983578786\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006013[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004347[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004565[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004735[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004846096911933273\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005214[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004952[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005238[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004974[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0050817428273148835\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006433[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.005036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.005137[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005365[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00519963675760664\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003234[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004224[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004935[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.005357[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.005221385962795466\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004222[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004536[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004453[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004485[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004620962357148528\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005264[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004481[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004589[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004391[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004471160349203274\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003872[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004435[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004299[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004513[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004504219739465043\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006059[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004543[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004433[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004526[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004546780633972958\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.006264[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004813[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004548[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004593[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004530408279970288\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003877[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004337[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004167[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004108[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004135206673527137\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004696[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003824[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004091[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004107[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004087218130007386\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004176[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003938[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004132[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004263[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004231813264777884\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004632[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004211[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004158[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0042470753192901615\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004500[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003903[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003804[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003826[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003959132835734636\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003509[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003453[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003723[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004009[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0040268644574098286\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003518[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003870[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004606[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004752[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.004623041726881638\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004754[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003599[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003806[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003807[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003789630107348785\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003507[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.004331[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.004210[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.004108[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0039697431377135215\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003353[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003574[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003658[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003643[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0037260897108353673\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003454[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003571[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003755[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003788[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0037978629290591924\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002553[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003204[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003257[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003679[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0036801188078243287\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004381[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003829[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003747[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003659[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003633585124043748\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002780[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003297[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003468[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003447[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003504313959274441\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002775[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003735[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003568[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003533[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003486305405385792\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003571[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003489[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003735[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003591[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0036685967294033616\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003952[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003192[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003248[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003326[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003458827844588086\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002813[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003169[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003451[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003678[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0038347492343746127\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002932[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003834[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003455[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003331[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003351021307753399\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003060[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003379[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003507[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0035183566913474353\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002749[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002970[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003338[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003540[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003502324823057279\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002026[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002827[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003018[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0031637275766115635\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002704[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002887[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003374[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003340783025487326\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003967[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003296[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003216[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003240[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0032569505681749435\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002704[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003829[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003459[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003295[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003232351108454168\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003514[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003194[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003044[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003110399321303703\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003155[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002939[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002978[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003047[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003030638297786936\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002888[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002947[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002993[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.003012248658342287\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002928[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003366[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003021[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002935[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0029492352274246513\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002888[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.003027[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.003340[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.003312[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00318575058481656\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002259[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002824[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002951[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002919[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028938437666511164\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003214[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002783[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002720[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002758[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028750787838362156\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003371[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002930[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002818[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002895[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0030251434131059796\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002637[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002850[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002800[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002779[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002752192044863477\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002810[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002822[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002774[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002829[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0027999364363495262\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002747[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002956[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002943[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002878[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002830042864661664\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002295[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002540[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002595[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002592[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026378813694464044\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002252[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002863[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002736[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002738[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002703975452459417\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005291[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002733[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002778[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002654[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0027057669503847137\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002536[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002492[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002588[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002633[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002669090623385273\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002185[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002539[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002559[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002701[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028542089625261725\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001836[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002887[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002715[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002644[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0026724972354713827\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003991[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002470[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002497[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002430[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002450049691833556\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002252[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002464[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002705[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002703[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002671584315248765\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002306[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002439[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002430[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002428[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002434299560263753\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002812[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002489[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002482[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002711[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0028073285822756587\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002436[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002318[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002298[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023819312831619755\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001815[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002149[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002484[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002415[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023665486747631802\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001776[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002121[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002254[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002388[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024765877140453087\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002908[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002217[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002449[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023903586057713254\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001973[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002192[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002303[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0022973793413257225\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002233[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002596[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002501[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002507[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0024200823972932994\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002928[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002370[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002395[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002297[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002408603334333748\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002846[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002443[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002325[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002459[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00247527965111658\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002846[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002411[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002260[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002228[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002264211233705282\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002002[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002065[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002101[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002169[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002234384839539416\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001827[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001935[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002128[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002178334869677201\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002096[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002208[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002150[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002106458690832369\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001909[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002179[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002145[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002203[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0021752621920313685\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001844[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002213[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002162[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002328[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023183373035863043\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003345[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002534[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002328[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002252[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002200933260610327\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001821[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001869[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001913[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001948[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001974572366452776\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001911[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001852[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002000[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019965240644523874\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002086[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001817[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001885[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002005[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002006201568292454\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002307[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002164[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002030941829434596\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001430[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001979[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001878[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002097[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0022940236958675086\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002352[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002929[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002713[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002511[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0023504137963755055\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002113[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001878[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001883[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001984[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019386349420528858\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001617[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001898[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001878[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001887[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019632451905636115\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001725[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001757[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001788[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001813[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019098526507150381\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002500[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002413[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002306[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002208[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.002226697086007334\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001852[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001963[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001921[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001921321195550263\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001722[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001998[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002076[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002055[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0019873227778589354\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001471[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001860[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001979[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001971[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0020149839605437593\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001964[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001941[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001867[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001816[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018251583707751707\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001544[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001947[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001762[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001806[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018245927611133083\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001058[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001700[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001704[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001724[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001750168140279129\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001894[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001725[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001768[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001882[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018458465347066522\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001587[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.002296[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.002322[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.002250[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0022151885117636994\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001489[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001706[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001712[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001757[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0017421318450942635\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001721[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001709[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001637[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001673[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016736131161451339\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002266[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001977[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001945[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001843[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018150950782001019\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001841[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001843[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001743[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001824[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001816468354081735\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001675[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001719[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001665[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001589[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016218550299527124\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001399[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001731[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001650[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001620[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001685758821258787\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001176[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001678[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001640[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001659[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016712084237951786\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001471[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001502[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001537[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001584795577218756\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001680[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001746[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001715[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001620[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001670482798363082\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001199[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001555[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001640[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001673[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016471427501528525\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001832[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001526[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001593[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001544[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015402300952700898\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001825[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001506[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001517[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001531[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016119058214826508\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001353[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001512[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001575[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001584[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015937452073558234\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001766[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001486[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001583[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001609[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0016609257872914895\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001222[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001452[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001511[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001492[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015093562979018316\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001479[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001522[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001456[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001502[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014956128696212545\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001165[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001387[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001443[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001495138494647108\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001384[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001541[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001497[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001495[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014937143976567313\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001929[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001830[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001891[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001764[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0017063229519408197\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001085[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001388[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001359[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001434[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014468080451479181\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001341[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001410[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001497[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001656[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001730784514802508\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001540[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001528[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001704[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001926[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0018897223140811547\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001197[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001397[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001367[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001352[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001376678264932707\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001241[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001272[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001523[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001536[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001462659632670693\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001080[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001392[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001351[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001357[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001423493810580112\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001810[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001442[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001399[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001354[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001356504209979903\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001699[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001338[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001319[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001314[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013097155795549042\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001178[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001230[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001296[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001273[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012910081903100945\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001510[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001287[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001381[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001366[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001330790846259333\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001632[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001281[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001323[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001336[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001317194827424828\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001193[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001152[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001232[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001229[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012541846081148833\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001537[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001245[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001180[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001294[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012957478233147413\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001015[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001268[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001429[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001395[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013900783495046198\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001475[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001490[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001389[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001403[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001371256167476531\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001418[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001285[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001249[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001318[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012775370763847605\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001165[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001398[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001339[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001299[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013042675098404288\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001064[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001337[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001376[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001466[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014802687030169182\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000781[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001417[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001560[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001529[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0014559276401996612\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001204[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001239[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001188[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001200[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00123486086667981\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000888[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001070[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001107[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001156[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012137210127548315\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001252[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001538[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001436[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001333[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012942161556566134\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001127[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001220[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001205[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001156[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011666629376122727\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001558[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001223[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001203[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001308996370062232\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001755[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001370[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001303[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001265[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001297908162814565\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000898[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001460[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001450[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001371[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012986692338017747\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000726[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001011[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001111[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011021684418665244\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000866[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000981[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001049[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001148[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001190174260409549\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001267[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001168[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001194[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001204[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001184894957987126\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000965[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001141[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001202[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001174[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011396279980544933\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000978[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001081[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001199[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001171[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001198237515927758\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001424[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001174[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001373[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001328[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012672348690102807\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001112[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001007[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001163[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012717275370960125\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000903[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000982[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0010522200420382433\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001663[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001002[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001126[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001121806247101631\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000955[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001053[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001165[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001229[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.001235197964706458\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001441[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001090[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001121[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0010875458319787867\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000889[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000946[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001123[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001123[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011529931842233054\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001125[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001106[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001013[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0010199636992183514\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000849[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001112[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001128[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001127[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011541336978552863\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001284[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001223[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001299[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001305[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012762245358317159\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000953[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001099[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011010687914676965\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000868[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001358[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001432[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001371[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0013106746235280297\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001210[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001042[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011318330653011799\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001548[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001299[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001445[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001535[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0015798947613802738\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001327[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000978[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001139[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011592232083785347\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001953[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0010212060369667597\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000788[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000891[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000899[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000914[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009082736141863279\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000619[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000805[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000871[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000913[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009427143377251923\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000983[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000954[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000947[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000975[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000995480855635833\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001040[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000919[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000960[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000920[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009124838950810954\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000845[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000894[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000875[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000935[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000946697466133628\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001162[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000888[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000916[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000912[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009330049317213707\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000929[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000876[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000946[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000984[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009794509605853818\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000858[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000871[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000844[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000875[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000921465914871078\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000716[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001033[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001021[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0011136059678392486\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001930[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001458[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001496[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001330[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0012290909857256339\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000780[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000825[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000868[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000854[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008329091317136772\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000879[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000871[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000855[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000842[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00082597467553569\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001110[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000817[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000851[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000869[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008799488816293888\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000897[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000940[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000978[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000975[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009803788780118339\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000873[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000884[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000924[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000913[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008732793459785171\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000708[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000814[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000851[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000863[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009122683899477124\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000874[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000837[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000884[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000908[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009070021580555477\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000703[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000777[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000837[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000861[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008917550076148473\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000861[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000692[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000838[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000836[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009402188632520847\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000843[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000929[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000901[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009251937372027896\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000791[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000754[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000782[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000797[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008103392334305682\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001125[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000775[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000775[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000811[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000800711294868961\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000661[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000705[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000744[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000779[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008054476435063407\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000706[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000778[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000757[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000767[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007693200197536499\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000535[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000742[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000759[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000933[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009913529836921952\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002010[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001194[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.001051[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000992[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0009909166881698183\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001048[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000799[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000840[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000857[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008720775615074672\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000806[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000946[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000972[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000921[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008817877955152653\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000544[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000729[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000742[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000746[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007618588555487804\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000701[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000693[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000730[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000730[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007482979868655093\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000816[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000765[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000859[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000846[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008845452117384412\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000603[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000851[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000951[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.001042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000999954126018565\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001003[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000793[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000858[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000839[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008140487901982851\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000510[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000695[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000766[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000758[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007468851224984973\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000664[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000717[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000730[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000709[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000728328236436937\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000764[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000785[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000721[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000691[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007045460108201951\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000778[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000739[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000708[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000725[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007126441058062482\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000618[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000654[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000653[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000661[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006781684212910477\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000409[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000630[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000672[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000729[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000801353043061681\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000665[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000792[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000763[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000753[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007512218871852383\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000848[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000760[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000694[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000741[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000726681081141578\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000861[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000646[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000654[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000693[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007228801041492261\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000524[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000640[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000660[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000779[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007634362562384922\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000892[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000868[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000806[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000778[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007541381477494724\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000659[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000676[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000682[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000683[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006798918657295872\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000461[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000643[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000610[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000614[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006360945502819959\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000644[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000602[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000654[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000662[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006763371100532822\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000697[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000650[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000675[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000689[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007941896459669806\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000728[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001070[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000893[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000841[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007980113892699592\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000703[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000913[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000837[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000770[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007569438974314834\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000649[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000727[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000905[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000825[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000792157062824117\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001342[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000823[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000806[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000951[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0010118062265974004\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002134[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.001132[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000955[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000856[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008217820133722853\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000435[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000621[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000609[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000621[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006480218027718366\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000864[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000776[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000756[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000738[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007099677044607233\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000462[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000617[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000592[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000595[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006026079216098878\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000692[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000746[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000770[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000779[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007552986717200838\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000766[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000567[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000610[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000666[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006959777580050286\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000736[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000603[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000587[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000569[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005852745875017717\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000779[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000589[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000606[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000582[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005995376115606633\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000827[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000692[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000652[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000696[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006630219802900683\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000545[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000614[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000595[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000593[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006041242311766836\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000550[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000649[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000639[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000608[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006323318011709489\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000573[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000647[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000656[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000625[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006078402962884866\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000708[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000568[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000526[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000555[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005689029567292891\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000413[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000568[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000548[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000581[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005734456470236182\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000520[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000551[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000585[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000670[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007252436087583191\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000435[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000602[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000596[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000597[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005852738693647552\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000569[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000556[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000579[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000584[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005892990295251366\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000587[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000612[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000616[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000590[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005818277182697784\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000628[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000662[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000727[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000788[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0008247324105468579\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000506[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000760[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000756[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000715[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006940828927326948\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000630[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000527[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000583[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000662[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006623913766816258\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000690[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000692[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000736[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000840[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007975680331583135\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000503[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000494[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000497[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000503[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005165416216186714\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000562[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000526[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000523[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000516[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005119545829074923\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000417[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000448[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000465[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000515[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000535978352127131\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000387[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000564[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000558[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000595[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005713252292480319\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000460[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000519[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000505[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000524[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005350066283426713\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000431[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000538[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000635[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000641[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006269513578445185\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000383[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000468[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000488[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000543[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005548090935917571\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000550[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000489[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000536[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000575[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000561218360235216\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000596[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000495[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000498[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000508[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005033693894802127\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000335[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000510[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000544[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000511[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005178861014428548\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000350[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000693[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000653[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000593[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006139198318123818\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000641[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000571[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000552[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005622306409350131\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000624[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000562[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000535[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000532[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005245456770353485\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000359[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000477[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000482[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000503[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000504017169441795\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000455[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000585[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000529[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000518[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005077104149677325\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000457[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000436[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000490[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000549[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005260020327114035\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000459[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000488[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000497[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000483[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004891371747362428\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000839[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000554[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000516[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000500[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004936962061037776\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000473[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000446[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000439[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000460[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004730031170765869\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000405[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000467[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000457[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000487[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00048739914273028263\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000569[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000466[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000506[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000516[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005104632786242291\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000431[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000492[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000559[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000618[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006578773718501907\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001180[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000660[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000643[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000697[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0007131965670851059\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000407[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000736[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000692[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000642[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006246518387342803\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000639[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000503[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000587[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000581[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005438720720121637\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000354[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000465[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000476[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000536[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005747150171373505\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000575[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000718[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000641[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000646[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006910558266099542\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000644[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000632[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000575[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000543[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005245380467385985\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000416[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000498[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000476[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000465[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004549799225060269\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000381[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000416[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000410[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000473[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005212753007072024\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000626[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000572[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000544[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005318031886417885\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000379[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000393[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000461[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000511[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004934362885251175\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000473[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000439[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000431[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000425[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004509816979407333\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000405[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000384[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000407[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000418[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00042256064407411034\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000424[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000385[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000443[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000466[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00047723938114359045\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000358[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000461[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000441[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000411[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00039848290107329375\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000313[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000350[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000364[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000439[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00046981230188976043\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000616[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000489[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000452[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000435[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00042483659781282767\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000482[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000392[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000425[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000427[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004304749447328504\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000293[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000403[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000402[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000417[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004148420179262757\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000316[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000358[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000376[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000385[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003966758195019793\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000296[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000381[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000396[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000386[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004275159335520584\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000400[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000358[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000353[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000376[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003965699452237459\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000423[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000497[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000545[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000535[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005185632646316663\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000558[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000416[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000442[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000473[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004580151828122325\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000395[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000523[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000488[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000581[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005459366606373805\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000969[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000637[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000739[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000717[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006481059572251979\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000511[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000472[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000437[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000434[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004409870543895522\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000363[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000420[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000398[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000396[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00038158628303790464\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000283[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000357[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000355[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000352[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003709545821038773\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000856[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000682[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000651[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000576[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005703445131075568\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000554[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000397[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000428[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000456[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004483820790483151\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000343[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000397[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000460[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000489[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005170243228349136\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000375[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000376[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000352[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000363[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00040248650111607276\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000274[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000516[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000492[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000507[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005042111682996619\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000435[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000527[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000606[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000603[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0006110217393143103\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000731[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000643[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000577[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000550[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005384727992350236\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000424[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000401[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000370[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000386[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000382899671967607\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000239[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000321[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000381[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000378[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003700577490235446\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000446[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000428[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000423[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000463[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00046842881420161575\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000299[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000656[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000578[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000499[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004882559678662801\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000315[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000405[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000415[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000415[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00041978654517151883\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000501[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000430[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000391[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000423[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004087609693669947\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000311[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000330[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000367[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000371[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003532893111696467\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000276[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000330[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000333[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000337[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00034828395037038716\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000269[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000385[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000365[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000365[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000359929168553208\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000501[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000418[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000440[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000460[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004483806129428558\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000291[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000329[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000312[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000304[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003248571083531715\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000211[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000441[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000448[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000445[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004266679061402101\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000263[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000317[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000326[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000341[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000371856264246162\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000226[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000432[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000410[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000397[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00039750376527081244\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000294[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000407[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000359[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000331[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003300041524198605\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000292[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000316[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000302[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000297[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002957626034913119\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000480[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000297[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000285[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000306[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00030668713297927753\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000312[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000415[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000392[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000399[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000403909285159898\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000273[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000262[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000298[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000323[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003219019708922133\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000383[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000282[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000304[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000323[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003216741650248878\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000404[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000495[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000530[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000534[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005224055268627126\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000313[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000320[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000292[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000309[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00035240950383013113\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000558[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000466[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000432[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000386[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003934347856556997\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000314[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000345[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000343[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000347[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00034604675238369964\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000383[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000331[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000327[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000314[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003061532341234852\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000224[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000288[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000324[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000347[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003370543799974257\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000318[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000287[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000293[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000317[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003389581968804123\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000313[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000299[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000293[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000307[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00030838904749543874\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000253[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000289[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000419[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000410[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003919669528841041\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000341[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000302[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000296[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000293[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00029119702412572224\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000788[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000505[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000410[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000368[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00035024617827730256\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000290[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000355[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000331[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000319[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000313936729071429\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000185[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000268[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000262[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000260[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002712106186663732\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000342[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000345[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000331[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000335[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003237464952690061\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000240[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000286[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000305[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000317[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00030201685985957737\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000258[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000288[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000305[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000307[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00030329396213346627\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000629[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000373[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000335[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000343[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003676897518744227\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000350[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000282[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000286[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000270[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00026702030954766086\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000339[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000362[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000362[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000353[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000335076743067475\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000264[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000293[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000279[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000283[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000284270296106115\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000263[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000264[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000272[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000267[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00026518295271671377\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000273[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000250[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000254[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000263[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002745065361523302\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000221[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000308[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000307[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000319[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00031217307841870936\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000200[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000334[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000312[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000305[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003014940968569135\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000304[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000299[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000288[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000335[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032948440020845736\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000255[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000222[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000236[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000258[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002583407342171995\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000367[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000273[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000282[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000283[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028946473648829853\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000150[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000417[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000412[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000357[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00033752803137758746\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000342[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000288[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000342[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000401[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00045623979676747697\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000403[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000667[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000654[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000578[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000509889879685943\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000263[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000236[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000272[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000277[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002975123486976372\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000318[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000314[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000355[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000331647911662003\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000259[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000229[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000226[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000232[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002392730504652718\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000408[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000325[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000294[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000283[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002836806161212735\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000213[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000232[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000262[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000271[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00026555107469903303\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000256[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000223[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000239[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000262[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00027108570175187195\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000215[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000259[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000236[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000257[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002831592461006949\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000200[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000312[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000272[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000261[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00027098980572191065\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000232[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000270[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000285[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000305[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00030017242788744626\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000307[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000253[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000253[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000243[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00024151989600795787\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000196[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000239[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000219[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000254[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002784782935123076\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000193[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000254[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000249[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000272[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00027721236692741514\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000523[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000317[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000279[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000271[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002761411375104217\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000212[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000228[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000219[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000237[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00025658986451162493\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000363[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000355[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000331[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032033920397225304\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000413[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000273[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000251[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000288[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00029779019314446487\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000384[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000380[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000369[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000331[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003115794275799999\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000172[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000279[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000277[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000286[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028105866003897973\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000245[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000212[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000197[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000217[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00022957628425501753\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000335[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000282[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000290[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000281[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002760901341389399\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000409[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000281[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000241[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000242[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002618233866087394\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000599[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000294[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000310[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000399[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004000711061962647\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000525[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000350[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000339[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000325[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00031901157199172304\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000287[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000452[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000479[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000445[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004011714987427695\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000181[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000298[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000275[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000259[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002636053159221774\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000305[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000228[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000213[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000244[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00026788468785525764\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000228[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000322[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000350[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000432[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000440334824816091\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000330[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000361[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000325[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000346[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00033099277934525164\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000185[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000259[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000262[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000271[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002718021951295668\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000143[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000192[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000194[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000202[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021049840743216918\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000206[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000227[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000226[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000264[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002850844599379343\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000292[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000256[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000221[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000209[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002076680368190864\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000160[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000201[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000217[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000257[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000265946717991028\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000153[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000204[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000195[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000193[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020091016085643787\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000363[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000280[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000257[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000233[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002430283660942223\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000187[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000319[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000286[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000266[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028424518313840963\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000366[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000317[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000283[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000265[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00027241694515396376\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000577[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000335[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000327[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000288[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002677248641703045\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000220[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000215[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000234[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000226[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002235425470644259\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000161[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000222[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000225[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000270[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002984779446705943\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000370[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000330[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000289[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000280[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028230126190464944\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000495[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000293[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000285[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000275[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002724175345065305\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000252[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000230[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000220[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000210[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021002455432608257\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000113[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000239[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000253[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000250[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002437821083731251\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000258[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000198[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000188[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000189[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019670189594762634\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000226[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000255[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000215[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000192[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018599256982270161\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000168[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000192[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000183[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000180[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018810372948792066\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000169[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000203[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000225[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000215[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021991517332935472\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000171[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000176[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000204[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020731156055262546\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000156[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000193[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000234[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000224[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002178091075620614\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000167[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000172[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000201[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000198[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019716357892320956\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000238[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000181[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000186[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000218[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002319131606782321\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000361[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000445[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000399[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000352[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032996143090713304\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000229[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000206[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000208[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000199[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019125561357213883\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000160[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000203[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000267[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000255[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002503880024960381\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000287[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000245[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000265[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000262[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00025306022489530734\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000362[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000233[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000200[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000191[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019525971765688155\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000265[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000194[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000188[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000182[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019858207597280852\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000196[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000208[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000181[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000193[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018592632713989587\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000112[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000257[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000227[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000243[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00022700063200318255\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000247[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000240[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000238[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000215[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00022796014527557418\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000120[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000230[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000265[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000283[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00027178153959539486\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000141[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000219[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000203[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019170177583873737\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000130[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000180[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000185[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000197[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020951147980667882\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000772[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000473[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000351[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000297[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00029013838575338015\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000337[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000260[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000218[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000215[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020813366918446264\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000221[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000189[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000165[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000169[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017050575952453072\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000188[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000206[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000205[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000210[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021238712797639893\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000158[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000158[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000161[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015798174790688791\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000110[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000156[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000150[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000147[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014821995573583991\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000140[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000147[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000160[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015847622125875205\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000356[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000301[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000264[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00023813983043510233\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000141[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000231[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000192[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000190[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020311725238570945\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000289[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000272[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000232[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000221[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002731760498136282\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000884[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000534[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000500[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000427[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00038377239579858726\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000168[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000215[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000198[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000187[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020218578101776074\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000141[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000252[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000253[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000257[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028058866864739685\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000871[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000563[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000434[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000362[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003132623898636666\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000145[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000134[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000140[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014879178525006864\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000153[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000152[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015828897376195527\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000222[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000160[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000158[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016324571370205377\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000178[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000158[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000142[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000141[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013980477797304047\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000115[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000161[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000237[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000232[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00022278693431871944\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000159[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000164[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000194[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000195[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00023080205464793836\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000355[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000311[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000310[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032925977757258806\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000686[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000356[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000313[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000259[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00023895930917205987\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000117[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000114[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000148[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000152[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016127254184539197\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000221[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000171[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000166[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000163[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015657060448575065\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000131[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000123[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000126[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001295113834203221\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000141[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000123[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000130[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000135[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015461486163985682\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000181[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000156[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000154[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015358453783846925\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000135[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000132[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000138[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014449449936364543\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000150[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000144[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000136[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000144[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001566957625982468\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000173[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000220[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000214[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000184[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018267116665811046\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000134[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000140[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000161[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001642257448111195\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000143[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000182[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000180[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000176[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019710602828126866\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000455[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000276[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000295[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000315[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002997745245011174\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000088[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000203[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000241[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000216[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021858294294361258\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000206[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000459[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000371[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000315[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002922080369899049\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000110[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000179[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000157[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000146[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001432518414731021\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000104[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000108[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011864682874147547\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000160[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000147[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000145[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000147[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014754330732102972\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000125[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000150[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000172[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000169[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001726032354781637\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000108[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000281[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000277[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000236[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021615805690089474\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000097[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000131[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000135[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013874332089471863\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000084[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000121[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012867651239503174\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000119[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000155[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000203[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000193[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017852717701316578\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000156[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000178[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000211[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00024704041079530724\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000163[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000161[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000155[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001559098618599819\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000149[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000147[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000135[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000145[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014306954417406815\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000140[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000115[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000134[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000156[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015290097780962242\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000102[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000141[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000142[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000173[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002086143063934287\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000275[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000228[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000195[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018580637079139705\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000143[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000126[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000139[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000174[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018326554181840037\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000268[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000214[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000199[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000190[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017684526173979975\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000087[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000139[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000124[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000123[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001211843282362679\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000116[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000115[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000120[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013316356871655443\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000205[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000216[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000183[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000208[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020357799039629753\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000135[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000142[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000147[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000150[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015729150818515335\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000123[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000163[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000141[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000139[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013838548420608277\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000083[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000160[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000200[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000223[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00024112466144288192\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000121[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000201[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000215[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000199[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021135398656042527\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000290[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000384[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000365[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003301486976852175\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000126[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000123[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000132[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000126[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012406070054566954\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000091[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000187[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000171[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016797600237623557\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000168[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000165[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000158[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000164[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017089056891563814\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000102[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000159[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000187[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000172[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015869208436924964\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000101[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000103[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000115[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001127989926317241\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000085[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000135[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000179[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000163[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015805598613951588\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000145[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000134[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000148[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000160[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016720086969144177\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000239[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000170[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000141[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014024646316102007\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000131[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000127[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013542920933105052\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000097[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000146[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000152[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000141[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013530187388823833\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000110[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000171[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016557170329178916\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000128[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000111[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000102[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010060563108709175\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000126[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000100[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000116[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000111[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011450356705609011\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000084[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000265[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000230[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000209[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001932269233293482\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000149[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000238[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000322[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002943603551102569\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000405[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000343[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000480[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000472[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004165013277088292\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000111[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000132[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000122[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000143[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014647638599853962\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000102[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000126[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001108552989535383\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000096[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000118[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000130[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013546334730563103\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000126[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015296824713004754\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000134[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000146[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000138[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000128[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013017156798014184\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000089[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000120[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000138[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001277586665310082\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000182[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000137[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000116[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000121[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012444485919331781\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000457[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000252[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000198[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000182[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016848095856403233\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000120[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000121[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000190[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000212[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00022636681296717143\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000151[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000190[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000216[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019341926363267704\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000109[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000111[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000141[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001667390779402922\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000256[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000150[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000139[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012884785264759556\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000119[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011854330914502497\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000197[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000145[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000156[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000151[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015133676570258103\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000115[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000113[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000118[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000128[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001289737039769534\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000172[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000234[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000245[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000242[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002473323509548209\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000282[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000194[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000252[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000262[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00024568219614593543\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000169[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000172[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000193[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000213[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002537964606744936\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000115[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000193[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000156[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012949259653396438\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000138[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000149[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000147[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014121015210548648\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000084[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010450550507812295\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000116[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000113[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000119[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011799105068348581\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000099[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000091[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000084[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000089[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001020451712065551\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000369[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000181[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000185[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000190[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001774517486410332\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000134[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000238[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000224[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002329373350221431\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000205[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000165[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000145[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001346718474451336\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000138[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000102[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010502233972147224\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000126[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000108[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000126[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001402776490067481\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000339[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000181[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000143[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013585692167907836\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000083[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000101[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000100[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000114[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013341824651433854\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000310[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000197[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000214[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000199[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017876387191790856\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000054[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000082[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000087[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000094[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.360136909890571e-05\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000139[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000130[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014150485258142\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000172[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000130[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000136[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001365222089589224\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000121[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000147[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012002355470031034\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000101[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000115[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000110[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010107215703101247\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000081[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000106[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010211351082034525\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000071[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000074[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000078[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.278365223508445e-05\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000352[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000199[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000146[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000125[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012923744761792478\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000123[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000105[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000092[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000088[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.273846626456361e-05\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000116[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000169[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000170[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000187[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002032968766798149\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000197[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000147[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000123[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010376979371358175\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000094[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000090[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000098[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010438749413879122\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000193[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000202[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000198[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000195[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019817417660306091\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000500[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000253[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000196[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000163[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015087922429302125\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000059[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000115[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000107[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010067618886751006\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000058[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000102[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000106[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010198346271863556\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000108[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000082[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.004480223462451e-05\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000219[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000125[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000111[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011635659593594028\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000123[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000095[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.729005332701491e-05\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000093[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000136[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000149[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016967185774774407\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000157[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000131[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013472054688463687\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000049[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000068[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000087[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.012190812427435e-05\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000168[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000139[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000119[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011625558199739317\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000195[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000120[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000137[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000171[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017399132111677317\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000332[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000181[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000152[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000183[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020419999500518316\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000160[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000310[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000262[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000244[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002516414598176198\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000286[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000198[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000180[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000181[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017281244618061464\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000084[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.145666861310019e-05\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000082[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000067[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000071[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.042747258514282e-05\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000051[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000133[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000117[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001082379856143234\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000061[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000075[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000098[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.17409207431774e-05\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000086[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000120[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011964370423811488\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000140[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000221[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000261[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028266757963137936\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000134[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000148[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000139[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000127[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012195703284305637\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000134[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000162[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000152[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013653853166033513\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000082[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.832875799067552e-05\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000113[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000099[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000087[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.671022717157029e-05\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000193[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000113[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000111[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000105[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001048198925673205\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000105[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000080[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000071[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000071[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.167061612562975e-05\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000074[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000077[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.654912487851107e-05\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000115[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000109[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000135[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014815716340308427\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000227[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000133[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000108[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000094[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.158506009043776e-05\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000056[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000069[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.380139618362591e-05\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000053[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000095[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000078[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.671797013448668e-05\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000093[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000102[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000092[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000098[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010782149311125978\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000101[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000096[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000083[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010050332402897766\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000093[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000141[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000156[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000146[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015067494214235922\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000197[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000213[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000203[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018487465040379903\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000177[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000210[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000165[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012961958118467009\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000120[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000185[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018556554714450614\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000122[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000259[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000294[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000272[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002564741424066597\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000115[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000142[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000117[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000112[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010336613213439705\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000057[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000080[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.348123456016764e-05\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000054[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000117[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000113[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010849263226191397\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000048[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000082[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000085[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.178613443305948e-05\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000057[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000057[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.318928963082726e-05\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.541103493873379e-05\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000057[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000072[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.313188398256898e-05\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000072[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000062[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.568082473881077e-05\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000215[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000157[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000143[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000142[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014983234668761725\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000092[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000163[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000127[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000134[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013057339301667526\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000228[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000133[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000111[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.541018007439562e-05\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000091[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000159[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000166[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000165116984044289\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000116[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000172[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000248[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00033652654237812383\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000641[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000287[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000199[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000162[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014636223559136852\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000073[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.149983412091387e-05\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000064[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.049438027024734e-05\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000100[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000077[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000084[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.030743585753953e-05\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000091[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011143800129502779\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000450[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000256[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000246[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000305[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003382702410817728\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000453[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000386[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000438[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000369[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00031989255930966467\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000095[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000165[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001717882262710191\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000165[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000169[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000182[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000155[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014214428529157885\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000101[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000063[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000063[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000067[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.658711954514729e-05\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000077[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.46530718060967e-05\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000046[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000098[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000098[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000084[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.832441169739469e-05\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000045[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000045[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000050[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.506687430170132e-05\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000069[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000051[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000049[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000049[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.888730500169913e-05\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000055[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000063[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000075[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.179369622463127e-05\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000059[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000059[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000091[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000111[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001050374417900457\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000109[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000097[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000097[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.700228327143122e-05\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000069[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000062[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000088[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000107[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001140284843131667\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000071[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000111[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000088[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.415080457984005e-05\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000099[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000091[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000098[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000117[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012157396231486928\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000127[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000093[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011419756856412278\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000067[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012168468256277266\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000171[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000117[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000106[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000112[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011368723708073957\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000175[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000107[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000103[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000106[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012945478829351487\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000096[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000229[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000236[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000210[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018820584755303572\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000058[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000074[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.406190652545775e-05\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000060[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000050[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.8597890711098446e-05\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000159[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000143[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000149[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011688870545185636\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000075[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000087[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000082[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.15930831777223e-05\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000036[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000073[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.202196279147756e-05\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000056[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000085[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000078[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000082[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.145658086708863e-05\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000195[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000089[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.146838652261067e-05\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000079[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.691192304264405e-05\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000063[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000071[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.97489223764569e-05\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000130[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000112[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000095[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.663219834692427e-05\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000077[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000079[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.0033428755778e-05\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.661855597689282e-05\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000076[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000074[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010430340325910948\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000130[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000143[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000161[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000157[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015668316582377883\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000510[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000234[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000191[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000200[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017518275290058228\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000122[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000096[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.764060678207898e-05\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000279[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000340[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000269[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000285[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002548946011302178\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000139[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000097[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010061569237223012\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000100[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000195[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000169[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012473306223910187\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000063[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000058[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.7095902002402e-05\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000118[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000111[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000107[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.732837297633523e-05\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000048[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000060[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000063[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000071[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.249242007674183e-05\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000191[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000074[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.088094343998819e-05\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000089[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000093[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000100[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000105[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.518548267806182e-05\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000064[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000112[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.648134464943723e-05\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000115[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000179[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000152[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013425831066342652\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000048[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.96693675511051e-05\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000127[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000125[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012209232909299317\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000248[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000254[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000220[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000180[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016083228420029627\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000050[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000056[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.611808883259073e-05\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000044[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000044[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000044[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.413495425978908e-05\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.706811182084493e-05\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000095[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.194562289849273e-05\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000084[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000049[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000051[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.685683451905789e-05\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000187[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000132[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000169[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000285[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003271214528467681\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000921[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000514[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000383[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000312[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00026465761875442694\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000086[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000071[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000098[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.93231003121764e-05\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000091[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.865615295690077e-05\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000076[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000062[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000067[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010888829515351973\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000135[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000106[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010313732409485964\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000148[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000074[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000094[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.34634907025611e-05\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000162[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000134[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000144[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014137264579403564\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000107[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010878255934585468\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000055[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.925153256815974e-05\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000063[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000057[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000092[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000131[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012566318473545836\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000041[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000102[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000113[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000108[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013011469945922727\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000286[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000159[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000247[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00028121185250711277\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000282[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000327[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000256[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000205[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018354128587816376\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000068[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000066[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000078[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010624100805216586\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000295[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000214[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000125[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010670443211893143\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000068[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000059[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000062[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.757450050827174e-05\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000057[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000051[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000075[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000086[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.465900418741512e-05\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000140[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000093[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.294792639593651e-05\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000080[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.6235699958051555e-05\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000055[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000048[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000051[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000048[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.5703596242674394e-05\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000038[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.7022209173519514e-05\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000040[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000039[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000047[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.6534432724220095e-05\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000204[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.905107016384136e-05\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000033[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.09101317927707e-05\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000045[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.956462389098305e-05\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000119[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000147[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000149[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000187[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00021116210582476923\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000393[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000272[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000216[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000181[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015309636555684846\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000074[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000050[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000052[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.953862816772016e-05\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000065[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.458895061565272e-05\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000143[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000099[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000101[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000090[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.167779462586623e-05\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000030[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000077[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000061[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.971385585326061e-05\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000073[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000086[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000153[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015250434307745309\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000256[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000138[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000149[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000122[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001105963241570862\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000077[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.317311319136933e-05\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000205[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000115[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.398096879820515e-05\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000026[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.4994042784528574e-05\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000045[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000042[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.83041564773157e-05\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000061[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000091[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000062[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.139516808616463e-05\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000064[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000072[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000143[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000134[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001270645431759476\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000080[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.75243426940142e-05\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000089[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000053[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.554676363317412e-05\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000088[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000065[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010346325584578153\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000059[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000077[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000091[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.760945784160867e-05\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000212[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000166[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000195[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000189[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000165468213162967\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000123[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000098[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000090[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.421275433647679e-05\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013545204983529403\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000356[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000289[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000331[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000288[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002522174777368491\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000103[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000073[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000063[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000066[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.14119758211018e-05\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000028[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.35808571587404e-05\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000033[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000052[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000086[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011700655668391846\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000216[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000105[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000062[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.164145908973296e-05\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000180[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000122[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000135[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000145[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015510412504227133\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000256[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000112[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000088[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.663019900974177e-05\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000140[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000097[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000116[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000137[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012939075377289556\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000045[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000054[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.234588201754377e-05\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000124[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000151[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000146[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013114302801113807\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000061[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000059[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.5313352822849994e-05\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000035[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000039[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000055[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.7315128333357276e-05\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000059[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000091[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000101[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000111467416263622\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000064[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000117[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.368211918219458e-05\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000145[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000103[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000122[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000117[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010300873250344012\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000086[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010766082536974863\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000350[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000190[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000153[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000130[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012086117021681275\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000063[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.428186470155197e-05\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000221[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000113[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000105[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.280919866796466e-05\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000053[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000053[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000072[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.619509267693501e-05\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.124814844952198e-05\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000130[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000191[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000171[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000144[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013776313280686737\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000211[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000214[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000231[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002364930004659982\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000156[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000221[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000224[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000196[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017845628235590993\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000163[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000107[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.239381956831494e-05\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000046[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000096[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000072[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000061[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.236201688807342e-05\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000051[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000075[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.632987870034413e-05\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000041[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000032[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000032[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.495152536743262e-05\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000050[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000046[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.04865194670856e-05\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000047[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000046[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000044[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.5537090863945194e-05\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000053[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000083[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000105[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014345306590257678\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000179[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000139[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000174[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000161[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014077964706302737\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000030[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000052[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.718847933167126e-05\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000038[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000036[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.183640721748816e-05\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000063[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000053[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000076[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.671830289837089e-05\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000084[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000128[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000111[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.673313461462385e-05\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000049[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000049[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.157469340701937e-05\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000044[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000049[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.936133282171795e-05\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000038[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.9848517801365236e-05\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000040[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000045[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.170332517787756e-05\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000046[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000072[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.322647650449653e-05\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000087[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000073[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000069[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000066[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.181932087405585e-05\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000105[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000162[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000219[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020615501953216153\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000193[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000096[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000113[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000247[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00025684821066533913\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000094[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000079[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000071[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.242113049665931e-05\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000066[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000063[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.073295717214933e-05\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000049[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000037[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000038[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.0694757242599735e-05\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000037[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000038[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.304689061882527e-05\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000028[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000035[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000049[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.595130733127007e-05\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000164[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000135[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000097[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.184739504031313e-05\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000055[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.0311976176308237e-05\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000063[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000049[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.746383685618639e-05\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000092[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000103[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000088[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.668745142836997e-05\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000074[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000142[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000143[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001357438779905351\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000357[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000311[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000266[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000260[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00033033724039341903\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000185[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000162[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000127[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000149[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014637604772360646\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000089[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000066[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.906191590838716e-05\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000058[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000046[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000051[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.87628821196995e-05\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000070[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000063[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.280387024162337e-05\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000111[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000095[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.148657223297051e-05\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000029[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000037[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.0792230629449476e-05\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.815018249042623e-05\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000037[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000029[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000031[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000034[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.7754991672045436e-05\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000039[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000065[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.011109603605291e-05\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000024[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000029[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000032[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000032[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.2796942625500375e-05\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000055[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.874888995298534e-05\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000165[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000163[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000177[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016107813744383747\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000051[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000056[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000047[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.273483573342673e-05\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000021[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000024[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000028[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000030[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.564246417226968e-05\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000087[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000066[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000085[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011471742291178089\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000185[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000110[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000093[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.081462858375743e-05\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000046[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000055[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.773810960112315e-05\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000062[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.256770384425182e-05\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000137[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000114[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000102[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.185095132124843e-05\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000137[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000080[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000073[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000066[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.806378869441687e-05\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000050[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000096[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000091[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000089[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001021041765852715\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000068[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.631221467614523e-05\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000068[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000047[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000040[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.5707236788293814e-05\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000047[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000061[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.096328859006462e-05\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000237[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000104[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.385202822864812e-05\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000137[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000101[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000158[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000200[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018594066677906084\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000209[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000200[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000194[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000202[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002472654282428266\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000090[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000208[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000202[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000165[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00015099506626938818\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000075[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000155[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000104[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.535390654491493e-05\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000039[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000054[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.028733961713442e-05\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000301[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000175[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000158[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000201[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018420908199914265\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000270[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000175[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000171[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000160[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00013697945523745149\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000044[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.2532036900411185e-05\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000018[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000041[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.9871132821645004e-05\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000029[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.9906616302687324e-05\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000119[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.3551684075282535e-05\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000036[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000037[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000049[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.041352285137691e-05\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000032[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000030[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000032[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.283413811914215e-05\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000018[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000025[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000132[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00016930442277498514\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000369[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000499[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000460[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000377[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032546673573961014\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000045[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000092[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000093[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000103[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010836409628609545\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000172[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000102[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000095[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.892255300452234e-05\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000041[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000075[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000060[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.51448329588311e-05\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000061[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000053[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.111435067803541e-05\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000039[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000038[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.507208996325062e-05\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000037[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.5344148045624024e-05\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000125[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.995116480335127e-05\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000065[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.0808616308349886e-05\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000044[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000035[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.688395040626347e-05\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000024[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000021[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000023[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.612862074329314e-05\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000030[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000026[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.3799931454959733e-05\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000018[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000020[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000024[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.322602460935741e-05\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000018[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000020[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000028[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.9795852401548473e-05\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000036[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000025[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000022[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000024[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.7506473497851402e-05\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000149[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000207[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000191[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00019332665356159852\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000123[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000116[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000116[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011313215827613021\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000040[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000146[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000193[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000178[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0002351128400732705\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000155[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000327[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000248[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000199[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00017169708444271238\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000073[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.20093415818701e-05\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000043[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000037[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.360255004234204e-05\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000103[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000067[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000053[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000044[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.914550372883241e-05\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000026[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000040[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000059[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.362080912367674e-05\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000243[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000109[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000078[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000071[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.530078612238867e-05\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000062[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000047[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.4278243123917494e-05\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000322[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000108[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000105[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010750099718279671\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000105[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000129[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000129[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000115[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.908489487315819e-05\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000040[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000032[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.424893009196239e-05\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000085[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.7567788508094965e-05\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000055[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000047[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000038[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.6199075975673625e-05\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000041[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000028[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000050[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000059[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.414304505393375e-05\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000047[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000045[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000108[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012260825158136868\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000131[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000137[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000122[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000133[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014247585413613707\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000125[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000149[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000075[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.026456269159098e-05\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000058[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000046[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.992169386037858e-05\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000040[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.454273366718553e-05\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000120[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000086[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000093[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000092[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.31424359603261e-05\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000033[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000041[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000037[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.524974904394185e-05\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000019[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000020[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000022[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.69573629743536e-05\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000045[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000065[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000064[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.38763747701887e-05\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000068[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000031[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.478795911178167e-05\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000025[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.030470540281385e-05\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000051[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.768179196616984e-05\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000214[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000143[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000151[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000115[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010038578880084969\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000154[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000159[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000248[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000399[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0004393024792534561\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000214[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000569[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000413[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000350[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00032282084630423923\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000191[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000120[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000096[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000078[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.008155889707269e-05\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000031[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000034[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000035[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.379191393833025e-05\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000050[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000077[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.002708088701183e-05\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000245[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000267[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000208[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000194[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000179876785750821\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000068[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000114[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000117[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.69251335845911e-05\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000069[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000067[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001198644621126732\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000128[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000180[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000154[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000134[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011386384471734345\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000038[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000032[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000035[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.37283905764707e-05\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000024[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000028[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.45067834814472e-05\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000050[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000043[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000037[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.3148540228467024e-05\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000015[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000028[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000036[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.596568751618179e-05\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000018[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000023[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000029[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.116297195902007e-05\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000078[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000059[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000045[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000050[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.74257962196134e-05\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000018[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000023[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000029[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000027[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.9812183061039833e-05\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000102[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000060[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.478710791066988e-05\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000030[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000030[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.8833288797613932e-05\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000012[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000042[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000031[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.069923361636029e-05\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000019[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000020[ 3100/ 4000]\n",
      "\n",
      "running train loss =   1.8746203272712593e-05\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000026[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.229944763916137e-05\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000013[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000017[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000019[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000020[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.0610115916497308e-05\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000019[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000038[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.7235708734660875e-05\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000025[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.1484581399563466e-05\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000099[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000078[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000076[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.264967420676839e-05\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000117[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000182[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000243[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000292[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0003540084923770337\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000807[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000585[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000747[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000668[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.000559187757971813\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000105[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000125[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000133[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000112[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.766913813109568e-05\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000042[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000042[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000042[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.287624051357852e-05\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000095[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000053[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000052[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.663983702106634e-05\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000034[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.1062759808264674e-05\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000067[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000039[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000036[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.559485403457074e-05\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000020[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000018[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000020[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.053369099712654e-05\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000020[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000027[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000028[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.710373175887071e-05\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000019[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000021[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.035604513821454e-05\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000038[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000022[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000024[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000035[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.6423904884941297e-05\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000026[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000035[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000037[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000038[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.32630074581175e-05\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000016[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000018[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000019[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.657395209553215e-05\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000035[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000025[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000028[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.043910990072618e-05\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000053[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000113[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000082[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.067272176253028e-05\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000099[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000087[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000068[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000077[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.007825706750737e-05\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000115[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000083[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000059[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.3426847284754333e-05\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000089[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000076[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000081[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.707959598519664e-05\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000089[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000205[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000182[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000153[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0001377554932332714\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000179[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000128[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000164[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000210[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00020791572887901566\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000093[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000106[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011430802692302677\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000139[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000177[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000123[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000109[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010501848769308708\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000049[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000032[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000031[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000027[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.627930525704869e-05\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000014[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000033[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000046[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.327228923557413e-05\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000022[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000034[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000030[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.6993592882718076e-05\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000035[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000055[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.565177377775399e-05\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000019[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000033[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000031[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.9247320981085068e-05\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000022[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000020[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000024[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.9063516831229207e-05\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000105[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000054[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000044[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000054[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.2889984362991524e-05\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000020[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000023[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000023[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.6268306874044357e-05\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000119[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000190[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000171[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014491350134449021\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000051[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000082[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000091[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000079[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.69963515974814e-05\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000034[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000064[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000072[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011209964122826932\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000091[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000072[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.487429652428546e-05\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000084[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000078[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000093[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000095[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010055927086796145\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000051[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000058[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000042[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000035[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.433289853091992e-05\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000053[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000049[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000045[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.596685621185315e-05\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000098[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000139[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000123[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00011584849094106175\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000048[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000053[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000086[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000096[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.896942031242361e-05\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000022[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000046[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000053[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.0889487713211565e-05\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000056[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000108[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000153[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014792194574511086\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000072[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000072[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000074[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000063[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.68487562759401e-05\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000028[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000085[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000089[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000069[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.866481137672963e-05\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000028[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000032[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000027[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.0137290127640882e-05\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000056[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000060[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000054[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.3083835655343134e-05\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000060[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000061[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000056[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.817428916794597e-05\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000037[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000090[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000080[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.261411437364587e-05\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000082[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000222[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000160[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000127[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010813920257533028\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000066[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000061[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000073[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.703149583699996e-05\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000040[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000048[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000053[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.76048319999245e-05\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000064[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000067[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000065[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.408885938071762e-05\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000079[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000044[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   5.118768237935001e-05\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000089[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000063[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000057[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.292375851444377e-05\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000028[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000089[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000141[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000222[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00025624143677305256\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000133[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000632[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000812[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000650[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.0005386585969972657\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000076[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000081[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000074[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.088357729116979e-05\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000047[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000059[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000055[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000044[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.059411999151053e-05\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000022[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000026[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000025[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000024[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.237692788185086e-05\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000021[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000026[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000027[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.8519134241378195e-05\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000018[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000052[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000046[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000043[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.352283285697922e-05\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000114[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000049[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000047[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000041[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.6130137368672875e-05\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000015[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000022[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000021[ 3100/ 4000]\n",
      "\n",
      "running train loss =   1.982443227461772e-05\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000014[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000014[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000015[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000016[ 3100/ 4000]\n",
      "\n",
      "running train loss =   1.6638169813631976e-05\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000073[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000041[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000032[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000026[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.6356909040714525e-05\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000033[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000021[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000021[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000023[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.1076379903206542e-05\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000031[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000047[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000050[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.925069122236891e-05\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000030[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000030[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000038[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000039[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.579000226636708e-05\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000027[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000040[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000034[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.327405552226992e-05\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000027[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000033[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.2679605669727604e-05\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000039[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000036[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000070[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00010241446807413013\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000227[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000169[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000112[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000091[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.2954347044506e-05\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000190[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000079[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000066[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000063[ 3100/ 4000]\n",
      "\n",
      "running train loss =   6.427829562198895e-05\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000033[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000046[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000062[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000078[ 3100/ 4000]\n",
      "\n",
      "running train loss =   8.816186641524837e-05\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000071[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000179[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000119[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000099[ 3100/ 4000]\n",
      "\n",
      "running train loss =   9.422209809599736e-05\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000036[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000035[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000031[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.8527986205517664e-05\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000029[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000023[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000020[ 3100/ 4000]\n",
      "\n",
      "running train loss =   2.218042525328201e-05\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000069[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000057[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000068[ 3100/ 4000]\n",
      "\n",
      "running train loss =   7.132182051918789e-05\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000056[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000082[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00012264174674783134\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000074[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000069[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000092[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000176[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018195535358245252\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000124[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000201[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000192[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000181[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00018500530513847479\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000372[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000219[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000190[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000168[ 3100/ 4000]\n",
      "\n",
      "running train loss =   0.00014346390648825035\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000043[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000042[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000037[ 3100/ 4000]\n",
      "\n",
      "running train loss =   3.248118005103606e-05\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/ 4000]\n",
      "Processing batch n. 11 ----> running loss: 0.000038[ 1100/ 4000]\n",
      "Processing batch n. 21 ----> running loss: 0.000047[ 2100/ 4000]\n",
      "Processing batch n. 31 ----> running loss: 0.000051[ 3100/ 4000]\n",
      "\n",
      "running train loss =   4.7820784106988865e-05\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002808[  100/  100]\n",
      "\n",
      "running validation loss =   0.0028083082288503647\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.029598[  100/  100]\n",
      "\n",
      "running validation loss =   0.029598301276564598\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001995[  100/  100]\n",
      "\n",
      "running validation loss =   0.0019953385926783085\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018982[  100/  100]\n",
      "\n",
      "running validation loss =   0.01898222416639328\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.018575[  100/  100]\n",
      "\n",
      "running validation loss =   0.018575044348835945\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005682[  100/  100]\n",
      "\n",
      "running validation loss =   0.005681633483618498\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002126[  100/  100]\n",
      "\n",
      "running validation loss =   0.0021257204934954643\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.008943[  100/  100]\n",
      "\n",
      "running validation loss =   0.008942973800003529\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.012124[  100/  100]\n",
      "\n",
      "running validation loss =   0.012123777531087399\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007424[  100/  100]\n",
      "\n",
      "running validation loss =   0.007424113340675831\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002092[  100/  100]\n",
      "\n",
      "running validation loss =   0.0020915225613862276\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002152[  100/  100]\n",
      "\n",
      "running validation loss =   0.0021518245339393616\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.005819[  100/  100]\n",
      "\n",
      "running validation loss =   0.005819285288453102\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.007232[  100/  100]\n",
      "\n",
      "running validation loss =   0.00723238755017519\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004722[  100/  100]\n",
      "\n",
      "running validation loss =   0.004721885081380606\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001681[  100/  100]\n",
      "\n",
      "running validation loss =   0.0016807684442028403\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001421[  100/  100]\n",
      "\n",
      "running validation loss =   0.001420629327185452\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003419[  100/  100]\n",
      "\n",
      "running validation loss =   0.0034193608444184065\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.004550[  100/  100]\n",
      "\n",
      "running validation loss =   0.004550454206764698\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.003353[  100/  100]\n",
      "\n",
      "running validation loss =   0.0033530776854604483\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001445[  100/  100]\n",
      "\n",
      "running validation loss =   0.0014452971518039703\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001006[  100/  100]\n",
      "\n",
      "running validation loss =   0.0010064000962302089\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002106[  100/  100]\n",
      "\n",
      "running validation loss =   0.0021056204568594694\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002943[  100/  100]\n",
      "\n",
      "running validation loss =   0.0029426778201013803\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.002365[  100/  100]\n",
      "\n",
      "running validation loss =   0.00236501800827682\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001173[  100/  100]\n",
      "\n",
      "running validation loss =   0.0011728985700756311\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000796[  100/  100]\n",
      "\n",
      "running validation loss =   0.0007959622307680547\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001430[  100/  100]\n",
      "\n",
      "running validation loss =   0.0014295604778453708\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001973[  100/  100]\n",
      "\n",
      "running validation loss =   0.0019726110622286797\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001629[  100/  100]\n",
      "\n",
      "running validation loss =   0.0016292104264721274\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000885[  100/  100]\n",
      "\n",
      "running validation loss =   0.0008847423014231026\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000668[  100/  100]\n",
      "\n",
      "running validation loss =   0.0006676246994175017\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001078[  100/  100]\n",
      "\n",
      "running validation loss =   0.0010779408039525151\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001378[  100/  100]\n",
      "\n",
      "running validation loss =   0.001378186629153788\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.001104[  100/  100]\n",
      "\n",
      "running validation loss =   0.0011044220300391316\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000645[  100/  100]\n",
      "\n",
      "running validation loss =   0.0006448767962865531\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000581[  100/  100]\n",
      "\n",
      "running validation loss =   0.0005812603048980236\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000865[  100/  100]\n",
      "\n",
      "running validation loss =   0.0008652720134705305\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000981[  100/  100]\n",
      "\n",
      "running validation loss =   0.0009809494949877262\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000736[  100/  100]\n",
      "\n",
      "running validation loss =   0.0007359472801908851\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000481[  100/  100]\n",
      "\n",
      "running validation loss =   0.00048122237785719335\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000528[  100/  100]\n",
      "\n",
      "running validation loss =   0.0005280838813632727\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000709[  100/  100]\n",
      "\n",
      "running validation loss =   0.0007085887482389808\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000692[  100/  100]\n",
      "\n",
      "running validation loss =   0.0006922255270183086\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000493[  100/  100]\n",
      "\n",
      "running validation loss =   0.0004932938027195632\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000395[  100/  100]\n",
      "\n",
      "running validation loss =   0.00039546642801724374\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000488[  100/  100]\n",
      "\n",
      "running validation loss =   0.0004883827059529722\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000563[  100/  100]\n",
      "\n",
      "running validation loss =   0.0005630012601613998\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000475[  100/  100]\n",
      "\n",
      "running validation loss =   0.0004745490732602775\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000354[  100/  100]\n",
      "\n",
      "running validation loss =   0.0003535009454935789\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000361[  100/  100]\n",
      "\n",
      "running validation loss =   0.0003611111897043884\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000434[  100/  100]\n",
      "\n",
      "running validation loss =   0.0004338204744271934\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000418[  100/  100]\n",
      "\n",
      "running validation loss =   0.0004179679963272065\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000327[  100/  100]\n",
      "\n",
      "running validation loss =   0.00032722833566367626\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000294[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002937242388725281\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000337[  100/  100]\n",
      "\n",
      "running validation loss =   0.00033701505162753165\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000350[  100/  100]\n",
      "\n",
      "running validation loss =   0.00035025508259423077\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000295[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002949258196167648\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000253[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002530022757127881\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000272[  100/  100]\n",
      "\n",
      "running validation loss =   0.00027160989702679217\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000290[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002899237733799964\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000259[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002588831994216889\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000222[  100/  100]\n",
      "\n",
      "running validation loss =   0.00022221184917725623\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000227[  100/  100]\n",
      "\n",
      "running validation loss =   0.00022656375949736685\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000241[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002409912704024464\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000223[  100/  100]\n",
      "\n",
      "running validation loss =   0.00022315632668323815\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000195[  100/  100]\n",
      "\n",
      "running validation loss =   0.00019491463899612427\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000193[  100/  100]\n",
      "\n",
      "running validation loss =   0.00019331857038196176\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000203[  100/  100]\n",
      "\n",
      "running validation loss =   0.0002028106537181884\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000191[  100/  100]\n",
      "\n",
      "running validation loss =   0.0001913743035402149\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000171[  100/  100]\n",
      "\n",
      "running validation loss =   0.00017051251779776067\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000167[  100/  100]\n",
      "\n",
      "running validation loss =   0.00016733567463234067\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000173[  100/  100]\n",
      "\n",
      "running validation loss =   0.00017281495092902333\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000164[  100/  100]\n",
      "\n",
      "running validation loss =   0.00016426506044808775\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000149[  100/  100]\n",
      "\n",
      "running validation loss =   0.00014896303764544427\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000146[  100/  100]\n",
      "\n",
      "running validation loss =   0.0001457950857002288\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000148[  100/  100]\n",
      "\n",
      "running validation loss =   0.00014829172869212925\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000141[  100/  100]\n",
      "\n",
      "running validation loss =   0.00014089481555856764\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000130[  100/  100]\n",
      "\n",
      "running validation loss =   0.0001296041300520301\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000127[  100/  100]\n",
      "\n",
      "running validation loss =   0.00012706991401501\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000127[  100/  100]\n",
      "\n",
      "running validation loss =   0.00012743324623443186\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000121[  100/  100]\n",
      "\n",
      "running validation loss =   0.00012069624062860385\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000113[  100/  100]\n",
      "\n",
      "running validation loss =   0.00011252421245444566\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000111[  100/  100]\n",
      "\n",
      "running validation loss =   0.00011062518024118617\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000110[  100/  100]\n",
      "\n",
      "running validation loss =   0.000109543398139067\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000103[  100/  100]\n",
      "\n",
      "running validation loss =   0.0001034288652590476\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000098[  100/  100]\n",
      "\n",
      "running validation loss =   9.764517744770274e-05\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000096[  100/  100]\n",
      "\n",
      "running validation loss =   9.625361417420208e-05\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000094[  100/  100]\n",
      "\n",
      "running validation loss =   9.420786227565259e-05\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000089[  100/  100]\n",
      "\n",
      "running validation loss =   8.897103543858975e-05\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000085[  100/  100]\n",
      "\n",
      "running validation loss =   8.511490159435198e-05\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000084[  100/  100]\n",
      "\n",
      "running validation loss =   8.38913838379085e-05\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000081[  100/  100]\n",
      "\n",
      "running validation loss =   8.127366891130805e-05\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000077[  100/  100]\n",
      "\n",
      "running validation loss =   7.697196997469291e-05\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000074[  100/  100]\n",
      "\n",
      "running validation loss =   7.438625470967963e-05\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000073[  100/  100]\n",
      "\n",
      "running validation loss =   7.304901373572648e-05\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000070[  100/  100]\n",
      "\n",
      "running validation loss =   7.022741920081899e-05\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000067[  100/  100]\n",
      "\n",
      "running validation loss =   6.698143988614902e-05\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000065[  100/  100]\n",
      "\n",
      "running validation loss =   6.526100332848728e-05\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000064[  100/  100]\n",
      "\n",
      "running validation loss =   6.367711466737092e-05\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000061[  100/  100]\n",
      "\n",
      "running validation loss =   6.098938320064917e-05\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000059[  100/  100]\n",
      "\n",
      "running validation loss =   5.8702178648673e-05\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000057[  100/  100]\n",
      "\n",
      "running validation loss =   5.739005791838281e-05\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000056[  100/  100]\n",
      "\n",
      "running validation loss =   5.5631371651543304e-05\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000053[  100/  100]\n",
      "\n",
      "running validation loss =   5.3334566473495215e-05\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000052[  100/  100]\n",
      "\n",
      "running validation loss =   5.168672214495018e-05\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000050[  100/  100]\n",
      "\n",
      "running validation loss =   5.038188464823179e-05\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000049[  100/  100]\n",
      "\n",
      "running validation loss =   4.8544268793193623e-05\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000047[  100/  100]\n",
      "\n",
      "running validation loss =   4.673606235883199e-05\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000045[  100/  100]\n",
      "\n",
      "running validation loss =   4.5488552132155746e-05\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000044[  100/  100]\n",
      "\n",
      "running validation loss =   4.4133867049822584e-05\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000042[  100/  100]\n",
      "\n",
      "running validation loss =   4.2479448893573135e-05\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000041[  100/  100]\n",
      "\n",
      "running validation loss =   4.115188858122565e-05\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000040[  100/  100]\n",
      "\n",
      "running validation loss =   4.003589128842577e-05\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000039[  100/  100]\n",
      "\n",
      "running validation loss =   3.867997293127701e-05\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000037[  100/  100]\n",
      "\n",
      "running validation loss =   3.7332491046981886e-05\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000036[  100/  100]\n",
      "\n",
      "running validation loss =   3.628460763138719e-05\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000035[  100/  100]\n",
      "\n",
      "running validation loss =   3.521363396430388e-05\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000034[  100/  100]\n",
      "\n",
      "running validation loss =   3.399510387680493e-05\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000033[  100/  100]\n",
      "\n",
      "running validation loss =   3.295594797236845e-05\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000032[  100/  100]\n",
      "\n",
      "running validation loss =   3.203554661013186e-05\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000031[  100/  100]\n",
      "\n",
      "running validation loss =   3.0993753171060234e-05\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000030[  100/  100]\n",
      "\n",
      "running validation loss =   2.9977072699693963e-05\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000029[  100/  100]\n",
      "\n",
      "running validation loss =   2.911157025664579e-05\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000028[  100/  100]\n",
      "\n",
      "running validation loss =   2.8222031687619165e-05\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000027[  100/  100]\n",
      "\n",
      "running validation loss =   2.727940955082886e-05\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000026[  100/  100]\n",
      "\n",
      "running validation loss =   2.645008498802781e-05\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000026[  100/  100]\n",
      "\n",
      "running validation loss =   2.5663048290880397e-05\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000025[  100/  100]\n",
      "\n",
      "running validation loss =   2.481766750861425e-05\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000024[  100/  100]\n",
      "\n",
      "running validation loss =   2.401976234978065e-05\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/  100]\n",
      "\n",
      "running validation loss =   2.330233110114932e-05\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000023[  100/  100]\n",
      "\n",
      "running validation loss =   2.256610241602175e-05\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000022[  100/  100]\n",
      "\n",
      "running validation loss =   2.1825353542226367e-05\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000021[  100/  100]\n",
      "\n",
      "running validation loss =   2.1154315618332475e-05\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/  100]\n",
      "\n",
      "running validation loss =   2.0493835108936764e-05\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000020[  100/  100]\n",
      "\n",
      "running validation loss =   1.9825492927338928e-05\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000019[  100/  100]\n",
      "\n",
      "running validation loss =   1.920264730870258e-05\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000019[  100/  100]\n",
      "\n",
      "running validation loss =   1.861360578914173e-05\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000018[  100/  100]\n",
      "\n",
      "running validation loss =   1.8001861462835222e-05\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/  100]\n",
      "\n",
      "running validation loss =   1.742129461490549e-05\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000017[  100/  100]\n",
      "\n",
      "running validation loss =   1.6877886082511395e-05\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/  100]\n",
      "\n",
      "running validation loss =   1.6335043255821802e-05\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000016[  100/  100]\n",
      "\n",
      "running validation loss =   1.580526259203907e-05\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000015[  100/  100]\n",
      "\n",
      "running validation loss =   1.5308069123420864e-05\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000015[  100/  100]\n",
      "\n",
      "running validation loss =   1.4823535821051337e-05\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000014[  100/  100]\n",
      "\n",
      "running validation loss =   1.4344288501888514e-05\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000014[  100/  100]\n",
      "\n",
      "running validation loss =   1.3888860848965123e-05\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000013[  100/  100]\n",
      "\n",
      "running validation loss =   1.3448186109599192e-05\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000013[  100/  100]\n",
      "\n",
      "running validation loss =   1.3017715900787152e-05\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000013[  100/  100]\n",
      "\n",
      "running validation loss =   1.2593708561325911e-05\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000012[  100/  100]\n",
      "\n",
      "running validation loss =   1.2196264833619352e-05\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000012[  100/  100]\n",
      "\n",
      "running validation loss =   1.1805186659330502e-05\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/  100]\n",
      "\n",
      "running validation loss =   1.142733253800543e-05\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/  100]\n",
      "\n",
      "running validation loss =   1.106468971556751e-05\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000011[  100/  100]\n",
      "\n",
      "running validation loss =   1.0709109119488858e-05\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000010[  100/  100]\n",
      "\n",
      "running validation loss =   1.0364296940679196e-05\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000010[  100/  100]\n",
      "\n",
      "running validation loss =   1.0032893442257773e-05\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000010[  100/  100]\n",
      "\n",
      "running validation loss =   9.708057405077852e-06\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000009[  100/  100]\n",
      "\n",
      "running validation loss =   9.393701475346461e-06\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000009[  100/  100]\n",
      "\n",
      "running validation loss =   9.094748747884296e-06\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000009[  100/  100]\n",
      "\n",
      "running validation loss =   8.801777767075691e-06\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000009[  100/  100]\n",
      "\n",
      "running validation loss =   8.514613000443205e-06\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000008[  100/  100]\n",
      "\n",
      "running validation loss =   8.239450835390016e-06\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000008[  100/  100]\n",
      "\n",
      "running validation loss =   7.973925676196814e-06\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000008[  100/  100]\n",
      "\n",
      "running validation loss =   7.716329491813667e-06\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000007[  100/  100]\n",
      "\n",
      "running validation loss =   7.476187420252245e-06\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000007[  100/  100]\n",
      "\n",
      "running validation loss =   7.233066753542516e-06\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000007[  100/  100]\n",
      "\n",
      "running validation loss =   6.987826964177657e-06\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000007[  100/  100]\n",
      "\n",
      "running validation loss =   6.7602145463752095e-06\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000007[  100/  100]\n",
      "\n",
      "running validation loss =   6.540720278280787e-06\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000006[  100/  100]\n",
      "\n",
      "running validation loss =   6.3262905314331874e-06\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000006[  100/  100]\n",
      "\n",
      "running validation loss =   6.11791028859443e-06\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000006[  100/  100]\n",
      "\n",
      "running validation loss =   5.915600468142657e-06\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000006[  100/  100]\n",
      "\n",
      "running validation loss =   5.719030923501123e-06\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000006[  100/  100]\n",
      "\n",
      "running validation loss =   5.529224381461972e-06\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   5.344400960893836e-06\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   5.165447873878293e-06\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   4.9914856390387286e-06\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   4.828345936402911e-06\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   4.664298103307374e-06\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000005[  100/  100]\n",
      "\n",
      "running validation loss =   4.5052720452076755e-06\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   4.354104021331295e-06\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   4.207335223327391e-06\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   4.065274879394565e-06\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   3.928963451471645e-06\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   3.795642669501831e-06\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   3.6661849662777968e-06\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000004[  100/  100]\n",
      "\n",
      "running validation loss =   3.540824764058925e-06\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   3.4195109037682414e-06\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   3.302640152469394e-06\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   3.1894328458292875e-06\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   3.079805082961684e-06\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.9764942155452445e-06\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.87607667814882e-06\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.7799451345345005e-06\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.6868999611906474e-06\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.597110096758115e-06\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000003[  100/  100]\n",
      "\n",
      "running validation loss =   2.5098665901168715e-06\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.425172624498373e-06\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.343020923945005e-06\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.263473106722813e-06\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.1862774701730814e-06\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.1117077722010436e-06\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   2.039853370661149e-06\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.970350467672688e-06\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.9025449091714108e-06\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.8376030084255035e-06\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.7770772728908923e-06\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.715723442430317e-06\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.6562937616981799e-06\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.6002118172764312e-06\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000002[  100/  100]\n",
      "\n",
      "running validation loss =   1.546042199151998e-06\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.493949866926414e-06\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.443571477466321e-06\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.3948744026492932e-06\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.3477610991685651e-06\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.3017913715884788e-06\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.257316512237594e-06\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.2145042092015501e-06\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.1731420954674832e-06\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.1330932920827763e-06\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.094302092496946e-06\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.0567333674771362e-06\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   1.0204178124695318e-06\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   9.861839771474479e-07\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   9.519592367723817e-07\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   9.186522333948233e-07\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   8.874365562405728e-07\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   8.570423801756988e-07\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   8.276696235043346e-07\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   7.992576911419746e-07\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   7.717713970123441e-07\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   7.45260138046433e-07\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   7.195947659965896e-07\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   6.947544193280919e-07\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   6.70777467348671e-07\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   6.475748364209721e-07\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   6.250484148040414e-07\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   6.031532961969788e-07\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   5.820861588290427e-07\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   5.617745273411856e-07\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   5.422907065621985e-07\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   5.239353413344361e-07\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000001[  100/  100]\n",
      "\n",
      "running validation loss =   5.053996687820472e-07\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.874663659393264e-07\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.7049178419911186e-07\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.5407975335365336e-07\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.3825198758895567e-07\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.2296778701711446e-07\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.080752944446431e-07\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.9376550375891384e-07\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.7990838563928264e-07\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.665254268980789e-07\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.5360633887648873e-07\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.411839202271949e-07\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2908229741224204e-07\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.1739196515445656e-07\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.06177014408604e-07\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.95289169116586e-07\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.847453970389324e-07\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7477071284920385e-07\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.648838233199058e-07\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5546236770424e-07\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.464320516537555e-07\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3760625822433212e-07\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2914181840860692e-07\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2093438190040615e-07\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1299896957316378e-07\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.053434258186826e-07\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9795332661942666e-07\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9078574098330137e-07\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8388456624052196e-07\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7726478063195827e-07\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.708103809505701e-07\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6460062113310414e-07\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5862296720570157e-07\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5283940513199923e-07\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4727847030826524e-07\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.420380328909232e-07\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.367910726912669e-07\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3180070368434826e-07\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2703019081072853e-07\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2239227942245634e-07\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1789106224568968e-07\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.136170766358191e-07\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.094894130915236e-07\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0544310669047263e-07\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0147947904215471e-07\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.764146113866445e-08\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.399091283057714e-08\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.044126159096777e-08\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.700722275989392e-08\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.368818527060284e-08\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.049055111314374e-08\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.739595986322456e-08\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.44497299365321e-08\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.159002990420049e-08\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.882996927970453e-08\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.615736936055328e-08\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.360102133839973e-08\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.111532968589017e-08\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.8765614596723026e-08\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.64955158210978e-08\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.429826899217005e-08\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.216059761892211e-08\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.009955117429854e-08\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.8142922537408595e-08\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.6232187855821394e-08\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.4397097553883214e-08\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.2653461207464716e-08\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.0943746171251405e-08\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.9321992773011516e-08\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.776387202947262e-08\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.624302991056538e-08\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.481096300106401e-08\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.342526611049834e-08\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2079011447194716e-08\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.077904864312586e-08\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.952383049148466e-08\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.8352324932257034e-08\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7217016196345867e-08\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6119250762235424e-08\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.505800900109989e-08\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4026327594128816e-08\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.305651847223089e-08\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.211640293126038e-08\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1210407652461072e-08\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0340070960855883e-08\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.950839312314656e-08\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8719026328994914e-08\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7938633689595918e-08\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.72005343301862e-08\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6496636945362297e-08\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5817679610563573e-08\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5173572620597042e-08\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4545547877276022e-08\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3945618881905375e-08\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3362575046471648e-08\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2812441774201488e-08\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2274890437424801e-08\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1764259788549225e-08\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.129011550915493e-08\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.083514789002038e-08\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0391798532793928e-08\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.96318494372872e-09\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.557303393137317e-09\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.168997117114941e-09\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.794967421010824e-09\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.447354815643848e-09\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.102849946567403e-09\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.770109000659886e-09\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.452552353015562e-09\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.148791336675231e-09\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.86140655403733e-09\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.5854020014910475e-09\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.317835143931916e-09\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.058208601444903e-09\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.8161164773196106e-09\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.578954631602073e-09\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.351353138394188e-09\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.130195379621227e-09\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.917178664243238e-09\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.715399182231295e-09\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.522032970299961e-09\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.336398351512116e-09\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.158467348247541e-09\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.9865728496124575e-09\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.825871175422435e-09\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.6663041491635795e-09\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.517429458810284e-09\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.3730731541226078e-09\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2318832055011626e-09\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.096763290244553e-09\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.970263146551133e-09\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.847525326643563e-09\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7300364191518156e-09\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.617165595353299e-09\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5086046573363774e-09\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4031121537149147e-09\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.30525198929854e-09\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.210387428647209e-09\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1174348940888876e-09\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0296020419863225e-09\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.944312266743964e-09\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8641930221718894e-09\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7870143143028372e-09\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7124860418604726e-09\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6410234282560054e-09\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5707882772275639e-09\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5067982417349413e-09\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.444954489393524e-09\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.384459102915514e-09\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3251602037911425e-09\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2705069218910126e-09\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2172977070790125e-09\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1662307786153292e-09\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.116377768006771e-09\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0710461406659988e-09\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.026025930883634e-09\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.827965108399894e-10\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.415879187457676e-10\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.017867563798632e-10\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.628463499249506e-10\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.277492025143829e-10\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.932118850639824e-10\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.588227823873694e-10\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.259786105606736e-10\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.952904363366486e-10\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.660476059572318e-10\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.379550221424779e-10\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.105658201249753e-10\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.84606363318585e-10\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.603771335849217e-10\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.36845123910723e-10\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.136358560697829e-10\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.917913853930145e-10\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.710025702792109e-10\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.506513218149877e-10\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.315856838577048e-10\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.131195385781439e-10\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.9519684746913697e-10\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.7803704611150124e-10\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.616960897456778e-10\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.4605929233322286e-10\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.3151134615216904e-10\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.172927476313703e-10\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.030540818294014e-10\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.90476365183423e-10\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7826366211236575e-10\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6602567371192265e-10\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.546915456314025e-10\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.440179724949587e-10\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.328321702105285e-10\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2284168954556094e-10\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1316352849520825e-10\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0405907519283062e-10\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9531690154117598e-10\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8671419965698988e-10\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7848723887770035e-10\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.70721617398506e-10\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6354279042118947e-10\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5651453744158772e-10\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4976272999511764e-10\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4305599760344734e-10\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3681705768320285e-10\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3086101646742065e-10\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2521195191794732e-10\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2015806405418772e-10\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.148029518671656e-10\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0967746849610549e-10\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0486519985697385e-10\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0001745959220543e-10\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.591952232490542e-11\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.184747407076088e-11\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.761005809709843e-11\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.378633897798693e-11\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.023672698476148e-11\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.670655777225477e-11\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.325474254971098e-11\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.02430058563408e-11\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.70458341622826e-11\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.413598124810349e-11\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.125589618877214e-11\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.856392454317572e-11\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.5970592743870284e-11\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.3587512899300194e-11\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.128812999299903e-11\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.908910083978313e-11\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.6973595152488556e-11\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.489963609244363e-11\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.28911212102534e-11\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.117542765080806e-11\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.941896462023031e-11\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.7650535467115276e-11\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.575198123662027e-11\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.44410576447185e-11\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.299205353357593e-11\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.1464587879614925e-11\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.022497876981056e-11\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.8952706551677032e-11\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7675353328482366e-11\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.632331158602952e-11\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5277566498282944e-11\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4187818414844742e-11\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3179704683462532e-11\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2092393886508077e-11\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1162653227047024e-11\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0262706443285872e-11\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9439198514770162e-11\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.861967177663182e-11\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.777042575590304e-11\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7010313296816904e-11\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6311880665642597e-11\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.561107666747663e-11\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4940548798136888e-11\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.439297812877438e-11\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3844331930856768e-11\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3235627004115624e-11\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2653942130236295e-11\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2157791266786955e-11\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1679150702104124e-11\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1235973089440687e-11\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0747358732132728e-11\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0317195882347807e-11\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.91686940898795e-12\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.482150307993376e-12\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.085562163724248e-12\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.753201288791423e-12\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.435259435390918e-12\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.128139654395294e-12\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.806750038086285e-12\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.504009566505765e-12\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.200505382914946e-12\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.911988342794828e-12\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.658445062673879e-12\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.3775391218046096e-12\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.144148557984952e-12\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.908554027989066e-12\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.697005801136301e-12\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.466508322393704e-12\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.294044282816435e-12\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.1033261817823306e-12\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.9470072135959775e-12\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.743158990572205e-12\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.580869537862409e-12\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.393846831513004e-12\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.2661429944246265e-12\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.124302462049645e-12\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.996740872286297e-12\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.854888196846984e-12\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.721868733347344e-12\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.60809277420715e-12\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.4840294711735442e-12\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.354873670535774e-12\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2599838627189737e-12\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.1741198211476807e-12\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.041746758983277e-12\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.9380267749928857e-12\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.865267918761294e-12\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.780888800485437e-12\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.679052509349522e-12\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5985585211385498e-12\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.522596498008567e-12\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4466756745611384e-12\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.37446954459708e-12\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3032088389274286e-12\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.215614193848414e-12\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.138413578156584e-12\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0811002660742206e-12\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0371707792898874e-12\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9743178446274268e-12\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.930793849455603e-12\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.861428546023891e-12\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8141731606552414e-12\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7609068500221148e-12\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7212822963838526e-12\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6825113351159882e-12\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.620964647230938e-12\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5830943327077573e-12\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5522121348671147e-12\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4957952411409625e-12\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.465223233541868e-12\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.41385904720015e-12\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3797060282455531e-12\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3411049614581172e-12\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3141454070775271e-12\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2776346820786433e-12\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.230915109423636e-12\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2037142116394506e-12\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1833797830540504e-12\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1423650653902273e-12\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1091413161176678e-12\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0808364836814577e-12\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0452616419978633e-12\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0223895298075436e-12\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.918059212452035e-13\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.615511512017783e-13\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.422243801154861e-13\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.217164791824883e-13\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.1369359994653e-13\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.899369956238967e-13\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.63099739248363e-13\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.572277002821815e-13\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.311964940117822e-13\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.106433818481917e-13\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.907144448753012e-13\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.681699785815066e-13\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.515402146093808e-13\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.418350872825941e-13\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.183750664642441e-13\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.1056008299486e-13\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.949048541454728e-13\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.705490112821066e-13\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.565207904228598e-13\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.367261318890316e-13\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.212604757895035e-13\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.045555766068245e-13\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.942741958253617e-13\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.829817423079475e-13\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.670116611476705e-13\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.54021943339339e-13\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.371619490358859e-13\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.264755103227825e-13\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.252621254614454e-13\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.151331292953254e-13\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.012975169520206e-13\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.888461055223281e-13\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.755802956307731e-13\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.605907127051834e-13\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.565299960984648e-13\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.5017817056590415e-13\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.425322142152105e-13\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.291415584434938e-13\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.289258835263321e-13\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.279440029338749e-13\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.1902494928719447e-13\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.076857937612005e-13\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.929927672348399e-13\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.8526668834359956e-13\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.775523188358221e-13\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.7063082637678324e-13\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.623161070210462e-13\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.5894548509711466e-13\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.5638961400075164e-13\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.444497833661464e-13\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.3825042376409153e-13\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.30182712768301e-13\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2451480196614424e-13\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.256094124794856e-13\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.154624456623567e-13\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.0930309312046655e-13\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.068185896321074e-13\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.028042497733169e-13\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.899465335794854e-13\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.831881322322427e-13\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7702292499862113e-13\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.72722057610697e-13\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.7258834837777524e-13\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.636884580044935e-13\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5525827111738675e-13\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.521228126446673e-13\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.486123557455394e-13\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.41541541542295e-13\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4091340901366554e-13\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3349288539968593e-13\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.301254889772175e-13\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2557692205046193e-13\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2428346885868672e-13\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2309691800111858e-13\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1683440362251644e-13\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1140162916651745e-13\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0792982407483868e-13\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0532398481085123e-13\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9882328476748118e-13\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9628989731367008e-13\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.898538970349431e-13\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8890593841795755e-13\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.847260137823742e-13\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.840244943191946e-13\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7998623424997362e-13\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.809727769117725e-13\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7900022013673383e-13\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7280641707081296e-13\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7334647172545514e-13\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7143051030382023e-13\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.663410078081573e-13\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6298629655110697e-13\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6144019714808833e-13\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5700524105648206e-13\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5613014082548754e-13\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.558474486615391e-13\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.563506675473711e-13\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4759044951895978e-13\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4893659493631778e-13\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4267048913801927e-13\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4272406227786721e-13\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3870342049394913e-13\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.380927300677695e-13\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3591912155237057e-13\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3229036464369737e-13\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2995035819987621e-13\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.294886371521961e-13\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2677439122348727e-13\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2825137273800996e-13\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2562558415404879e-13\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2413614786706967e-13\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.217162763807178e-13\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1981877350106945e-13\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1792258521655524e-13\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.164034824476315e-13\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1371265528584834e-13\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1082114230193812e-13\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0651208918761174e-13\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.060125565891662e-13\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0330875288263111e-13\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0119849565542322e-13\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.886841643774388e-14\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0024564457613433e-13\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0072665086995752e-13\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.740239891769045e-14\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.831493122869003e-14\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.554499796589691e-14\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.408987667767696e-14\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.332274265427126e-14\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.250506448083701e-14\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.178917933887198e-14\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.036150191814307e-14\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.72031600033206e-14\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.498573616762609e-14\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.417176461036902e-14\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.549088628857782e-14\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.426297989439294e-14\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.226957255632467e-14\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.076095266715613e-14\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.976196200916441e-14\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.893883571981342e-14\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.783319344936701e-14\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.584117524533224e-14\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.390067697328126e-14\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.321733822528154e-14\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.083746702283081e-14\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.067870594346104e-14\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.284468438607111e-14\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.088240042661675e-14\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.91870876996098e-14\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.91845872583495e-14\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.79044290957144e-14\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.720423100393252e-14\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.524475241759947e-14\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.405931286725813e-14\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.167139823994028e-14\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.125696873577127e-14\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.254913443746665e-14\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.328916340656021e-14\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.258542132892703e-14\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.104637601629312e-14\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.123914038629746e-14\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.116933131891655e-14\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.203932902343681e-14\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.097302973932447e-14\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.946576510287155e-14\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.693459348725047e-14\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.6685386229772886e-14\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.566658177707899e-14\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.4436005535044366e-14\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.399878406833064e-14\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.2780001913053584e-14\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.2752524164244655e-14\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.0665790736047905e-14\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.185428638126294e-14\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.210984299771314e-14\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.020213506511627e-14\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.103958976380302e-14\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.9964339035504096e-14\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.967318331834669e-14\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.9074320860242535e-14\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.719249147825018e-14\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.721220024086689e-14\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.8253451066663025e-14\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.821792311672339e-14\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.8900848512644854e-14\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.6825841408569893e-14\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.591351577360944e-14\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.4679398933855294e-14\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.494821330999592e-14\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.398564845686792e-14\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.4330926191223086e-14\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.3527403632403344e-14\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.357347883660219e-14\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.163405799046012e-14\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.005171235356257e-14\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.996011759877828e-14\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.9370728357156215e-14\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.905691620272565e-14\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.0240831093761933e-14\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   4.013452845888152e-14\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.853219962069235e-14\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.736802398545888e-14\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.814435001040818e-14\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.7528174198862146e-14\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.776659703285529e-14\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.67166014389117e-14\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.74386631451281e-14\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.5696865249975826e-14\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.545636210306423e-14\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.493775093451473e-14\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.398930781468336e-14\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.262373146151534e-14\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.225180607063956e-14\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.2600973380288514e-14\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.194694874852737e-14\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.183585529529728e-14\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   3.11142814800585e-14\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.9715399791404445e-14\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.8168615985764422e-14\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.793130445899808e-14\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5875519920628108e-14\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5991953069557684e-14\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6151271496606746e-14\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6310310402783213e-14\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6082021470971023e-14\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6419111784858028e-14\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.562783562838969e-14\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.5465603408002073e-14\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.6032649614541464e-14\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.581199414364993e-14\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.59262080662577e-14\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4708154359315282e-14\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.367967170816606e-14\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3806237065210692e-14\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3704859079885614e-14\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3550538147225353e-14\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.464160975691309e-14\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4638001396557786e-14\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.418489297988536e-14\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.4294942888524428e-14\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3873613453967088e-14\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.438344936118303e-14\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3657084727594577e-14\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.377740575775205e-14\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.3568371578896845e-14\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2686300266746423e-14\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.309319625988255e-14\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.1554427079989447e-14\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.081099473251382e-14\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.091941494976237e-14\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.178108632041112e-14\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.2048650475921597e-14\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.124928854293877e-14\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.043008401613356e-14\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.056344935367875e-14\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0597310342778188e-14\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   2.0144339145443217e-14\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9983843342597472e-14\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8871677224391314e-14\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.913521627933644e-14\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9130637219223583e-14\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.965438479556523e-14\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9021558010342068e-14\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9184760930487238e-14\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.854291324437582e-14\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9079288387895132e-14\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9641338794111618e-14\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.9612751432141785e-14\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8959384097947708e-14\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.932797895527489e-14\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8797848138642735e-14\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.827062942128324e-14\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8204155969648618e-14\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.8352508708162525e-14\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.805358231074701e-14\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.829227958341506e-14\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.802582673513138e-14\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.685898125204817e-14\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6806800634365515e-14\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6096258576231773e-14\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5454271976717004e-14\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.55461428642421e-14\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5300782830410947e-14\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6319968447931106e-14\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.6078355687858606e-14\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5363095656208657e-14\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.705452050198772e-14\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.7497220577804284e-14\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5968097409114514e-14\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.625737948939259e-14\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4676421292462646e-14\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.5081514799487016e-14\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3663204716121789e-14\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3478352480877746e-14\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2894236866385286e-14\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.242797741617379e-14\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2873315999621052e-14\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2086271619757196e-14\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2172591900378935e-14\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2802088152053491e-14\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.237978717070565e-14\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.30495284988031e-14\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3892466041757427e-14\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.335581137736552e-14\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3108925837196361e-14\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3218837679465027e-14\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4055807028272999e-14\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.50439759933306e-14\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.473200190039558e-14\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4496496247872782e-14\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4801251926031302e-14\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3960050800618847e-14\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4922336979907198e-14\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.485842834403686e-14\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3359836477930872e-14\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3271573103728132e-14\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.322005232471139e-14\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2486610730848627e-14\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.360294001599054e-14\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3555131782381612e-14\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.4183309203757294e-14\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3860234744048506e-14\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.3217970317727038e-14\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2714276246411638e-14\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2656752544897704e-14\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2873940262903179e-14\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2054040322048275e-14\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2592082426407785e-14\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2640793597138485e-14\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2303424609411834e-14\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.2151948013388453e-14\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0917137453650503e-14\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1468432235502753e-14\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1102613952176672e-14\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0855172758394115e-14\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1087070897594555e-14\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0726317871293054e-14\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0093768065843646e-14\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0398802417841812e-14\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0335380825916146e-14\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.1293087098058693e-14\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   1.0802056169304745e-14\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.459796092075798e-15\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.764274790460565e-15\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.709318445809759e-15\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.951730483719618e-15\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.774094969186182e-15\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.676394800918082e-15\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.810801141955447e-15\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.895422800253092e-15\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.666716281197906e-15\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.462191179985784e-15\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.619044215390237e-15\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.125377845872531e-15\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.381875194662282e-15\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.796787609862848e-15\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.24587304355229e-15\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.985941925445995e-15\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.286290388928325e-15\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.374552916098118e-15\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.141927701364084e-15\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.654851043672768e-15\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.637087915735898e-15\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.95658874344022e-15\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.085097194065853e-15\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.640453177635339e-15\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.746340766371599e-15\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.393739059386375e-15\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   9.016574243963724e-15\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.910131848647012e-15\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.840188103027488e-15\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.287296664069663e-15\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.689752510496283e-15\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.791338018893438e-15\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.04443537743291e-15\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.652942678442604e-15\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.877138747053661e-15\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.794010933610124e-15\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.002282885064425e-15\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.186718803614892e-15\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.391659272236017e-15\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.351829241957278e-15\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.375421650637152e-15\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.232619519026602e-15\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.698808665503477e-15\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.618595492431442e-15\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.215861819198123e-15\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.17363892084339e-15\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.229150072074648e-15\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.064351341856852e-15\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.525509959659983e-15\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.942745683983243e-15\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.814376146760959e-15\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.136585464565751e-15\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   8.264571617127778e-15\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.91263112160951e-15\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.94968457788715e-15\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.835886548456482e-15\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.743982728898068e-15\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.9837908852764426e-15\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.811256305402997e-15\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.537897495248728e-15\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.287958401304718e-15\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.182070812568458e-15\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.809346990888261e-15\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.974215177807733e-15\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.303501455886835e-15\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.405503280977566e-15\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.6137395547964575e-15\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.1739524252854994e-15\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.089020431664311e-15\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.392804986548803e-15\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.068481153242815e-15\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.880749429494582e-15\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.42333766193363e-15\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.968179323277223e-15\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.636533634923544e-15\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.463339114132563e-15\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.556597865141735e-15\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.404774832642927e-15\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.96876790892394e-15\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   6.90701200728905e-15\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   7.447447403241027e-15\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 0.000000[  100/  100]\n",
      "\n",
      "running validation loss =   5.960304034450126e-15\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define just one learning rate\n",
    "\n",
    "# define the neural network model\n",
    "d = dataset[\"X_train\"].shape[1]\n",
    "p = 1000 \n",
    "\n",
    "model = NeuralNetwork(d,p)\n",
    "\n",
    "\n",
    "# define the hyper-parameters of the network training\n",
    "epochs = 1000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# train the network\n",
    "\n",
    "train_loop(train_dataloader, model, epochs = epochs, learning_rate = learning_rate,datatype = \"train\")\n",
    "plot_loss(learning_rate,\"train\")\n",
    "\n",
    "#validation set\n",
    "train_loop(val_dataloader, model, epochs = epochs, learning_rate = learning_rate, datatype = \"validation\")\n",
    "plot_loss(learning_rate,\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_dataloader, model):\n",
    "\n",
    "  # initialize the variable for test loss \n",
    "  test_loss = 0.\n",
    "\n",
    "  # select which loss function to use\n",
    "  l = torch.nn.MSELoss()\n",
    "\n",
    "  # extract the size of the test set\n",
    "  size = len(test_dataloader.dataset)\n",
    "\n",
    "  # create the container to avoid gradient computation\n",
    "  with torch.no_grad():\n",
    "    # loop over all test input-output pairs\n",
    "    for X, y in test_dataloader: \n",
    "        \n",
    "        y = torch.reshape(y,(y.shape[0],1))\n",
    "        \n",
    "        pred = model(X) # get predictions through forward pass\n",
    "        loss = l(pred, y) # compute the loss\n",
    "\n",
    "        \n",
    "    # compute the metrics of interest    \n",
    "        test_loss += loss.item()\n",
    "\n",
    "  test_loss = test_loss/len(test_dataloader)\n",
    "  \n",
    "  print(\"test loss = \", test_loss)\n",
    "  return X, y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss =  0.004515015741344541\n"
     ]
    }
   ],
   "source": [
    "#Finally, evaluate the performance of this model on the test set: compute the mean square error on the test set.\n",
    "\n",
    "X, y, pred = test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "Plot the energies predicted by this model on the test set vs the true energies in kcal/mol units\n",
    "and compute the unnormalized root mean square error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the energies predicted by this model on the test set vs the true energies in kcal/mol units\n",
    "x1 = np.linspace(-2.5,2,1000)\n",
    "y1 = x1\n",
    "\n",
    "plt.scatter(y,pred)\n",
    "plt.plot(x1,y1,label = \"y = x\", color = \"red\")\n",
    "plt.legend()\n",
    "plt.xlabel('Atomization energies kcal/mol')\n",
    "plt.ylabel('Prediction by model')\n",
    "plt.title('Energies predicted by this model on the test set vs the true energies')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unnormalized RMSE is 15.462437629699707\n"
     ]
    }
   ],
   "source": [
    "#compute the unnormalized root mean square error on the test set.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Use the unnormalized predictions and labels. \n",
    "y = (y * std_y_train) +  mean_y_train\n",
    "pred = (pred * std_y_train) +  mean_y_train\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y, pred))\n",
    "\n",
    "print(f\"The unnormalized RMSE is {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "Retrain the neural network, varying the number of training examples n ∈ {100, 200, 500, 1000}\n",
    "and using the same architecture, number of epochs, loss, optimizer, and the learning rate you selected\n",
    "in question 5. Plot the learning curve , i.e., unnormalized mean squared error on the test set $ε_T$ as a\n",
    "function of the size of the training set n on a log-log scale. What type of curve do you observe? Identify\n",
    "what function fits well $ε_T$ (n), e.g., A exp (−βn), $A (log_{10} n)^{−β}$ ,$ A n^{−β}$ , −β n + A. Find the values of\n",
    "the parameters A, β by manual inspection or using another curve fitting method. Estimate the test\n",
    "error that this model would achieve with n = 4000 training points, i.e., compute $ε_T$(n = 4000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2436372.750000[  100/  100]\n",
      "\n",
      "running train loss =   2436372.75\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2436018.000000[  100/  100]\n",
      "\n",
      "running train loss =   2436018.0\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2435663.750000[  100/  100]\n",
      "\n",
      "running train loss =   2435663.75\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2435308.750000[  100/  100]\n",
      "\n",
      "running train loss =   2435308.75\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2434952.750000[  100/  100]\n",
      "\n",
      "running train loss =   2434952.75\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2434596.250000[  100/  100]\n",
      "\n",
      "running train loss =   2434596.25\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2434239.250000[  100/  100]\n",
      "\n",
      "running train loss =   2434239.25\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2433880.500000[  100/  100]\n",
      "\n",
      "running train loss =   2433880.5\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2433520.250000[  100/  100]\n",
      "\n",
      "running train loss =   2433520.25\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2433158.500000[  100/  100]\n",
      "\n",
      "running train loss =   2433158.5\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2432795.500000[  100/  100]\n",
      "\n",
      "running train loss =   2432795.5\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2432430.750000[  100/  100]\n",
      "\n",
      "running train loss =   2432430.75\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2432063.750000[  100/  100]\n",
      "\n",
      "running train loss =   2432063.75\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2431694.500000[  100/  100]\n",
      "\n",
      "running train loss =   2431694.5\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2431322.750000[  100/  100]\n",
      "\n",
      "running train loss =   2431322.75\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2430948.250000[  100/  100]\n",
      "\n",
      "running train loss =   2430948.25\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2430571.000000[  100/  100]\n",
      "\n",
      "running train loss =   2430571.0\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2430190.500000[  100/  100]\n",
      "\n",
      "running train loss =   2430190.5\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2429806.750000[  100/  100]\n",
      "\n",
      "running train loss =   2429806.75\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2429420.250000[  100/  100]\n",
      "\n",
      "running train loss =   2429420.25\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2429029.750000[  100/  100]\n",
      "\n",
      "running train loss =   2429029.75\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2428635.250000[  100/  100]\n",
      "\n",
      "running train loss =   2428635.25\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2428237.000000[  100/  100]\n",
      "\n",
      "running train loss =   2428237.0\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2427834.000000[  100/  100]\n",
      "\n",
      "running train loss =   2427834.0\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2427427.000000[  100/  100]\n",
      "\n",
      "running train loss =   2427427.0\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2427015.250000[  100/  100]\n",
      "\n",
      "running train loss =   2427015.25\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2426599.000000[  100/  100]\n",
      "\n",
      "running train loss =   2426599.0\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2426178.000000[  100/  100]\n",
      "\n",
      "running train loss =   2426178.0\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2425752.250000[  100/  100]\n",
      "\n",
      "running train loss =   2425752.25\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2425321.500000[  100/  100]\n",
      "\n",
      "running train loss =   2425321.5\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2424885.500000[  100/  100]\n",
      "\n",
      "running train loss =   2424885.5\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2424443.750000[  100/  100]\n",
      "\n",
      "running train loss =   2424443.75\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2423996.750000[  100/  100]\n",
      "\n",
      "running train loss =   2423996.75\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2423544.750000[  100/  100]\n",
      "\n",
      "running train loss =   2423544.75\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2423086.750000[  100/  100]\n",
      "\n",
      "running train loss =   2423086.75\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2422623.250000[  100/  100]\n",
      "\n",
      "running train loss =   2422623.25\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2422154.250000[  100/  100]\n",
      "\n",
      "running train loss =   2422154.25\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2421679.250000[  100/  100]\n",
      "\n",
      "running train loss =   2421679.25\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2421199.000000[  100/  100]\n",
      "\n",
      "running train loss =   2421199.0\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2420712.750000[  100/  100]\n",
      "\n",
      "running train loss =   2420712.75\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2420220.750000[  100/  100]\n",
      "\n",
      "running train loss =   2420220.75\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2419723.000000[  100/  100]\n",
      "\n",
      "running train loss =   2419723.0\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2419219.000000[  100/  100]\n",
      "\n",
      "running train loss =   2419219.0\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2418709.000000[  100/  100]\n",
      "\n",
      "running train loss =   2418709.0\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2418193.500000[  100/  100]\n",
      "\n",
      "running train loss =   2418193.5\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2417672.000000[  100/  100]\n",
      "\n",
      "running train loss =   2417672.0\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2417144.250000[  100/  100]\n",
      "\n",
      "running train loss =   2417144.25\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2416611.000000[  100/  100]\n",
      "\n",
      "running train loss =   2416611.0\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2416071.250000[  100/  100]\n",
      "\n",
      "running train loss =   2416071.25\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2415525.000000[  100/  100]\n",
      "\n",
      "running train loss =   2415525.0\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2414973.500000[  100/  100]\n",
      "\n",
      "running train loss =   2414973.5\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2414415.750000[  100/  100]\n",
      "\n",
      "running train loss =   2414415.75\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2413851.250000[  100/  100]\n",
      "\n",
      "running train loss =   2413851.25\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2413280.750000[  100/  100]\n",
      "\n",
      "running train loss =   2413280.75\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2412704.000000[  100/  100]\n",
      "\n",
      "running train loss =   2412704.0\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2412120.750000[  100/  100]\n",
      "\n",
      "running train loss =   2412120.75\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2411531.000000[  100/  100]\n",
      "\n",
      "running train loss =   2411531.0\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2410934.500000[  100/  100]\n",
      "\n",
      "running train loss =   2410934.5\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2410331.250000[  100/  100]\n",
      "\n",
      "running train loss =   2410331.25\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2409721.500000[  100/  100]\n",
      "\n",
      "running train loss =   2409721.5\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2409105.500000[  100/  100]\n",
      "\n",
      "running train loss =   2409105.5\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2408482.250000[  100/  100]\n",
      "\n",
      "running train loss =   2408482.25\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2407852.750000[  100/  100]\n",
      "\n",
      "running train loss =   2407852.75\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2407216.500000[  100/  100]\n",
      "\n",
      "running train loss =   2407216.5\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2406573.500000[  100/  100]\n",
      "\n",
      "running train loss =   2406573.5\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2405923.250000[  100/  100]\n",
      "\n",
      "running train loss =   2405923.25\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2405266.500000[  100/  100]\n",
      "\n",
      "running train loss =   2405266.5\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2404603.000000[  100/  100]\n",
      "\n",
      "running train loss =   2404603.0\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2403932.500000[  100/  100]\n",
      "\n",
      "running train loss =   2403932.5\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2403255.250000[  100/  100]\n",
      "\n",
      "running train loss =   2403255.25\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2402571.000000[  100/  100]\n",
      "\n",
      "running train loss =   2402571.0\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2401879.750000[  100/  100]\n",
      "\n",
      "running train loss =   2401879.75\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2401181.500000[  100/  100]\n",
      "\n",
      "running train loss =   2401181.5\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2400476.250000[  100/  100]\n",
      "\n",
      "running train loss =   2400476.25\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2399764.000000[  100/  100]\n",
      "\n",
      "running train loss =   2399764.0\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2399044.750000[  100/  100]\n",
      "\n",
      "running train loss =   2399044.75\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2398318.000000[  100/  100]\n",
      "\n",
      "running train loss =   2398318.0\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2397584.250000[  100/  100]\n",
      "\n",
      "running train loss =   2397584.25\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2396843.000000[  100/  100]\n",
      "\n",
      "running train loss =   2396843.0\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2396094.750000[  100/  100]\n",
      "\n",
      "running train loss =   2396094.75\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2395339.000000[  100/  100]\n",
      "\n",
      "running train loss =   2395339.0\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2394575.750000[  100/  100]\n",
      "\n",
      "running train loss =   2394575.75\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2393805.000000[  100/  100]\n",
      "\n",
      "running train loss =   2393805.0\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2393027.250000[  100/  100]\n",
      "\n",
      "running train loss =   2393027.25\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2392242.000000[  100/  100]\n",
      "\n",
      "running train loss =   2392242.0\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2391448.750000[  100/  100]\n",
      "\n",
      "running train loss =   2391448.75\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2390648.250000[  100/  100]\n",
      "\n",
      "running train loss =   2390648.25\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2389839.750000[  100/  100]\n",
      "\n",
      "running train loss =   2389839.75\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2389023.750000[  100/  100]\n",
      "\n",
      "running train loss =   2389023.75\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2388200.000000[  100/  100]\n",
      "\n",
      "running train loss =   2388200.0\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2387368.500000[  100/  100]\n",
      "\n",
      "running train loss =   2387368.5\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2386529.500000[  100/  100]\n",
      "\n",
      "running train loss =   2386529.5\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2385682.750000[  100/  100]\n",
      "\n",
      "running train loss =   2385682.75\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2384828.250000[  100/  100]\n",
      "\n",
      "running train loss =   2384828.25\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2383965.750000[  100/  100]\n",
      "\n",
      "running train loss =   2383965.75\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2383095.750000[  100/  100]\n",
      "\n",
      "running train loss =   2383095.75\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2382217.750000[  100/  100]\n",
      "\n",
      "running train loss =   2382217.75\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2381331.750000[  100/  100]\n",
      "\n",
      "running train loss =   2381331.75\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2380438.250000[  100/  100]\n",
      "\n",
      "running train loss =   2380438.25\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2379536.750000[  100/  100]\n",
      "\n",
      "running train loss =   2379536.75\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2378627.500000[  100/  100]\n",
      "\n",
      "running train loss =   2378627.5\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2377710.500000[  100/  100]\n",
      "\n",
      "running train loss =   2377710.5\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2376785.000000[  100/  100]\n",
      "\n",
      "running train loss =   2376785.0\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2375851.750000[  100/  100]\n",
      "\n",
      "running train loss =   2375851.75\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2374911.000000[  100/  100]\n",
      "\n",
      "running train loss =   2374911.0\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2373961.500000[  100/  100]\n",
      "\n",
      "running train loss =   2373961.5\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2373004.500000[  100/  100]\n",
      "\n",
      "running train loss =   2373004.5\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2372039.250000[  100/  100]\n",
      "\n",
      "running train loss =   2372039.25\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2371066.250000[  100/  100]\n",
      "\n",
      "running train loss =   2371066.25\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2370085.000000[  100/  100]\n",
      "\n",
      "running train loss =   2370085.0\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2369095.750000[  100/  100]\n",
      "\n",
      "running train loss =   2369095.75\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2368098.500000[  100/  100]\n",
      "\n",
      "running train loss =   2368098.5\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2367092.750000[  100/  100]\n",
      "\n",
      "running train loss =   2367092.75\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2366079.250000[  100/  100]\n",
      "\n",
      "running train loss =   2366079.25\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2365057.250000[  100/  100]\n",
      "\n",
      "running train loss =   2365057.25\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2364027.500000[  100/  100]\n",
      "\n",
      "running train loss =   2364027.5\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2362989.500000[  100/  100]\n",
      "\n",
      "running train loss =   2362989.5\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2361943.500000[  100/  100]\n",
      "\n",
      "running train loss =   2361943.5\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2360889.000000[  100/  100]\n",
      "\n",
      "running train loss =   2360889.0\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2359826.750000[  100/  100]\n",
      "\n",
      "running train loss =   2359826.75\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2358756.250000[  100/  100]\n",
      "\n",
      "running train loss =   2358756.25\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2357677.250000[  100/  100]\n",
      "\n",
      "running train loss =   2357677.25\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2356590.000000[  100/  100]\n",
      "\n",
      "running train loss =   2356590.0\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2355494.750000[  100/  100]\n",
      "\n",
      "running train loss =   2355494.75\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2354391.250000[  100/  100]\n",
      "\n",
      "running train loss =   2354391.25\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2353279.250000[  100/  100]\n",
      "\n",
      "running train loss =   2353279.25\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2352159.750000[  100/  100]\n",
      "\n",
      "running train loss =   2352159.75\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2351031.250000[  100/  100]\n",
      "\n",
      "running train loss =   2351031.25\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2349895.250000[  100/  100]\n",
      "\n",
      "running train loss =   2349895.25\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2348750.500000[  100/  100]\n",
      "\n",
      "running train loss =   2348750.5\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2347598.000000[  100/  100]\n",
      "\n",
      "running train loss =   2347598.0\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2346436.750000[  100/  100]\n",
      "\n",
      "running train loss =   2346436.75\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2345267.500000[  100/  100]\n",
      "\n",
      "running train loss =   2345267.5\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2344090.000000[  100/  100]\n",
      "\n",
      "running train loss =   2344090.0\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2342904.250000[  100/  100]\n",
      "\n",
      "running train loss =   2342904.25\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2341710.250000[  100/  100]\n",
      "\n",
      "running train loss =   2341710.25\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2340508.250000[  100/  100]\n",
      "\n",
      "running train loss =   2340508.25\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2339297.500000[  100/  100]\n",
      "\n",
      "running train loss =   2339297.5\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2338079.000000[  100/  100]\n",
      "\n",
      "running train loss =   2338079.0\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2336851.750000[  100/  100]\n",
      "\n",
      "running train loss =   2336851.75\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2335616.750000[  100/  100]\n",
      "\n",
      "running train loss =   2335616.75\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2334373.500000[  100/  100]\n",
      "\n",
      "running train loss =   2334373.5\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2333121.500000[  100/  100]\n",
      "\n",
      "running train loss =   2333121.5\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2331861.500000[  100/  100]\n",
      "\n",
      "running train loss =   2331861.5\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2330593.000000[  100/  100]\n",
      "\n",
      "running train loss =   2330593.0\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2329316.750000[  100/  100]\n",
      "\n",
      "running train loss =   2329316.75\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2328032.250000[  100/  100]\n",
      "\n",
      "running train loss =   2328032.25\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2326739.250000[  100/  100]\n",
      "\n",
      "running train loss =   2326739.25\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2325438.000000[  100/  100]\n",
      "\n",
      "running train loss =   2325438.0\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2324128.500000[  100/  100]\n",
      "\n",
      "running train loss =   2324128.5\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2322810.750000[  100/  100]\n",
      "\n",
      "running train loss =   2322810.75\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2321484.750000[  100/  100]\n",
      "\n",
      "running train loss =   2321484.75\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2320150.500000[  100/  100]\n",
      "\n",
      "running train loss =   2320150.5\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2318808.250000[  100/  100]\n",
      "\n",
      "running train loss =   2318808.25\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2317457.500000[  100/  100]\n",
      "\n",
      "running train loss =   2317457.5\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2316099.000000[  100/  100]\n",
      "\n",
      "running train loss =   2316099.0\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2314732.000000[  100/  100]\n",
      "\n",
      "running train loss =   2314732.0\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2313356.750000[  100/  100]\n",
      "\n",
      "running train loss =   2313356.75\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2311973.500000[  100/  100]\n",
      "\n",
      "running train loss =   2311973.5\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2310582.000000[  100/  100]\n",
      "\n",
      "running train loss =   2310582.0\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2309182.000000[  100/  100]\n",
      "\n",
      "running train loss =   2309182.0\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2307774.000000[  100/  100]\n",
      "\n",
      "running train loss =   2307774.0\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2306358.500000[  100/  100]\n",
      "\n",
      "running train loss =   2306358.5\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2304934.000000[  100/  100]\n",
      "\n",
      "running train loss =   2304934.0\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2303502.000000[  100/  100]\n",
      "\n",
      "running train loss =   2303502.0\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2302061.500000[  100/  100]\n",
      "\n",
      "running train loss =   2302061.5\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2300613.000000[  100/  100]\n",
      "\n",
      "running train loss =   2300613.0\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2299156.250000[  100/  100]\n",
      "\n",
      "running train loss =   2299156.25\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2297691.250000[  100/  100]\n",
      "\n",
      "running train loss =   2297691.25\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2296218.500000[  100/  100]\n",
      "\n",
      "running train loss =   2296218.5\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2294737.500000[  100/  100]\n",
      "\n",
      "running train loss =   2294737.5\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2293248.750000[  100/  100]\n",
      "\n",
      "running train loss =   2293248.75\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2291751.250000[  100/  100]\n",
      "\n",
      "running train loss =   2291751.25\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2290246.500000[  100/  100]\n",
      "\n",
      "running train loss =   2290246.5\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2288733.250000[  100/  100]\n",
      "\n",
      "running train loss =   2288733.25\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2287211.750000[  100/  100]\n",
      "\n",
      "running train loss =   2287211.75\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2285682.500000[  100/  100]\n",
      "\n",
      "running train loss =   2285682.5\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2284145.000000[  100/  100]\n",
      "\n",
      "running train loss =   2284145.0\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2282600.000000[  100/  100]\n",
      "\n",
      "running train loss =   2282600.0\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2281046.500000[  100/  100]\n",
      "\n",
      "running train loss =   2281046.5\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2279484.750000[  100/  100]\n",
      "\n",
      "running train loss =   2279484.75\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2277915.250000[  100/  100]\n",
      "\n",
      "running train loss =   2277915.25\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2276338.000000[  100/  100]\n",
      "\n",
      "running train loss =   2276338.0\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2274752.000000[  100/  100]\n",
      "\n",
      "running train loss =   2274752.0\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2273159.000000[  100/  100]\n",
      "\n",
      "running train loss =   2273159.0\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2271557.000000[  100/  100]\n",
      "\n",
      "running train loss =   2271557.0\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2269947.750000[  100/  100]\n",
      "\n",
      "running train loss =   2269947.75\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2268330.250000[  100/  100]\n",
      "\n",
      "running train loss =   2268330.25\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2266704.750000[  100/  100]\n",
      "\n",
      "running train loss =   2266704.75\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2265071.000000[  100/  100]\n",
      "\n",
      "running train loss =   2265071.0\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2263429.750000[  100/  100]\n",
      "\n",
      "running train loss =   2263429.75\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2261780.250000[  100/  100]\n",
      "\n",
      "running train loss =   2261780.25\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2260122.500000[  100/  100]\n",
      "\n",
      "running train loss =   2260122.5\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2258457.250000[  100/  100]\n",
      "\n",
      "running train loss =   2258457.25\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2256783.750000[  100/  100]\n",
      "\n",
      "running train loss =   2256783.75\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2255102.500000[  100/  100]\n",
      "\n",
      "running train loss =   2255102.5\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2253412.750000[  100/  100]\n",
      "\n",
      "running train loss =   2253412.75\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2251715.750000[  100/  100]\n",
      "\n",
      "running train loss =   2251715.75\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2250010.000000[  100/  100]\n",
      "\n",
      "running train loss =   2250010.0\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2248297.250000[  100/  100]\n",
      "\n",
      "running train loss =   2248297.25\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2246575.750000[  100/  100]\n",
      "\n",
      "running train loss =   2246575.75\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2244847.000000[  100/  100]\n",
      "\n",
      "running train loss =   2244847.0\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2243109.750000[  100/  100]\n",
      "\n",
      "running train loss =   2243109.75\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2241364.750000[  100/  100]\n",
      "\n",
      "running train loss =   2241364.75\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2239612.000000[  100/  100]\n",
      "\n",
      "running train loss =   2239612.0\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2237851.250000[  100/  100]\n",
      "\n",
      "running train loss =   2237851.25\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2236082.500000[  100/  100]\n",
      "\n",
      "running train loss =   2236082.5\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2234306.000000[  100/  100]\n",
      "\n",
      "running train loss =   2234306.0\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2232522.000000[  100/  100]\n",
      "\n",
      "running train loss =   2232522.0\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2230729.500000[  100/  100]\n",
      "\n",
      "running train loss =   2230729.5\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2228929.500000[  100/  100]\n",
      "\n",
      "running train loss =   2228929.5\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2227122.000000[  100/  100]\n",
      "\n",
      "running train loss =   2227122.0\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2225306.000000[  100/  100]\n",
      "\n",
      "running train loss =   2225306.0\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2223482.500000[  100/  100]\n",
      "\n",
      "running train loss =   2223482.5\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2221651.500000[  100/  100]\n",
      "\n",
      "running train loss =   2221651.5\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2219812.750000[  100/  100]\n",
      "\n",
      "running train loss =   2219812.75\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2217966.000000[  100/  100]\n",
      "\n",
      "running train loss =   2217966.0\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2216111.250000[  100/  100]\n",
      "\n",
      "running train loss =   2216111.25\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2214249.250000[  100/  100]\n",
      "\n",
      "running train loss =   2214249.25\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2212379.250000[  100/  100]\n",
      "\n",
      "running train loss =   2212379.25\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2210502.000000[  100/  100]\n",
      "\n",
      "running train loss =   2210502.0\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2208617.250000[  100/  100]\n",
      "\n",
      "running train loss =   2208617.25\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2206724.500000[  100/  100]\n",
      "\n",
      "running train loss =   2206724.5\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2204824.000000[  100/  100]\n",
      "\n",
      "running train loss =   2204824.0\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2202916.000000[  100/  100]\n",
      "\n",
      "running train loss =   2202916.0\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2201000.750000[  100/  100]\n",
      "\n",
      "running train loss =   2201000.75\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2199077.750000[  100/  100]\n",
      "\n",
      "running train loss =   2199077.75\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2197147.000000[  100/  100]\n",
      "\n",
      "running train loss =   2197147.0\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2195209.000000[  100/  100]\n",
      "\n",
      "running train loss =   2195209.0\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2193263.250000[  100/  100]\n",
      "\n",
      "running train loss =   2193263.25\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2191310.000000[  100/  100]\n",
      "\n",
      "running train loss =   2191310.0\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2189349.500000[  100/  100]\n",
      "\n",
      "running train loss =   2189349.5\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2187381.000000[  100/  100]\n",
      "\n",
      "running train loss =   2187381.0\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2185405.500000[  100/  100]\n",
      "\n",
      "running train loss =   2185405.5\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2183422.250000[  100/  100]\n",
      "\n",
      "running train loss =   2183422.25\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2181431.500000[  100/  100]\n",
      "\n",
      "running train loss =   2181431.5\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2179433.250000[  100/  100]\n",
      "\n",
      "running train loss =   2179433.25\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2177427.250000[  100/  100]\n",
      "\n",
      "running train loss =   2177427.25\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2175414.000000[  100/  100]\n",
      "\n",
      "running train loss =   2175414.0\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2173393.000000[  100/  100]\n",
      "\n",
      "running train loss =   2173393.0\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2171364.500000[  100/  100]\n",
      "\n",
      "running train loss =   2171364.5\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2169328.750000[  100/  100]\n",
      "\n",
      "running train loss =   2169328.75\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2167285.000000[  100/  100]\n",
      "\n",
      "running train loss =   2167285.0\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2165234.250000[  100/  100]\n",
      "\n",
      "running train loss =   2165234.25\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2163176.000000[  100/  100]\n",
      "\n",
      "running train loss =   2163176.0\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2161110.500000[  100/  100]\n",
      "\n",
      "running train loss =   2161110.5\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2159037.500000[  100/  100]\n",
      "\n",
      "running train loss =   2159037.5\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2156957.500000[  100/  100]\n",
      "\n",
      "running train loss =   2156957.5\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2154870.000000[  100/  100]\n",
      "\n",
      "running train loss =   2154870.0\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2152775.250000[  100/  100]\n",
      "\n",
      "running train loss =   2152775.25\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2150673.000000[  100/  100]\n",
      "\n",
      "running train loss =   2150673.0\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2148564.250000[  100/  100]\n",
      "\n",
      "running train loss =   2148564.25\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2146448.000000[  100/  100]\n",
      "\n",
      "running train loss =   2146448.0\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2144324.500000[  100/  100]\n",
      "\n",
      "running train loss =   2144324.5\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2142194.250000[  100/  100]\n",
      "\n",
      "running train loss =   2142194.25\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2140056.500000[  100/  100]\n",
      "\n",
      "running train loss =   2140056.5\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2137911.750000[  100/  100]\n",
      "\n",
      "running train loss =   2137911.75\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2135760.250000[  100/  100]\n",
      "\n",
      "running train loss =   2135760.25\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2133601.500000[  100/  100]\n",
      "\n",
      "running train loss =   2133601.5\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2131435.750000[  100/  100]\n",
      "\n",
      "running train loss =   2131435.75\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2129263.250000[  100/  100]\n",
      "\n",
      "running train loss =   2129263.25\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2127083.750000[  100/  100]\n",
      "\n",
      "running train loss =   2127083.75\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2124897.500000[  100/  100]\n",
      "\n",
      "running train loss =   2124897.5\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2122704.250000[  100/  100]\n",
      "\n",
      "running train loss =   2122704.25\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2120504.250000[  100/  100]\n",
      "\n",
      "running train loss =   2120504.25\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2118297.000000[  100/  100]\n",
      "\n",
      "running train loss =   2118297.0\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2116083.250000[  100/  100]\n",
      "\n",
      "running train loss =   2116083.25\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2113862.500000[  100/  100]\n",
      "\n",
      "running train loss =   2113862.5\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2111634.750000[  100/  100]\n",
      "\n",
      "running train loss =   2111634.75\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2109400.250000[  100/  100]\n",
      "\n",
      "running train loss =   2109400.25\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2107159.000000[  100/  100]\n",
      "\n",
      "running train loss =   2107159.0\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2104911.000000[  100/  100]\n",
      "\n",
      "running train loss =   2104911.0\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2102655.750000[  100/  100]\n",
      "\n",
      "running train loss =   2102655.75\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2100394.000000[  100/  100]\n",
      "\n",
      "running train loss =   2100394.0\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2098125.250000[  100/  100]\n",
      "\n",
      "running train loss =   2098125.25\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2095849.875000[  100/  100]\n",
      "\n",
      "running train loss =   2095849.875\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2093568.000000[  100/  100]\n",
      "\n",
      "running train loss =   2093568.0\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2091279.375000[  100/  100]\n",
      "\n",
      "running train loss =   2091279.375\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2088983.875000[  100/  100]\n",
      "\n",
      "running train loss =   2088983.875\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2086681.750000[  100/  100]\n",
      "\n",
      "running train loss =   2086681.75\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2084373.500000[  100/  100]\n",
      "\n",
      "running train loss =   2084373.5\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2082058.250000[  100/  100]\n",
      "\n",
      "running train loss =   2082058.25\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2079736.625000[  100/  100]\n",
      "\n",
      "running train loss =   2079736.625\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2077408.625000[  100/  100]\n",
      "\n",
      "running train loss =   2077408.625\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2075074.250000[  100/  100]\n",
      "\n",
      "running train loss =   2075074.25\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2072733.250000[  100/  100]\n",
      "\n",
      "running train loss =   2072733.25\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2070386.125000[  100/  100]\n",
      "\n",
      "running train loss =   2070386.125\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2068032.500000[  100/  100]\n",
      "\n",
      "running train loss =   2068032.5\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2065672.625000[  100/  100]\n",
      "\n",
      "running train loss =   2065672.625\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2063306.500000[  100/  100]\n",
      "\n",
      "running train loss =   2063306.5\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2060934.125000[  100/  100]\n",
      "\n",
      "running train loss =   2060934.125\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2058555.250000[  100/  100]\n",
      "\n",
      "running train loss =   2058555.25\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2056170.250000[  100/  100]\n",
      "\n",
      "running train loss =   2056170.25\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2053779.375000[  100/  100]\n",
      "\n",
      "running train loss =   2053779.375\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2051382.125000[  100/  100]\n",
      "\n",
      "running train loss =   2051382.125\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2048978.750000[  100/  100]\n",
      "\n",
      "running train loss =   2048978.75\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2046569.250000[  100/  100]\n",
      "\n",
      "running train loss =   2046569.25\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2044153.250000[  100/  100]\n",
      "\n",
      "running train loss =   2044153.25\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2041731.875000[  100/  100]\n",
      "\n",
      "running train loss =   2041731.875\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2039304.125000[  100/  100]\n",
      "\n",
      "running train loss =   2039304.125\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2036870.500000[  100/  100]\n",
      "\n",
      "running train loss =   2036870.5\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2034430.875000[  100/  100]\n",
      "\n",
      "running train loss =   2034430.875\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2031985.250000[  100/  100]\n",
      "\n",
      "running train loss =   2031985.25\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2029533.875000[  100/  100]\n",
      "\n",
      "running train loss =   2029533.875\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2027076.625000[  100/  100]\n",
      "\n",
      "running train loss =   2027076.625\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2024613.250000[  100/  100]\n",
      "\n",
      "running train loss =   2024613.25\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2022144.000000[  100/  100]\n",
      "\n",
      "running train loss =   2022144.0\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2019669.125000[  100/  100]\n",
      "\n",
      "running train loss =   2019669.125\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2017188.375000[  100/  100]\n",
      "\n",
      "running train loss =   2017188.375\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2014701.750000[  100/  100]\n",
      "\n",
      "running train loss =   2014701.75\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2012209.625000[  100/  100]\n",
      "\n",
      "running train loss =   2012209.625\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2009711.500000[  100/  100]\n",
      "\n",
      "running train loss =   2009711.5\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2007207.625000[  100/  100]\n",
      "\n",
      "running train loss =   2007207.625\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2004698.250000[  100/  100]\n",
      "\n",
      "running train loss =   2004698.25\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2002182.875000[  100/  100]\n",
      "\n",
      "running train loss =   2002182.875\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1999661.750000[  100/  100]\n",
      "\n",
      "running train loss =   1999661.75\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1997135.000000[  100/  100]\n",
      "\n",
      "running train loss =   1997135.0\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1994602.375000[  100/  100]\n",
      "\n",
      "running train loss =   1994602.375\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1992064.375000[  100/  100]\n",
      "\n",
      "running train loss =   1992064.375\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1989520.500000[  100/  100]\n",
      "\n",
      "running train loss =   1989520.5\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1986970.875000[  100/  100]\n",
      "\n",
      "running train loss =   1986970.875\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1984416.125000[  100/  100]\n",
      "\n",
      "running train loss =   1984416.125\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1981855.625000[  100/  100]\n",
      "\n",
      "running train loss =   1981855.625\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1979289.625000[  100/  100]\n",
      "\n",
      "running train loss =   1979289.625\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1976718.500000[  100/  100]\n",
      "\n",
      "running train loss =   1976718.5\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1974141.625000[  100/  100]\n",
      "\n",
      "running train loss =   1974141.625\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1971559.375000[  100/  100]\n",
      "\n",
      "running train loss =   1971559.375\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1968972.000000[  100/  100]\n",
      "\n",
      "running train loss =   1968972.0\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1966378.875000[  100/  100]\n",
      "\n",
      "running train loss =   1966378.875\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1963781.125000[  100/  100]\n",
      "\n",
      "running train loss =   1963781.125\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1961177.750000[  100/  100]\n",
      "\n",
      "running train loss =   1961177.75\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1958569.000000[  100/  100]\n",
      "\n",
      "running train loss =   1958569.0\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1955955.375000[  100/  100]\n",
      "\n",
      "running train loss =   1955955.375\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1953336.375000[  100/  100]\n",
      "\n",
      "running train loss =   1953336.375\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1950712.375000[  100/  100]\n",
      "\n",
      "running train loss =   1950712.375\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1948083.250000[  100/  100]\n",
      "\n",
      "running train loss =   1948083.25\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1945448.625000[  100/  100]\n",
      "\n",
      "running train loss =   1945448.625\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1942809.750000[  100/  100]\n",
      "\n",
      "running train loss =   1942809.75\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1940165.250000[  100/  100]\n",
      "\n",
      "running train loss =   1940165.25\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1937515.875000[  100/  100]\n",
      "\n",
      "running train loss =   1937515.875\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1934861.750000[  100/  100]\n",
      "\n",
      "running train loss =   1934861.75\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1932202.375000[  100/  100]\n",
      "\n",
      "running train loss =   1932202.375\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1929538.125000[  100/  100]\n",
      "\n",
      "running train loss =   1929538.125\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1926869.250000[  100/  100]\n",
      "\n",
      "running train loss =   1926869.25\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1924195.500000[  100/  100]\n",
      "\n",
      "running train loss =   1924195.5\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1921516.625000[  100/  100]\n",
      "\n",
      "running train loss =   1921516.625\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1918833.250000[  100/  100]\n",
      "\n",
      "running train loss =   1918833.25\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1916145.000000[  100/  100]\n",
      "\n",
      "running train loss =   1916145.0\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1913452.000000[  100/  100]\n",
      "\n",
      "running train loss =   1913452.0\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1910754.250000[  100/  100]\n",
      "\n",
      "running train loss =   1910754.25\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1908052.000000[  100/  100]\n",
      "\n",
      "running train loss =   1908052.0\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1905345.000000[  100/  100]\n",
      "\n",
      "running train loss =   1905345.0\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1902633.250000[  100/  100]\n",
      "\n",
      "running train loss =   1902633.25\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1899916.750000[  100/  100]\n",
      "\n",
      "running train loss =   1899916.75\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1897196.375000[  100/  100]\n",
      "\n",
      "running train loss =   1897196.375\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1894470.750000[  100/  100]\n",
      "\n",
      "running train loss =   1894470.75\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1891740.750000[  100/  100]\n",
      "\n",
      "running train loss =   1891740.75\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1889006.500000[  100/  100]\n",
      "\n",
      "running train loss =   1889006.5\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1886267.875000[  100/  100]\n",
      "\n",
      "running train loss =   1886267.875\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1883524.500000[  100/  100]\n",
      "\n",
      "running train loss =   1883524.5\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1880776.625000[  100/  100]\n",
      "\n",
      "running train loss =   1880776.625\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1878025.000000[  100/  100]\n",
      "\n",
      "running train loss =   1878025.0\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1875268.500000[  100/  100]\n",
      "\n",
      "running train loss =   1875268.5\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1872508.000000[  100/  100]\n",
      "\n",
      "running train loss =   1872508.0\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1869743.000000[  100/  100]\n",
      "\n",
      "running train loss =   1869743.0\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1866973.875000[  100/  100]\n",
      "\n",
      "running train loss =   1866973.875\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1864200.375000[  100/  100]\n",
      "\n",
      "running train loss =   1864200.375\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1861422.875000[  100/  100]\n",
      "\n",
      "running train loss =   1861422.875\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1858641.000000[  100/  100]\n",
      "\n",
      "running train loss =   1858641.0\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1855855.000000[  100/  100]\n",
      "\n",
      "running train loss =   1855855.0\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1853065.000000[  100/  100]\n",
      "\n",
      "running train loss =   1853065.0\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1850270.375000[  100/  100]\n",
      "\n",
      "running train loss =   1850270.375\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1847472.125000[  100/  100]\n",
      "\n",
      "running train loss =   1847472.125\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1844669.875000[  100/  100]\n",
      "\n",
      "running train loss =   1844669.875\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1841863.375000[  100/  100]\n",
      "\n",
      "running train loss =   1841863.375\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1839052.750000[  100/  100]\n",
      "\n",
      "running train loss =   1839052.75\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1836238.250000[  100/  100]\n",
      "\n",
      "running train loss =   1836238.25\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1833419.625000[  100/  100]\n",
      "\n",
      "running train loss =   1833419.625\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1830597.125000[  100/  100]\n",
      "\n",
      "running train loss =   1830597.125\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1827770.500000[  100/  100]\n",
      "\n",
      "running train loss =   1827770.5\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1824939.625000[  100/  100]\n",
      "\n",
      "running train loss =   1824939.625\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1822105.250000[  100/  100]\n",
      "\n",
      "running train loss =   1822105.25\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1819266.750000[  100/  100]\n",
      "\n",
      "running train loss =   1819266.75\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1816424.125000[  100/  100]\n",
      "\n",
      "running train loss =   1816424.125\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1813577.750000[  100/  100]\n",
      "\n",
      "running train loss =   1813577.75\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1810727.000000[  100/  100]\n",
      "\n",
      "running train loss =   1810727.0\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1807872.375000[  100/  100]\n",
      "\n",
      "running train loss =   1807872.375\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1805013.250000[  100/  100]\n",
      "\n",
      "running train loss =   1805013.25\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1802150.375000[  100/  100]\n",
      "\n",
      "running train loss =   1802150.375\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1799283.250000[  100/  100]\n",
      "\n",
      "running train loss =   1799283.25\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1796411.500000[  100/  100]\n",
      "\n",
      "running train loss =   1796411.5\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1793536.125000[  100/  100]\n",
      "\n",
      "running train loss =   1793536.125\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1790656.375000[  100/  100]\n",
      "\n",
      "running train loss =   1790656.375\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1787772.500000[  100/  100]\n",
      "\n",
      "running train loss =   1787772.5\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1784884.500000[  100/  100]\n",
      "\n",
      "running train loss =   1784884.5\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1781992.750000[  100/  100]\n",
      "\n",
      "running train loss =   1781992.75\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1779097.000000[  100/  100]\n",
      "\n",
      "running train loss =   1779097.0\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1776197.500000[  100/  100]\n",
      "\n",
      "running train loss =   1776197.5\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1773294.375000[  100/  100]\n",
      "\n",
      "running train loss =   1773294.375\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1770387.375000[  100/  100]\n",
      "\n",
      "running train loss =   1770387.375\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1767477.000000[  100/  100]\n",
      "\n",
      "running train loss =   1767477.0\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1764562.750000[  100/  100]\n",
      "\n",
      "running train loss =   1764562.75\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1761644.750000[  100/  100]\n",
      "\n",
      "running train loss =   1761644.75\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1758723.500000[  100/  100]\n",
      "\n",
      "running train loss =   1758723.5\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1755798.500000[  100/  100]\n",
      "\n",
      "running train loss =   1755798.5\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1752870.125000[  100/  100]\n",
      "\n",
      "running train loss =   1752870.125\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1749938.500000[  100/  100]\n",
      "\n",
      "running train loss =   1749938.5\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1747003.250000[  100/  100]\n",
      "\n",
      "running train loss =   1747003.25\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1744064.625000[  100/  100]\n",
      "\n",
      "running train loss =   1744064.625\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1741122.500000[  100/  100]\n",
      "\n",
      "running train loss =   1741122.5\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1738177.250000[  100/  100]\n",
      "\n",
      "running train loss =   1738177.25\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1735228.750000[  100/  100]\n",
      "\n",
      "running train loss =   1735228.75\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1732277.125000[  100/  100]\n",
      "\n",
      "running train loss =   1732277.125\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1729322.250000[  100/  100]\n",
      "\n",
      "running train loss =   1729322.25\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1726364.375000[  100/  100]\n",
      "\n",
      "running train loss =   1726364.375\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1723403.000000[  100/  100]\n",
      "\n",
      "running train loss =   1723403.0\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1720438.875000[  100/  100]\n",
      "\n",
      "running train loss =   1720438.875\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1717471.500000[  100/  100]\n",
      "\n",
      "running train loss =   1717471.5\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1714501.500000[  100/  100]\n",
      "\n",
      "running train loss =   1714501.5\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1711528.125000[  100/  100]\n",
      "\n",
      "running train loss =   1711528.125\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1708552.000000[  100/  100]\n",
      "\n",
      "running train loss =   1708552.0\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1705573.000000[  100/  100]\n",
      "\n",
      "running train loss =   1705573.0\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1702591.000000[  100/  100]\n",
      "\n",
      "running train loss =   1702591.0\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1699606.500000[  100/  100]\n",
      "\n",
      "running train loss =   1699606.5\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1696618.875000[  100/  100]\n",
      "\n",
      "running train loss =   1696618.875\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1693628.750000[  100/  100]\n",
      "\n",
      "running train loss =   1693628.75\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1690635.875000[  100/  100]\n",
      "\n",
      "running train loss =   1690635.875\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1687640.125000[  100/  100]\n",
      "\n",
      "running train loss =   1687640.125\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1684642.125000[  100/  100]\n",
      "\n",
      "running train loss =   1684642.125\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1681641.125000[  100/  100]\n",
      "\n",
      "running train loss =   1681641.125\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1678637.875000[  100/  100]\n",
      "\n",
      "running train loss =   1678637.875\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1675632.125000[  100/  100]\n",
      "\n",
      "running train loss =   1675632.125\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1672623.625000[  100/  100]\n",
      "\n",
      "running train loss =   1672623.625\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1669613.000000[  100/  100]\n",
      "\n",
      "running train loss =   1669613.0\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1666599.625000[  100/  100]\n",
      "\n",
      "running train loss =   1666599.625\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1663584.000000[  100/  100]\n",
      "\n",
      "running train loss =   1663584.0\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1660566.125000[  100/  100]\n",
      "\n",
      "running train loss =   1660566.125\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1657545.875000[  100/  100]\n",
      "\n",
      "running train loss =   1657545.875\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1654523.375000[  100/  100]\n",
      "\n",
      "running train loss =   1654523.375\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1651498.750000[  100/  100]\n",
      "\n",
      "running train loss =   1651498.75\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1648471.625000[  100/  100]\n",
      "\n",
      "running train loss =   1648471.625\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1645442.500000[  100/  100]\n",
      "\n",
      "running train loss =   1645442.5\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1642411.375000[  100/  100]\n",
      "\n",
      "running train loss =   1642411.375\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1639377.875000[  100/  100]\n",
      "\n",
      "running train loss =   1639377.875\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1636342.375000[  100/  100]\n",
      "\n",
      "running train loss =   1636342.375\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1633304.750000[  100/  100]\n",
      "\n",
      "running train loss =   1633304.75\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1630265.125000[  100/  100]\n",
      "\n",
      "running train loss =   1630265.125\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1627223.500000[  100/  100]\n",
      "\n",
      "running train loss =   1627223.5\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1624180.000000[  100/  100]\n",
      "\n",
      "running train loss =   1624180.0\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1621134.250000[  100/  100]\n",
      "\n",
      "running train loss =   1621134.25\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1618086.375000[  100/  100]\n",
      "\n",
      "running train loss =   1618086.375\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1615036.625000[  100/  100]\n",
      "\n",
      "running train loss =   1615036.625\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1611985.000000[  100/  100]\n",
      "\n",
      "running train loss =   1611985.0\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1608931.250000[  100/  100]\n",
      "\n",
      "running train loss =   1608931.25\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1605875.500000[  100/  100]\n",
      "\n",
      "running train loss =   1605875.5\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1602818.250000[  100/  100]\n",
      "\n",
      "running train loss =   1602818.25\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1599759.250000[  100/  100]\n",
      "\n",
      "running train loss =   1599759.25\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1596698.125000[  100/  100]\n",
      "\n",
      "running train loss =   1596698.125\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1593635.500000[  100/  100]\n",
      "\n",
      "running train loss =   1593635.5\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1590571.250000[  100/  100]\n",
      "\n",
      "running train loss =   1590571.25\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1587505.250000[  100/  100]\n",
      "\n",
      "running train loss =   1587505.25\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1584437.625000[  100/  100]\n",
      "\n",
      "running train loss =   1584437.625\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1581368.500000[  100/  100]\n",
      "\n",
      "running train loss =   1581368.5\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1578297.875000[  100/  100]\n",
      "\n",
      "running train loss =   1578297.875\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1575225.750000[  100/  100]\n",
      "\n",
      "running train loss =   1575225.75\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1572152.000000[  100/  100]\n",
      "\n",
      "running train loss =   1572152.0\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1569076.750000[  100/  100]\n",
      "\n",
      "running train loss =   1569076.75\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1566000.375000[  100/  100]\n",
      "\n",
      "running train loss =   1566000.375\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1562922.375000[  100/  100]\n",
      "\n",
      "running train loss =   1562922.375\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1559843.250000[  100/  100]\n",
      "\n",
      "running train loss =   1559843.25\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1556762.500000[  100/  100]\n",
      "\n",
      "running train loss =   1556762.5\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1553680.625000[  100/  100]\n",
      "\n",
      "running train loss =   1553680.625\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1550597.500000[  100/  100]\n",
      "\n",
      "running train loss =   1550597.5\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1547513.000000[  100/  100]\n",
      "\n",
      "running train loss =   1547513.0\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1544427.000000[  100/  100]\n",
      "\n",
      "running train loss =   1544427.0\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1541340.375000[  100/  100]\n",
      "\n",
      "running train loss =   1541340.375\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1538252.375000[  100/  100]\n",
      "\n",
      "running train loss =   1538252.375\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1535163.250000[  100/  100]\n",
      "\n",
      "running train loss =   1535163.25\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1532072.750000[  100/  100]\n",
      "\n",
      "running train loss =   1532072.75\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1528981.625000[  100/  100]\n",
      "\n",
      "running train loss =   1528981.625\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1525889.250000[  100/  100]\n",
      "\n",
      "running train loss =   1525889.25\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1522795.625000[  100/  100]\n",
      "\n",
      "running train loss =   1522795.625\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1519701.625000[  100/  100]\n",
      "\n",
      "running train loss =   1519701.625\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1516606.500000[  100/  100]\n",
      "\n",
      "running train loss =   1516606.5\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1513510.250000[  100/  100]\n",
      "\n",
      "running train loss =   1513510.25\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1510413.250000[  100/  100]\n",
      "\n",
      "running train loss =   1510413.25\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1507315.500000[  100/  100]\n",
      "\n",
      "running train loss =   1507315.5\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1504217.000000[  100/  100]\n",
      "\n",
      "running train loss =   1504217.0\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1501117.500000[  100/  100]\n",
      "\n",
      "running train loss =   1501117.5\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1498017.250000[  100/  100]\n",
      "\n",
      "running train loss =   1498017.25\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1494916.375000[  100/  100]\n",
      "\n",
      "running train loss =   1494916.375\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1491814.750000[  100/  100]\n",
      "\n",
      "running train loss =   1491814.75\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1488712.625000[  100/  100]\n",
      "\n",
      "running train loss =   1488712.625\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1485609.750000[  100/  100]\n",
      "\n",
      "running train loss =   1485609.75\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1482506.375000[  100/  100]\n",
      "\n",
      "running train loss =   1482506.375\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1479402.250000[  100/  100]\n",
      "\n",
      "running train loss =   1479402.25\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1476297.500000[  100/  100]\n",
      "\n",
      "running train loss =   1476297.5\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1473192.125000[  100/  100]\n",
      "\n",
      "running train loss =   1473192.125\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1470086.125000[  100/  100]\n",
      "\n",
      "running train loss =   1470086.125\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1466979.625000[  100/  100]\n",
      "\n",
      "running train loss =   1466979.625\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1463872.750000[  100/  100]\n",
      "\n",
      "running train loss =   1463872.75\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1460765.250000[  100/  100]\n",
      "\n",
      "running train loss =   1460765.25\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1457657.500000[  100/  100]\n",
      "\n",
      "running train loss =   1457657.5\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1454549.250000[  100/  100]\n",
      "\n",
      "running train loss =   1454549.25\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1451440.625000[  100/  100]\n",
      "\n",
      "running train loss =   1451440.625\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1448331.875000[  100/  100]\n",
      "\n",
      "running train loss =   1448331.875\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1445222.750000[  100/  100]\n",
      "\n",
      "running train loss =   1445222.75\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1442113.250000[  100/  100]\n",
      "\n",
      "running train loss =   1442113.25\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1439003.625000[  100/  100]\n",
      "\n",
      "running train loss =   1439003.625\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1435893.875000[  100/  100]\n",
      "\n",
      "running train loss =   1435893.875\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1432784.125000[  100/  100]\n",
      "\n",
      "running train loss =   1432784.125\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1429674.250000[  100/  100]\n",
      "\n",
      "running train loss =   1429674.25\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1426564.125000[  100/  100]\n",
      "\n",
      "running train loss =   1426564.125\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1423454.250000[  100/  100]\n",
      "\n",
      "running train loss =   1423454.25\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1420344.375000[  100/  100]\n",
      "\n",
      "running train loss =   1420344.375\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1417234.250000[  100/  100]\n",
      "\n",
      "running train loss =   1417234.25\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1414124.375000[  100/  100]\n",
      "\n",
      "running train loss =   1414124.375\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1411014.375000[  100/  100]\n",
      "\n",
      "running train loss =   1411014.375\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1407905.000000[  100/  100]\n",
      "\n",
      "running train loss =   1407905.0\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1404795.375000[  100/  100]\n",
      "\n",
      "running train loss =   1404795.375\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1401685.875000[  100/  100]\n",
      "\n",
      "running train loss =   1401685.875\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1398577.000000[  100/  100]\n",
      "\n",
      "running train loss =   1398577.0\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1395468.125000[  100/  100]\n",
      "\n",
      "running train loss =   1395468.125\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1392359.625000[  100/  100]\n",
      "\n",
      "running train loss =   1392359.625\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1389251.500000[  100/  100]\n",
      "\n",
      "running train loss =   1389251.5\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1386143.625000[  100/  100]\n",
      "\n",
      "running train loss =   1386143.625\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1383036.375000[  100/  100]\n",
      "\n",
      "running train loss =   1383036.375\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1379929.250000[  100/  100]\n",
      "\n",
      "running train loss =   1379929.25\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1376822.875000[  100/  100]\n",
      "\n",
      "running train loss =   1376822.875\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1373716.750000[  100/  100]\n",
      "\n",
      "running train loss =   1373716.75\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1370611.375000[  100/  100]\n",
      "\n",
      "running train loss =   1370611.375\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1367506.250000[  100/  100]\n",
      "\n",
      "running train loss =   1367506.25\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1364401.750000[  100/  100]\n",
      "\n",
      "running train loss =   1364401.75\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1361297.875000[  100/  100]\n",
      "\n",
      "running train loss =   1361297.875\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1358194.875000[  100/  100]\n",
      "\n",
      "running train loss =   1358194.875\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1355092.375000[  100/  100]\n",
      "\n",
      "running train loss =   1355092.375\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1351990.500000[  100/  100]\n",
      "\n",
      "running train loss =   1351990.5\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1348889.500000[  100/  100]\n",
      "\n",
      "running train loss =   1348889.5\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1345789.125000[  100/  100]\n",
      "\n",
      "running train loss =   1345789.125\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1342689.625000[  100/  100]\n",
      "\n",
      "running train loss =   1342689.625\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1339591.000000[  100/  100]\n",
      "\n",
      "running train loss =   1339591.0\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1336493.000000[  100/  100]\n",
      "\n",
      "running train loss =   1336493.0\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1333396.125000[  100/  100]\n",
      "\n",
      "running train loss =   1333396.125\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1330300.125000[  100/  100]\n",
      "\n",
      "running train loss =   1330300.125\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1327205.000000[  100/  100]\n",
      "\n",
      "running train loss =   1327205.0\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1324110.500000[  100/  100]\n",
      "\n",
      "running train loss =   1324110.5\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1321017.625000[  100/  100]\n",
      "\n",
      "running train loss =   1321017.625\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1317925.250000[  100/  100]\n",
      "\n",
      "running train loss =   1317925.25\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1314834.125000[  100/  100]\n",
      "\n",
      "running train loss =   1314834.125\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1311744.125000[  100/  100]\n",
      "\n",
      "running train loss =   1311744.125\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1308655.250000[  100/  100]\n",
      "\n",
      "running train loss =   1308655.25\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1305567.375000[  100/  100]\n",
      "\n",
      "running train loss =   1305567.375\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1302480.625000[  100/  100]\n",
      "\n",
      "running train loss =   1302480.625\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1299395.000000[  100/  100]\n",
      "\n",
      "running train loss =   1299395.0\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1296310.750000[  100/  100]\n",
      "\n",
      "running train loss =   1296310.75\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1293227.625000[  100/  100]\n",
      "\n",
      "running train loss =   1293227.625\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1290145.875000[  100/  100]\n",
      "\n",
      "running train loss =   1290145.875\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1287065.375000[  100/  100]\n",
      "\n",
      "running train loss =   1287065.375\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1283986.250000[  100/  100]\n",
      "\n",
      "running train loss =   1283986.25\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1280908.375000[  100/  100]\n",
      "\n",
      "running train loss =   1280908.375\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1277831.875000[  100/  100]\n",
      "\n",
      "running train loss =   1277831.875\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1274756.750000[  100/  100]\n",
      "\n",
      "running train loss =   1274756.75\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1271683.000000[  100/  100]\n",
      "\n",
      "running train loss =   1271683.0\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1268610.750000[  100/  100]\n",
      "\n",
      "running train loss =   1268610.75\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1265539.875000[  100/  100]\n",
      "\n",
      "running train loss =   1265539.875\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1262470.500000[  100/  100]\n",
      "\n",
      "running train loss =   1262470.5\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1259402.750000[  100/  100]\n",
      "\n",
      "running train loss =   1259402.75\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1256336.375000[  100/  100]\n",
      "\n",
      "running train loss =   1256336.375\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1253271.625000[  100/  100]\n",
      "\n",
      "running train loss =   1253271.625\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1250208.500000[  100/  100]\n",
      "\n",
      "running train loss =   1250208.5\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1247147.000000[  100/  100]\n",
      "\n",
      "running train loss =   1247147.0\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1244087.125000[  100/  100]\n",
      "\n",
      "running train loss =   1244087.125\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1241028.750000[  100/  100]\n",
      "\n",
      "running train loss =   1241028.75\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1237972.125000[  100/  100]\n",
      "\n",
      "running train loss =   1237972.125\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1234917.125000[  100/  100]\n",
      "\n",
      "running train loss =   1234917.125\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1231863.875000[  100/  100]\n",
      "\n",
      "running train loss =   1231863.875\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1228812.500000[  100/  100]\n",
      "\n",
      "running train loss =   1228812.5\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1225762.750000[  100/  100]\n",
      "\n",
      "running train loss =   1225762.75\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1222714.750000[  100/  100]\n",
      "\n",
      "running train loss =   1222714.75\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1219668.500000[  100/  100]\n",
      "\n",
      "running train loss =   1219668.5\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1216624.250000[  100/  100]\n",
      "\n",
      "running train loss =   1216624.25\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1213581.750000[  100/  100]\n",
      "\n",
      "running train loss =   1213581.75\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1210541.125000[  100/  100]\n",
      "\n",
      "running train loss =   1210541.125\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1207502.500000[  100/  100]\n",
      "\n",
      "running train loss =   1207502.5\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1204465.625000[  100/  100]\n",
      "\n",
      "running train loss =   1204465.625\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1201430.875000[  100/  100]\n",
      "\n",
      "running train loss =   1201430.875\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1198398.125000[  100/  100]\n",
      "\n",
      "running train loss =   1198398.125\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1195367.250000[  100/  100]\n",
      "\n",
      "running train loss =   1195367.25\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1192338.375000[  100/  100]\n",
      "\n",
      "running train loss =   1192338.375\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1189311.500000[  100/  100]\n",
      "\n",
      "running train loss =   1189311.5\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1186286.875000[  100/  100]\n",
      "\n",
      "running train loss =   1186286.875\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1183264.375000[  100/  100]\n",
      "\n",
      "running train loss =   1183264.375\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1180244.125000[  100/  100]\n",
      "\n",
      "running train loss =   1180244.125\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1177225.750000[  100/  100]\n",
      "\n",
      "running train loss =   1177225.75\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1174209.625000[  100/  100]\n",
      "\n",
      "running train loss =   1174209.625\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1171195.625000[  100/  100]\n",
      "\n",
      "running train loss =   1171195.625\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1168183.875000[  100/  100]\n",
      "\n",
      "running train loss =   1168183.875\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1165174.500000[  100/  100]\n",
      "\n",
      "running train loss =   1165174.5\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1162167.375000[  100/  100]\n",
      "\n",
      "running train loss =   1162167.375\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1159162.375000[  100/  100]\n",
      "\n",
      "running train loss =   1159162.375\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1156159.875000[  100/  100]\n",
      "\n",
      "running train loss =   1156159.875\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1153159.750000[  100/  100]\n",
      "\n",
      "running train loss =   1153159.75\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1150161.875000[  100/  100]\n",
      "\n",
      "running train loss =   1150161.875\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1147166.375000[  100/  100]\n",
      "\n",
      "running train loss =   1147166.375\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1144173.375000[  100/  100]\n",
      "\n",
      "running train loss =   1144173.375\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1141182.750000[  100/  100]\n",
      "\n",
      "running train loss =   1141182.75\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1138194.750000[  100/  100]\n",
      "\n",
      "running train loss =   1138194.75\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1135209.125000[  100/  100]\n",
      "\n",
      "running train loss =   1135209.125\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1132226.125000[  100/  100]\n",
      "\n",
      "running train loss =   1132226.125\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1129245.500000[  100/  100]\n",
      "\n",
      "running train loss =   1129245.5\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1126267.500000[  100/  100]\n",
      "\n",
      "running train loss =   1126267.5\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1123292.375000[  100/  100]\n",
      "\n",
      "running train loss =   1123292.375\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1120319.500000[  100/  100]\n",
      "\n",
      "running train loss =   1120319.5\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1117349.500000[  100/  100]\n",
      "\n",
      "running train loss =   1117349.5\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1114381.875000[  100/  100]\n",
      "\n",
      "running train loss =   1114381.875\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1111417.250000[  100/  100]\n",
      "\n",
      "running train loss =   1111417.25\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1108455.250000[  100/  100]\n",
      "\n",
      "running train loss =   1108455.25\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1105496.000000[  100/  100]\n",
      "\n",
      "running train loss =   1105496.0\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1102539.375000[  100/  100]\n",
      "\n",
      "running train loss =   1102539.375\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1099585.625000[  100/  100]\n",
      "\n",
      "running train loss =   1099585.625\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1096634.750000[  100/  100]\n",
      "\n",
      "running train loss =   1096634.75\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1093686.625000[  100/  100]\n",
      "\n",
      "running train loss =   1093686.625\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1090741.500000[  100/  100]\n",
      "\n",
      "running train loss =   1090741.5\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1087799.125000[  100/  100]\n",
      "\n",
      "running train loss =   1087799.125\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1084859.500000[  100/  100]\n",
      "\n",
      "running train loss =   1084859.5\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1081922.875000[  100/  100]\n",
      "\n",
      "running train loss =   1081922.875\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1078989.000000[  100/  100]\n",
      "\n",
      "running train loss =   1078989.0\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1076058.250000[  100/  100]\n",
      "\n",
      "running train loss =   1076058.25\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1073129.875000[  100/  100]\n",
      "\n",
      "running train loss =   1073129.875\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1070204.750000[  100/  100]\n",
      "\n",
      "running train loss =   1070204.75\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1067282.125000[  100/  100]\n",
      "\n",
      "running train loss =   1067282.125\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1064362.375000[  100/  100]\n",
      "\n",
      "running train loss =   1064362.375\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1061445.500000[  100/  100]\n",
      "\n",
      "running train loss =   1061445.5\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1058531.625000[  100/  100]\n",
      "\n",
      "running train loss =   1058531.625\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1055620.750000[  100/  100]\n",
      "\n",
      "running train loss =   1055620.75\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1052712.875000[  100/  100]\n",
      "\n",
      "running train loss =   1052712.875\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1049808.125000[  100/  100]\n",
      "\n",
      "running train loss =   1049808.125\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1046906.250000[  100/  100]\n",
      "\n",
      "running train loss =   1046906.25\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1044007.500000[  100/  100]\n",
      "\n",
      "running train loss =   1044007.5\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1041112.000000[  100/  100]\n",
      "\n",
      "running train loss =   1041112.0\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1038219.625000[  100/  100]\n",
      "\n",
      "running train loss =   1038219.625\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1035330.312500[  100/  100]\n",
      "\n",
      "running train loss =   1035330.3125\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1032444.062500[  100/  100]\n",
      "\n",
      "running train loss =   1032444.0625\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1029561.250000[  100/  100]\n",
      "\n",
      "running train loss =   1029561.25\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1026681.687500[  100/  100]\n",
      "\n",
      "running train loss =   1026681.6875\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1023805.187500[  100/  100]\n",
      "\n",
      "running train loss =   1023805.1875\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1020932.187500[  100/  100]\n",
      "\n",
      "running train loss =   1020932.1875\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1018062.187500[  100/  100]\n",
      "\n",
      "running train loss =   1018062.1875\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1015195.687500[  100/  100]\n",
      "\n",
      "running train loss =   1015195.6875\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1012332.562500[  100/  100]\n",
      "\n",
      "running train loss =   1012332.5625\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1009472.625000[  100/  100]\n",
      "\n",
      "running train loss =   1009472.625\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1006616.187500[  100/  100]\n",
      "\n",
      "running train loss =   1006616.1875\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1003762.937500[  100/  100]\n",
      "\n",
      "running train loss =   1003762.9375\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1000913.250000[  100/  100]\n",
      "\n",
      "running train loss =   1000913.25\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 998067.062500[  100/  100]\n",
      "\n",
      "running train loss =   998067.0625\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 995224.250000[  100/  100]\n",
      "\n",
      "running train loss =   995224.25\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 992384.750000[  100/  100]\n",
      "\n",
      "running train loss =   992384.75\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 989548.812500[  100/  100]\n",
      "\n",
      "running train loss =   989548.8125\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 986716.375000[  100/  100]\n",
      "\n",
      "running train loss =   986716.375\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 983887.437500[  100/  100]\n",
      "\n",
      "running train loss =   983887.4375\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 981062.062500[  100/  100]\n",
      "\n",
      "running train loss =   981062.0625\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 978240.062500[  100/  100]\n",
      "\n",
      "running train loss =   978240.0625\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 975421.750000[  100/  100]\n",
      "\n",
      "running train loss =   975421.75\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 972607.062500[  100/  100]\n",
      "\n",
      "running train loss =   972607.0625\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 969795.937500[  100/  100]\n",
      "\n",
      "running train loss =   969795.9375\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 966988.375000[  100/  100]\n",
      "\n",
      "running train loss =   966988.375\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 964184.250000[  100/  100]\n",
      "\n",
      "running train loss =   964184.25\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 961384.000000[  100/  100]\n",
      "\n",
      "running train loss =   961384.0\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 958587.500000[  100/  100]\n",
      "\n",
      "running train loss =   958587.5\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 955794.562500[  100/  100]\n",
      "\n",
      "running train loss =   955794.5625\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 953005.375000[  100/  100]\n",
      "\n",
      "running train loss =   953005.375\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 950219.812500[  100/  100]\n",
      "\n",
      "running train loss =   950219.8125\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 947438.062500[  100/  100]\n",
      "\n",
      "running train loss =   947438.0625\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 944660.187500[  100/  100]\n",
      "\n",
      "running train loss =   944660.1875\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 941886.000000[  100/  100]\n",
      "\n",
      "running train loss =   941886.0\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 939115.750000[  100/  100]\n",
      "\n",
      "running train loss =   939115.75\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 936349.125000[  100/  100]\n",
      "\n",
      "running train loss =   936349.125\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 933586.625000[  100/  100]\n",
      "\n",
      "running train loss =   933586.625\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 930827.812500[  100/  100]\n",
      "\n",
      "running train loss =   930827.8125\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 928072.937500[  100/  100]\n",
      "\n",
      "running train loss =   928072.9375\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 925322.187500[  100/  100]\n",
      "\n",
      "running train loss =   925322.1875\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 922575.187500[  100/  100]\n",
      "\n",
      "running train loss =   922575.1875\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 919832.250000[  100/  100]\n",
      "\n",
      "running train loss =   919832.25\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 917093.250000[  100/  100]\n",
      "\n",
      "running train loss =   917093.25\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 914358.312500[  100/  100]\n",
      "\n",
      "running train loss =   914358.3125\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 911627.437500[  100/  100]\n",
      "\n",
      "running train loss =   911627.4375\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 908900.750000[  100/  100]\n",
      "\n",
      "running train loss =   908900.75\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 906177.937500[  100/  100]\n",
      "\n",
      "running train loss =   906177.9375\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 903459.250000[  100/  100]\n",
      "\n",
      "running train loss =   903459.25\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 900744.875000[  100/  100]\n",
      "\n",
      "running train loss =   900744.875\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 898034.625000[  100/  100]\n",
      "\n",
      "running train loss =   898034.625\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 895328.500000[  100/  100]\n",
      "\n",
      "running train loss =   895328.5\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 892626.625000[  100/  100]\n",
      "\n",
      "running train loss =   892626.625\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 889928.875000[  100/  100]\n",
      "\n",
      "running train loss =   889928.875\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 887235.375000[  100/  100]\n",
      "\n",
      "running train loss =   887235.375\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 884546.062500[  100/  100]\n",
      "\n",
      "running train loss =   884546.0625\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 881861.250000[  100/  100]\n",
      "\n",
      "running train loss =   881861.25\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 879180.625000[  100/  100]\n",
      "\n",
      "running train loss =   879180.625\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 876504.375000[  100/  100]\n",
      "\n",
      "running train loss =   876504.375\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 873832.500000[  100/  100]\n",
      "\n",
      "running train loss =   873832.5\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 871164.750000[  100/  100]\n",
      "\n",
      "running train loss =   871164.75\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 868501.625000[  100/  100]\n",
      "\n",
      "running train loss =   868501.625\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 865842.812500[  100/  100]\n",
      "\n",
      "running train loss =   865842.8125\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 863188.375000[  100/  100]\n",
      "\n",
      "running train loss =   863188.375\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 860538.500000[  100/  100]\n",
      "\n",
      "running train loss =   860538.5\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 857893.062500[  100/  100]\n",
      "\n",
      "running train loss =   857893.0625\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 855252.000000[  100/  100]\n",
      "\n",
      "running train loss =   855252.0\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 852615.375000[  100/  100]\n",
      "\n",
      "running train loss =   852615.375\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 849983.375000[  100/  100]\n",
      "\n",
      "running train loss =   849983.375\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 847355.937500[  100/  100]\n",
      "\n",
      "running train loss =   847355.9375\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 844732.937500[  100/  100]\n",
      "\n",
      "running train loss =   844732.9375\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 842114.625000[  100/  100]\n",
      "\n",
      "running train loss =   842114.625\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 839500.875000[  100/  100]\n",
      "\n",
      "running train loss =   839500.875\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 836891.687500[  100/  100]\n",
      "\n",
      "running train loss =   836891.6875\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 834287.062500[  100/  100]\n",
      "\n",
      "running train loss =   834287.0625\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 831687.062500[  100/  100]\n",
      "\n",
      "running train loss =   831687.0625\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 829091.812500[  100/  100]\n",
      "\n",
      "running train loss =   829091.8125\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 826501.125000[  100/  100]\n",
      "\n",
      "running train loss =   826501.125\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 823915.187500[  100/  100]\n",
      "\n",
      "running train loss =   823915.1875\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 821334.000000[  100/  100]\n",
      "\n",
      "running train loss =   821334.0\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 818757.437500[  100/  100]\n",
      "\n",
      "running train loss =   818757.4375\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 816185.625000[  100/  100]\n",
      "\n",
      "running train loss =   816185.625\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 813618.500000[  100/  100]\n",
      "\n",
      "running train loss =   813618.5\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 811056.187500[  100/  100]\n",
      "\n",
      "running train loss =   811056.1875\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 808498.625000[  100/  100]\n",
      "\n",
      "running train loss =   808498.625\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 805945.812500[  100/  100]\n",
      "\n",
      "running train loss =   805945.8125\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 803397.937500[  100/  100]\n",
      "\n",
      "running train loss =   803397.9375\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 800854.812500[  100/  100]\n",
      "\n",
      "running train loss =   800854.8125\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 798316.500000[  100/  100]\n",
      "\n",
      "running train loss =   798316.5\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 795783.062500[  100/  100]\n",
      "\n",
      "running train loss =   795783.0625\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 793254.312500[  100/  100]\n",
      "\n",
      "running train loss =   793254.3125\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 790730.562500[  100/  100]\n",
      "\n",
      "running train loss =   790730.5625\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 788211.687500[  100/  100]\n",
      "\n",
      "running train loss =   788211.6875\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 785697.750000[  100/  100]\n",
      "\n",
      "running train loss =   785697.75\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 783188.812500[  100/  100]\n",
      "\n",
      "running train loss =   783188.8125\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 780684.562500[  100/  100]\n",
      "\n",
      "running train loss =   780684.5625\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 778185.437500[  100/  100]\n",
      "\n",
      "running train loss =   778185.4375\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 775691.187500[  100/  100]\n",
      "\n",
      "running train loss =   775691.1875\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 773201.937500[  100/  100]\n",
      "\n",
      "running train loss =   773201.9375\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 770717.687500[  100/  100]\n",
      "\n",
      "running train loss =   770717.6875\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 768238.500000[  100/  100]\n",
      "\n",
      "running train loss =   768238.5\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 765764.000000[  100/  100]\n",
      "\n",
      "running train loss =   765764.0\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 763294.750000[  100/  100]\n",
      "\n",
      "running train loss =   763294.75\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 760830.500000[  100/  100]\n",
      "\n",
      "running train loss =   760830.5\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 758371.250000[  100/  100]\n",
      "\n",
      "running train loss =   758371.25\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 755917.125000[  100/  100]\n",
      "\n",
      "running train loss =   755917.125\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 753467.937500[  100/  100]\n",
      "\n",
      "running train loss =   753467.9375\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 751024.000000[  100/  100]\n",
      "\n",
      "running train loss =   751024.0\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 748584.937500[  100/  100]\n",
      "\n",
      "running train loss =   748584.9375\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 746151.125000[  100/  100]\n",
      "\n",
      "running train loss =   746151.125\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 743722.312500[  100/  100]\n",
      "\n",
      "running train loss =   743722.3125\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 741298.625000[  100/  100]\n",
      "\n",
      "running train loss =   741298.625\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 738880.187500[  100/  100]\n",
      "\n",
      "running train loss =   738880.1875\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 736466.625000[  100/  100]\n",
      "\n",
      "running train loss =   736466.625\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 734058.375000[  100/  100]\n",
      "\n",
      "running train loss =   734058.375\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 731655.250000[  100/  100]\n",
      "\n",
      "running train loss =   731655.25\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 729257.250000[  100/  100]\n",
      "\n",
      "running train loss =   729257.25\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 726864.500000[  100/  100]\n",
      "\n",
      "running train loss =   726864.5\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 724476.937500[  100/  100]\n",
      "\n",
      "running train loss =   724476.9375\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 722094.562500[  100/  100]\n",
      "\n",
      "running train loss =   722094.5625\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 719717.375000[  100/  100]\n",
      "\n",
      "running train loss =   719717.375\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 717345.250000[  100/  100]\n",
      "\n",
      "running train loss =   717345.25\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 714978.562500[  100/  100]\n",
      "\n",
      "running train loss =   714978.5625\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 712616.937500[  100/  100]\n",
      "\n",
      "running train loss =   712616.9375\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 710260.625000[  100/  100]\n",
      "\n",
      "running train loss =   710260.625\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 707909.625000[  100/  100]\n",
      "\n",
      "running train loss =   707909.625\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 705563.750000[  100/  100]\n",
      "\n",
      "running train loss =   705563.75\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 703223.125000[  100/  100]\n",
      "\n",
      "running train loss =   703223.125\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 700887.687500[  100/  100]\n",
      "\n",
      "running train loss =   700887.6875\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 698557.625000[  100/  100]\n",
      "\n",
      "running train loss =   698557.625\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 696232.812500[  100/  100]\n",
      "\n",
      "running train loss =   696232.8125\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 693913.187500[  100/  100]\n",
      "\n",
      "running train loss =   693913.1875\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 691598.875000[  100/  100]\n",
      "\n",
      "running train loss =   691598.875\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 689289.937500[  100/  100]\n",
      "\n",
      "running train loss =   689289.9375\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 686986.187500[  100/  100]\n",
      "\n",
      "running train loss =   686986.1875\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 684687.750000[  100/  100]\n",
      "\n",
      "running train loss =   684687.75\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 682394.750000[  100/  100]\n",
      "\n",
      "running train loss =   682394.75\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 680106.875000[  100/  100]\n",
      "\n",
      "running train loss =   680106.875\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 677824.375000[  100/  100]\n",
      "\n",
      "running train loss =   677824.375\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 675547.250000[  100/  100]\n",
      "\n",
      "running train loss =   675547.25\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 673275.437500[  100/  100]\n",
      "\n",
      "running train loss =   673275.4375\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 671008.937500[  100/  100]\n",
      "\n",
      "running train loss =   671008.9375\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 668747.750000[  100/  100]\n",
      "\n",
      "running train loss =   668747.75\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 666492.062500[  100/  100]\n",
      "\n",
      "running train loss =   666492.0625\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 664241.625000[  100/  100]\n",
      "\n",
      "running train loss =   664241.625\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 661996.500000[  100/  100]\n",
      "\n",
      "running train loss =   661996.5\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 659756.875000[  100/  100]\n",
      "\n",
      "running train loss =   659756.875\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 657522.500000[  100/  100]\n",
      "\n",
      "running train loss =   657522.5\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 655293.562500[  100/  100]\n",
      "\n",
      "running train loss =   655293.5625\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 653070.062500[  100/  100]\n",
      "\n",
      "running train loss =   653070.0625\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 650851.812500[  100/  100]\n",
      "\n",
      "running train loss =   650851.8125\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 648639.187500[  100/  100]\n",
      "\n",
      "running train loss =   648639.1875\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 646431.750000[  100/  100]\n",
      "\n",
      "running train loss =   646431.75\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 644229.812500[  100/  100]\n",
      "\n",
      "running train loss =   644229.8125\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 642033.312500[  100/  100]\n",
      "\n",
      "running train loss =   642033.3125\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 639842.250000[  100/  100]\n",
      "\n",
      "running train loss =   639842.25\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 637656.625000[  100/  100]\n",
      "\n",
      "running train loss =   637656.625\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 635476.437500[  100/  100]\n",
      "\n",
      "running train loss =   635476.4375\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 633301.687500[  100/  100]\n",
      "\n",
      "running train loss =   633301.6875\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 631132.375000[  100/  100]\n",
      "\n",
      "running train loss =   631132.375\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 628968.625000[  100/  100]\n",
      "\n",
      "running train loss =   628968.625\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 626810.250000[  100/  100]\n",
      "\n",
      "running train loss =   626810.25\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 624657.250000[  100/  100]\n",
      "\n",
      "running train loss =   624657.25\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 622509.937500[  100/  100]\n",
      "\n",
      "running train loss =   622509.9375\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 620368.000000[  100/  100]\n",
      "\n",
      "running train loss =   620368.0\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 618231.687500[  100/  100]\n",
      "\n",
      "running train loss =   618231.6875\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 616100.875000[  100/  100]\n",
      "\n",
      "running train loss =   616100.875\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 613975.437500[  100/  100]\n",
      "\n",
      "running train loss =   613975.4375\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 611855.750000[  100/  100]\n",
      "\n",
      "running train loss =   611855.75\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 609741.437500[  100/  100]\n",
      "\n",
      "running train loss =   609741.4375\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 607632.625000[  100/  100]\n",
      "\n",
      "running train loss =   607632.625\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 605529.437500[  100/  100]\n",
      "\n",
      "running train loss =   605529.4375\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 603431.812500[  100/  100]\n",
      "\n",
      "running train loss =   603431.8125\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 601339.687500[  100/  100]\n",
      "\n",
      "running train loss =   601339.6875\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 599253.187500[  100/  100]\n",
      "\n",
      "running train loss =   599253.1875\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 597172.187500[  100/  100]\n",
      "\n",
      "running train loss =   597172.1875\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 595096.750000[  100/  100]\n",
      "\n",
      "running train loss =   595096.75\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 593027.000000[  100/  100]\n",
      "\n",
      "running train loss =   593027.0\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 590962.750000[  100/  100]\n",
      "\n",
      "running train loss =   590962.75\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 588904.125000[  100/  100]\n",
      "\n",
      "running train loss =   588904.125\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 586851.125000[  100/  100]\n",
      "\n",
      "running train loss =   586851.125\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 584803.625000[  100/  100]\n",
      "\n",
      "running train loss =   584803.625\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 582761.687500[  100/  100]\n",
      "\n",
      "running train loss =   582761.6875\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 580725.500000[  100/  100]\n",
      "\n",
      "running train loss =   580725.5\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 578694.750000[  100/  100]\n",
      "\n",
      "running train loss =   578694.75\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 576669.687500[  100/  100]\n",
      "\n",
      "running train loss =   576669.6875\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 574650.125000[  100/  100]\n",
      "\n",
      "running train loss =   574650.125\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 572636.187500[  100/  100]\n",
      "\n",
      "running train loss =   572636.1875\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 570627.750000[  100/  100]\n",
      "\n",
      "running train loss =   570627.75\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 568624.875000[  100/  100]\n",
      "\n",
      "running train loss =   568624.875\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 566627.500000[  100/  100]\n",
      "\n",
      "running train loss =   566627.5\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 564635.562500[  100/  100]\n",
      "\n",
      "running train loss =   564635.5625\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 562649.187500[  100/  100]\n",
      "\n",
      "running train loss =   562649.1875\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 560668.437500[  100/  100]\n",
      "\n",
      "running train loss =   560668.4375\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 558693.250000[  100/  100]\n",
      "\n",
      "running train loss =   558693.25\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 556723.687500[  100/  100]\n",
      "\n",
      "running train loss =   556723.6875\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 554759.687500[  100/  100]\n",
      "\n",
      "running train loss =   554759.6875\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 552801.250000[  100/  100]\n",
      "\n",
      "running train loss =   552801.25\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 550848.375000[  100/  100]\n",
      "\n",
      "running train loss =   550848.375\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 548901.187500[  100/  100]\n",
      "\n",
      "running train loss =   548901.1875\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 546959.562500[  100/  100]\n",
      "\n",
      "running train loss =   546959.5625\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 545023.500000[  100/  100]\n",
      "\n",
      "running train loss =   545023.5\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 543093.250000[  100/  100]\n",
      "\n",
      "running train loss =   543093.25\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 541168.437500[  100/  100]\n",
      "\n",
      "running train loss =   541168.4375\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 539249.312500[  100/  100]\n",
      "\n",
      "running train loss =   539249.3125\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 537335.812500[  100/  100]\n",
      "\n",
      "running train loss =   537335.8125\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 535427.937500[  100/  100]\n",
      "\n",
      "running train loss =   535427.9375\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 533525.687500[  100/  100]\n",
      "\n",
      "running train loss =   533525.6875\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 531629.062500[  100/  100]\n",
      "\n",
      "running train loss =   531629.0625\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 529738.125000[  100/  100]\n",
      "\n",
      "running train loss =   529738.125\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 527852.875000[  100/  100]\n",
      "\n",
      "running train loss =   527852.875\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 525973.187500[  100/  100]\n",
      "\n",
      "running train loss =   525973.1875\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 524099.187500[  100/  100]\n",
      "\n",
      "running train loss =   524099.1875\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 522230.968750[  100/  100]\n",
      "\n",
      "running train loss =   522230.96875\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 520368.281250[  100/  100]\n",
      "\n",
      "running train loss =   520368.28125\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 518511.312500[  100/  100]\n",
      "\n",
      "running train loss =   518511.3125\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 516660.031250[  100/  100]\n",
      "\n",
      "running train loss =   516660.03125\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 514814.531250[  100/  100]\n",
      "\n",
      "running train loss =   514814.53125\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 512974.562500[  100/  100]\n",
      "\n",
      "running train loss =   512974.5625\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 511140.281250[  100/  100]\n",
      "\n",
      "running train loss =   511140.28125\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 509311.687500[  100/  100]\n",
      "\n",
      "running train loss =   509311.6875\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 507488.875000[  100/  100]\n",
      "\n",
      "running train loss =   507488.875\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 505671.625000[  100/  100]\n",
      "\n",
      "running train loss =   505671.625\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 503860.156250[  100/  100]\n",
      "\n",
      "running train loss =   503860.15625\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 502054.312500[  100/  100]\n",
      "\n",
      "running train loss =   502054.3125\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 500254.250000[  100/  100]\n",
      "\n",
      "running train loss =   500254.25\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 498459.843750[  100/  100]\n",
      "\n",
      "running train loss =   498459.84375\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 496671.125000[  100/  100]\n",
      "\n",
      "running train loss =   496671.125\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 494888.187500[  100/  100]\n",
      "\n",
      "running train loss =   494888.1875\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 493110.843750[  100/  100]\n",
      "\n",
      "running train loss =   493110.84375\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 491339.281250[  100/  100]\n",
      "\n",
      "running train loss =   491339.28125\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 489573.375000[  100/  100]\n",
      "\n",
      "running train loss =   489573.375\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 487813.187500[  100/  100]\n",
      "\n",
      "running train loss =   487813.1875\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 486058.687500[  100/  100]\n",
      "\n",
      "running train loss =   486058.6875\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 484309.875000[  100/  100]\n",
      "\n",
      "running train loss =   484309.875\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 482566.812500[  100/  100]\n",
      "\n",
      "running train loss =   482566.8125\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 480829.437500[  100/  100]\n",
      "\n",
      "running train loss =   480829.4375\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 479097.687500[  100/  100]\n",
      "\n",
      "running train loss =   479097.6875\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 477371.812500[  100/  100]\n",
      "\n",
      "running train loss =   477371.8125\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 475651.593750[  100/  100]\n",
      "\n",
      "running train loss =   475651.59375\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 473937.000000[  100/  100]\n",
      "\n",
      "running train loss =   473937.0\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 472228.156250[  100/  100]\n",
      "\n",
      "running train loss =   472228.15625\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 470525.031250[  100/  100]\n",
      "\n",
      "running train loss =   470525.03125\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 468827.562500[  100/  100]\n",
      "\n",
      "running train loss =   468827.5625\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 467135.812500[  100/  100]\n",
      "\n",
      "running train loss =   467135.8125\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 465449.750000[  100/  100]\n",
      "\n",
      "running train loss =   465449.75\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 463769.375000[  100/  100]\n",
      "\n",
      "running train loss =   463769.375\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 462094.812500[  100/  100]\n",
      "\n",
      "running train loss =   462094.8125\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 460425.843750[  100/  100]\n",
      "\n",
      "running train loss =   460425.84375\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 458762.562500[  100/  100]\n",
      "\n",
      "running train loss =   458762.5625\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 457105.000000[  100/  100]\n",
      "\n",
      "running train loss =   457105.0\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 455453.125000[  100/  100]\n",
      "\n",
      "running train loss =   455453.125\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 453806.875000[  100/  100]\n",
      "\n",
      "running train loss =   453806.875\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 452166.312500[  100/  100]\n",
      "\n",
      "running train loss =   452166.3125\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 450531.593750[  100/  100]\n",
      "\n",
      "running train loss =   450531.59375\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 448902.437500[  100/  100]\n",
      "\n",
      "running train loss =   448902.4375\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 447278.968750[  100/  100]\n",
      "\n",
      "running train loss =   447278.96875\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 445661.125000[  100/  100]\n",
      "\n",
      "running train loss =   445661.125\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 444049.031250[  100/  100]\n",
      "\n",
      "running train loss =   444049.03125\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 442442.562500[  100/  100]\n",
      "\n",
      "running train loss =   442442.5625\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 440841.812500[  100/  100]\n",
      "\n",
      "running train loss =   440841.8125\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 439246.687500[  100/  100]\n",
      "\n",
      "running train loss =   439246.6875\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 437657.250000[  100/  100]\n",
      "\n",
      "running train loss =   437657.25\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 436073.406250[  100/  100]\n",
      "\n",
      "running train loss =   436073.40625\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 434495.250000[  100/  100]\n",
      "\n",
      "running train loss =   434495.25\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 432922.687500[  100/  100]\n",
      "\n",
      "running train loss =   432922.6875\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 431355.812500[  100/  100]\n",
      "\n",
      "running train loss =   431355.8125\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 429794.593750[  100/  100]\n",
      "\n",
      "running train loss =   429794.59375\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 428239.000000[  100/  100]\n",
      "\n",
      "running train loss =   428239.0\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 426689.031250[  100/  100]\n",
      "\n",
      "running train loss =   426689.03125\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 425144.718750[  100/  100]\n",
      "\n",
      "running train loss =   425144.71875\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 423605.968750[  100/  100]\n",
      "\n",
      "running train loss =   423605.96875\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 422072.843750[  100/  100]\n",
      "\n",
      "running train loss =   422072.84375\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 420545.406250[  100/  100]\n",
      "\n",
      "running train loss =   420545.40625\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 419023.593750[  100/  100]\n",
      "\n",
      "running train loss =   419023.59375\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 417507.281250[  100/  100]\n",
      "\n",
      "running train loss =   417507.28125\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 415996.593750[  100/  100]\n",
      "\n",
      "running train loss =   415996.59375\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 414491.531250[  100/  100]\n",
      "\n",
      "running train loss =   414491.53125\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 412992.031250[  100/  100]\n",
      "\n",
      "running train loss =   412992.03125\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 411498.156250[  100/  100]\n",
      "\n",
      "running train loss =   411498.15625\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 410009.875000[  100/  100]\n",
      "\n",
      "running train loss =   410009.875\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 408527.156250[  100/  100]\n",
      "\n",
      "running train loss =   408527.15625\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 407049.968750[  100/  100]\n",
      "\n",
      "running train loss =   407049.96875\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 405578.312500[  100/  100]\n",
      "\n",
      "running train loss =   405578.3125\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 404112.281250[  100/  100]\n",
      "\n",
      "running train loss =   404112.28125\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 402651.750000[  100/  100]\n",
      "\n",
      "running train loss =   402651.75\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 401196.875000[  100/  100]\n",
      "\n",
      "running train loss =   401196.875\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 399747.437500[  100/  100]\n",
      "\n",
      "running train loss =   399747.4375\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 398303.531250[  100/  100]\n",
      "\n",
      "running train loss =   398303.53125\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 396865.156250[  100/  100]\n",
      "\n",
      "running train loss =   396865.15625\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 395432.312500[  100/  100]\n",
      "\n",
      "running train loss =   395432.3125\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 394005.093750[  100/  100]\n",
      "\n",
      "running train loss =   394005.09375\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 392583.250000[  100/  100]\n",
      "\n",
      "running train loss =   392583.25\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 391166.906250[  100/  100]\n",
      "\n",
      "running train loss =   391166.90625\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 389756.093750[  100/  100]\n",
      "\n",
      "running train loss =   389756.09375\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 388350.718750[  100/  100]\n",
      "\n",
      "running train loss =   388350.71875\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 386951.000000[  100/  100]\n",
      "\n",
      "running train loss =   386951.0\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 385556.625000[  100/  100]\n",
      "\n",
      "running train loss =   385556.625\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 384167.750000[  100/  100]\n",
      "\n",
      "running train loss =   384167.75\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 382784.281250[  100/  100]\n",
      "\n",
      "running train loss =   382784.28125\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 381406.250000[  100/  100]\n",
      "\n",
      "running train loss =   381406.25\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 380033.812500[  100/  100]\n",
      "\n",
      "running train loss =   380033.8125\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 378666.687500[  100/  100]\n",
      "\n",
      "running train loss =   378666.6875\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 377305.031250[  100/  100]\n",
      "\n",
      "running train loss =   377305.03125\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 375948.812500[  100/  100]\n",
      "\n",
      "running train loss =   375948.8125\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 374597.968750[  100/  100]\n",
      "\n",
      "running train loss =   374597.96875\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 373252.593750[  100/  100]\n",
      "\n",
      "running train loss =   373252.59375\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 371912.562500[  100/  100]\n",
      "\n",
      "running train loss =   371912.5625\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 370578.000000[  100/  100]\n",
      "\n",
      "running train loss =   370578.0\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 369248.750000[  100/  100]\n",
      "\n",
      "running train loss =   369248.75\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 367924.875000[  100/  100]\n",
      "\n",
      "running train loss =   367924.875\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 366606.437500[  100/  100]\n",
      "\n",
      "running train loss =   366606.4375\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 365293.375000[  100/  100]\n",
      "\n",
      "running train loss =   365293.375\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 363985.593750[  100/  100]\n",
      "\n",
      "running train loss =   363985.59375\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 362683.156250[  100/  100]\n",
      "\n",
      "running train loss =   362683.15625\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 361386.125000[  100/  100]\n",
      "\n",
      "running train loss =   361386.125\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 360094.406250[  100/  100]\n",
      "\n",
      "running train loss =   360094.40625\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 358807.968750[  100/  100]\n",
      "\n",
      "running train loss =   358807.96875\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 357526.812500[  100/  100]\n",
      "\n",
      "running train loss =   357526.8125\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 356251.093750[  100/  100]\n",
      "\n",
      "running train loss =   356251.09375\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 354980.531250[  100/  100]\n",
      "\n",
      "running train loss =   354980.53125\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 353715.312500[  100/  100]\n",
      "\n",
      "running train loss =   353715.3125\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 352455.375000[  100/  100]\n",
      "\n",
      "running train loss =   352455.375\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 351200.718750[  100/  100]\n",
      "\n",
      "running train loss =   351200.71875\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 349951.281250[  100/  100]\n",
      "\n",
      "running train loss =   349951.28125\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 348707.156250[  100/  100]\n",
      "\n",
      "running train loss =   348707.15625\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 347468.156250[  100/  100]\n",
      "\n",
      "running train loss =   347468.15625\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 346234.562500[  100/  100]\n",
      "\n",
      "running train loss =   346234.5625\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 345006.093750[  100/  100]\n",
      "\n",
      "running train loss =   345006.09375\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 343782.843750[  100/  100]\n",
      "\n",
      "running train loss =   343782.84375\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 342564.812500[  100/  100]\n",
      "\n",
      "running train loss =   342564.8125\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 341351.906250[  100/  100]\n",
      "\n",
      "running train loss =   341351.90625\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 340144.281250[  100/  100]\n",
      "\n",
      "running train loss =   340144.28125\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 338941.843750[  100/  100]\n",
      "\n",
      "running train loss =   338941.84375\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 337744.468750[  100/  100]\n",
      "\n",
      "running train loss =   337744.46875\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 336552.375000[  100/  100]\n",
      "\n",
      "running train loss =   336552.375\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 335365.312500[  100/  100]\n",
      "\n",
      "running train loss =   335365.3125\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 334183.437500[  100/  100]\n",
      "\n",
      "running train loss =   334183.4375\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 333006.687500[  100/  100]\n",
      "\n",
      "running train loss =   333006.6875\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 331835.062500[  100/  100]\n",
      "\n",
      "running train loss =   331835.0625\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 330668.562500[  100/  100]\n",
      "\n",
      "running train loss =   330668.5625\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 329507.125000[  100/  100]\n",
      "\n",
      "running train loss =   329507.125\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 328350.750000[  100/  100]\n",
      "\n",
      "running train loss =   328350.75\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 327199.531250[  100/  100]\n",
      "\n",
      "running train loss =   327199.53125\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 326053.375000[  100/  100]\n",
      "\n",
      "running train loss =   326053.375\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 324912.187500[  100/  100]\n",
      "\n",
      "running train loss =   324912.1875\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 323776.125000[  100/  100]\n",
      "\n",
      "running train loss =   323776.125\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 322645.031250[  100/  100]\n",
      "\n",
      "running train loss =   322645.03125\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 321519.031250[  100/  100]\n",
      "\n",
      "running train loss =   321519.03125\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 320398.031250[  100/  100]\n",
      "\n",
      "running train loss =   320398.03125\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 319282.000000[  100/  100]\n",
      "\n",
      "running train loss =   319282.0\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 318171.031250[  100/  100]\n",
      "\n",
      "running train loss =   318171.03125\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 317064.968750[  100/  100]\n",
      "\n",
      "running train loss =   317064.96875\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 315963.906250[  100/  100]\n",
      "\n",
      "running train loss =   315963.90625\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 314867.875000[  100/  100]\n",
      "\n",
      "running train loss =   314867.875\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 313776.718750[  100/  100]\n",
      "\n",
      "running train loss =   313776.71875\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 312690.437500[  100/  100]\n",
      "\n",
      "running train loss =   312690.4375\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 311609.218750[  100/  100]\n",
      "\n",
      "running train loss =   311609.21875\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 310532.750000[  100/  100]\n",
      "\n",
      "running train loss =   310532.75\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 309461.343750[  100/  100]\n",
      "\n",
      "running train loss =   309461.34375\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2440186.500000[  100/  200]\n",
      "\n",
      "running train loss =   2453230.25\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2423175.750000[  100/  200]\n",
      "\n",
      "running train loss =   2452514.625\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2335353.000000[  100/  200]\n",
      "\n",
      "running train loss =   2451795.0\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2466629.000000[  100/  200]\n",
      "\n",
      "running train loss =   2451079.75\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2566298.500000[  100/  200]\n",
      "\n",
      "running train loss =   2450356.25\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2387439.000000[  100/  200]\n",
      "\n",
      "running train loss =   2449620.375\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2417471.250000[  100/  200]\n",
      "\n",
      "running train loss =   2448882.125\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2418008.000000[  100/  200]\n",
      "\n",
      "running train loss =   2448135.0\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2445258.000000[  100/  200]\n",
      "\n",
      "running train loss =   2447377.125\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2511033.500000[  100/  200]\n",
      "\n",
      "running train loss =   2446606.0\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2426771.500000[  100/  200]\n",
      "\n",
      "running train loss =   2445813.375\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2454093.000000[  100/  200]\n",
      "\n",
      "running train loss =   2445006.5\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2519067.750000[  100/  200]\n",
      "\n",
      "running train loss =   2444184.875\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2438573.500000[  100/  200]\n",
      "\n",
      "running train loss =   2443340.25\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2450151.000000[  100/  200]\n",
      "\n",
      "running train loss =   2442469.875\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2458350.000000[  100/  200]\n",
      "\n",
      "running train loss =   2441575.125\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2415323.250000[  100/  200]\n",
      "\n",
      "running train loss =   2440654.125\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2408798.250000[  100/  200]\n",
      "\n",
      "running train loss =   2439713.875\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2544419.000000[  100/  200]\n",
      "\n",
      "running train loss =   2438744.375\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2470950.750000[  100/  200]\n",
      "\n",
      "running train loss =   2437741.25\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2475627.750000[  100/  200]\n",
      "\n",
      "running train loss =   2436718.5\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2405200.000000[  100/  200]\n",
      "\n",
      "running train loss =   2435666.625\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2518349.500000[  100/  200]\n",
      "\n",
      "running train loss =   2434585.25\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2406022.250000[  100/  200]\n",
      "\n",
      "running train loss =   2433471.875\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2491057.000000[  100/  200]\n",
      "\n",
      "running train loss =   2432333.75\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2451352.750000[  100/  200]\n",
      "\n",
      "running train loss =   2431161.875\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2448304.250000[  100/  200]\n",
      "\n",
      "running train loss =   2429964.875\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2442500.250000[  100/  200]\n",
      "\n",
      "running train loss =   2428743.125\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2407473.750000[  100/  200]\n",
      "\n",
      "running train loss =   2427486.875\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2414887.250000[  100/  200]\n",
      "\n",
      "running train loss =   2426200.625\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2403790.250000[  100/  200]\n",
      "\n",
      "running train loss =   2424890.75\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2375928.750000[  100/  200]\n",
      "\n",
      "running train loss =   2423550.875\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2390765.250000[  100/  200]\n",
      "\n",
      "running train loss =   2422185.125\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2460175.000000[  100/  200]\n",
      "\n",
      "running train loss =   2420790.25\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2479545.500000[  100/  200]\n",
      "\n",
      "running train loss =   2419360.25\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2480411.500000[  100/  200]\n",
      "\n",
      "running train loss =   2417899.0\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2271850.000000[  100/  200]\n",
      "\n",
      "running train loss =   2416407.75\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2382633.500000[  100/  200]\n",
      "\n",
      "running train loss =   2414892.25\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2419328.250000[  100/  200]\n",
      "\n",
      "running train loss =   2413343.625\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2398829.000000[  100/  200]\n",
      "\n",
      "running train loss =   2411761.375\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2438295.000000[  100/  200]\n",
      "\n",
      "running train loss =   2410151.0\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2364039.250000[  100/  200]\n",
      "\n",
      "running train loss =   2408508.625\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2458797.250000[  100/  200]\n",
      "\n",
      "running train loss =   2406835.875\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2421826.000000[  100/  200]\n",
      "\n",
      "running train loss =   2405127.375\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2438896.750000[  100/  200]\n",
      "\n",
      "running train loss =   2403388.25\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2467945.000000[  100/  200]\n",
      "\n",
      "running train loss =   2401614.875\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2403850.250000[  100/  200]\n",
      "\n",
      "running train loss =   2399806.0\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2359978.500000[  100/  200]\n",
      "\n",
      "running train loss =   2397974.0\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2350643.500000[  100/  200]\n",
      "\n",
      "running train loss =   2396096.375\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2478696.250000[  100/  200]\n",
      "\n",
      "running train loss =   2394199.75\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2356215.250000[  100/  200]\n",
      "\n",
      "running train loss =   2392264.0\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2428387.500000[  100/  200]\n",
      "\n",
      "running train loss =   2390287.5\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2427675.000000[  100/  200]\n",
      "\n",
      "running train loss =   2388287.75\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2453263.750000[  100/  200]\n",
      "\n",
      "running train loss =   2386249.125\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2381510.500000[  100/  200]\n",
      "\n",
      "running train loss =   2384169.125\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2429223.250000[  100/  200]\n",
      "\n",
      "running train loss =   2382067.5\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2378439.000000[  100/  200]\n",
      "\n",
      "running train loss =   2379922.375\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2404729.750000[  100/  200]\n",
      "\n",
      "running train loss =   2377738.75\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2312486.500000[  100/  200]\n",
      "\n",
      "running train loss =   2375529.0\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2307403.750000[  100/  200]\n",
      "\n",
      "running train loss =   2373273.625\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2373788.250000[  100/  200]\n",
      "\n",
      "running train loss =   2371006.0\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2373637.000000[  100/  200]\n",
      "\n",
      "running train loss =   2368688.375\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2370143.250000[  100/  200]\n",
      "\n",
      "running train loss =   2366335.125\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2384893.500000[  100/  200]\n",
      "\n",
      "running train loss =   2363945.125\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2343384.750000[  100/  200]\n",
      "\n",
      "running train loss =   2361516.625\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2285761.500000[  100/  200]\n",
      "\n",
      "running train loss =   2359067.75\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2343136.000000[  100/  200]\n",
      "\n",
      "running train loss =   2356571.625\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2316663.500000[  100/  200]\n",
      "\n",
      "running train loss =   2354055.0\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2407218.500000[  100/  200]\n",
      "\n",
      "running train loss =   2351489.625\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2323321.750000[  100/  200]\n",
      "\n",
      "running train loss =   2348881.25\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2339414.750000[  100/  200]\n",
      "\n",
      "running train loss =   2346246.375\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2370838.000000[  100/  200]\n",
      "\n",
      "running train loss =   2343583.75\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2276662.000000[  100/  200]\n",
      "\n",
      "running train loss =   2340872.75\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2374102.500000[  100/  200]\n",
      "\n",
      "running train loss =   2338140.25\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2419680.250000[  100/  200]\n",
      "\n",
      "running train loss =   2335373.625\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2307381.500000[  100/  200]\n",
      "\n",
      "running train loss =   2332544.0\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2209367.750000[  100/  200]\n",
      "\n",
      "running train loss =   2329703.125\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2392010.500000[  100/  200]\n",
      "\n",
      "running train loss =   2326828.75\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2361567.000000[  100/  200]\n",
      "\n",
      "running train loss =   2323920.625\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2341466.000000[  100/  200]\n",
      "\n",
      "running train loss =   2320954.5\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2296191.250000[  100/  200]\n",
      "\n",
      "running train loss =   2317968.875\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2279024.500000[  100/  200]\n",
      "\n",
      "running train loss =   2314945.375\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2333752.750000[  100/  200]\n",
      "\n",
      "running train loss =   2311886.125\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2286015.750000[  100/  200]\n",
      "\n",
      "running train loss =   2308790.5\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2269160.000000[  100/  200]\n",
      "\n",
      "running train loss =   2305661.375\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2315742.750000[  100/  200]\n",
      "\n",
      "running train loss =   2302501.625\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2256555.250000[  100/  200]\n",
      "\n",
      "running train loss =   2299289.5\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2359936.000000[  100/  200]\n",
      "\n",
      "running train loss =   2296061.75\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2295938.250000[  100/  200]\n",
      "\n",
      "running train loss =   2292775.0\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2228864.750000[  100/  200]\n",
      "\n",
      "running train loss =   2289468.0\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2355755.000000[  100/  200]\n",
      "\n",
      "running train loss =   2286129.875\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2240914.000000[  100/  200]\n",
      "\n",
      "running train loss =   2282738.375\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2217355.250000[  100/  200]\n",
      "\n",
      "running train loss =   2279299.75\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2270528.750000[  100/  200]\n",
      "\n",
      "running train loss =   2275865.875\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2246631.750000[  100/  200]\n",
      "\n",
      "running train loss =   2272368.875\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2272204.750000[  100/  200]\n",
      "\n",
      "running train loss =   2268838.25\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2223593.750000[  100/  200]\n",
      "\n",
      "running train loss =   2265271.0\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2331032.500000[  100/  200]\n",
      "\n",
      "running train loss =   2261699.5\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2301968.250000[  100/  200]\n",
      "\n",
      "running train loss =   2258042.375\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2268518.250000[  100/  200]\n",
      "\n",
      "running train loss =   2254376.375\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2265363.750000[  100/  200]\n",
      "\n",
      "running train loss =   2250668.0\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2321394.000000[  100/  200]\n",
      "\n",
      "running train loss =   2246946.875\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2308555.750000[  100/  200]\n",
      "\n",
      "running train loss =   2243168.625\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2192696.750000[  100/  200]\n",
      "\n",
      "running train loss =   2239348.375\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2259383.500000[  100/  200]\n",
      "\n",
      "running train loss =   2235508.375\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2280028.000000[  100/  200]\n",
      "\n",
      "running train loss =   2231637.875\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2224820.250000[  100/  200]\n",
      "\n",
      "running train loss =   2227716.375\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2110940.250000[  100/  200]\n",
      "\n",
      "running train loss =   2223757.25\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2265010.750000[  100/  200]\n",
      "\n",
      "running train loss =   2219802.0\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2259604.250000[  100/  200]\n",
      "\n",
      "running train loss =   2215792.5\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2201359.000000[  100/  200]\n",
      "\n",
      "running train loss =   2211729.375\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2200871.750000[  100/  200]\n",
      "\n",
      "running train loss =   2207657.125\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2176756.750000[  100/  200]\n",
      "\n",
      "running train loss =   2203534.125\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2198065.500000[  100/  200]\n",
      "\n",
      "running train loss =   2199398.25\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2225772.000000[  100/  200]\n",
      "\n",
      "running train loss =   2195224.375\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2227583.750000[  100/  200]\n",
      "\n",
      "running train loss =   2191009.375\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2167070.500000[  100/  200]\n",
      "\n",
      "running train loss =   2186741.375\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2170158.500000[  100/  200]\n",
      "\n",
      "running train loss =   2182476.5\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2131033.750000[  100/  200]\n",
      "\n",
      "running train loss =   2178169.875\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2169351.250000[  100/  200]\n",
      "\n",
      "running train loss =   2173846.375\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2227557.500000[  100/  200]\n",
      "\n",
      "running train loss =   2169474.25\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2171578.000000[  100/  200]\n",
      "\n",
      "running train loss =   2165051.125\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2193102.250000[  100/  200]\n",
      "\n",
      "running train loss =   2160635.25\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2195191.750000[  100/  200]\n",
      "\n",
      "running train loss =   2156158.125\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2075345.250000[  100/  200]\n",
      "\n",
      "running train loss =   2151648.375\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2114113.500000[  100/  200]\n",
      "\n",
      "running train loss =   2147125.75\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2114808.250000[  100/  200]\n",
      "\n",
      "running train loss =   2142595.0\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2218495.500000[  100/  200]\n",
      "\n",
      "running train loss =   2138018.0\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2127404.750000[  100/  200]\n",
      "\n",
      "running train loss =   2133386.375\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2127032.750000[  100/  200]\n",
      "\n",
      "running train loss =   2128726.5\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2158612.750000[  100/  200]\n",
      "\n",
      "running train loss =   2124075.625\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2065605.125000[  100/  200]\n",
      "\n",
      "running train loss =   2119348.4375\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2098395.250000[  100/  200]\n",
      "\n",
      "running train loss =   2114636.375\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2073113.625000[  100/  200]\n",
      "\n",
      "running train loss =   2109870.1875\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2079931.000000[  100/  200]\n",
      "\n",
      "running train loss =   2105087.625\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2107741.500000[  100/  200]\n",
      "\n",
      "running train loss =   2100250.8125\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2128425.500000[  100/  200]\n",
      "\n",
      "running train loss =   2095417.3125\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2119323.000000[  100/  200]\n",
      "\n",
      "running train loss =   2090535.8125\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2072830.750000[  100/  200]\n",
      "\n",
      "running train loss =   2085630.25\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2022993.625000[  100/  200]\n",
      "\n",
      "running train loss =   2080691.1875\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2086766.125000[  100/  200]\n",
      "\n",
      "running train loss =   2075733.8125\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2072861.000000[  100/  200]\n",
      "\n",
      "running train loss =   2070745.9375\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2096570.125000[  100/  200]\n",
      "\n",
      "running train loss =   2065732.3125\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2021819.000000[  100/  200]\n",
      "\n",
      "running train loss =   2060660.375\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2096684.375000[  100/  200]\n",
      "\n",
      "running train loss =   2055616.5\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2001752.000000[  100/  200]\n",
      "\n",
      "running train loss =   2050497.125\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2109326.500000[  100/  200]\n",
      "\n",
      "running train loss =   2045407.4375\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2051110.250000[  100/  200]\n",
      "\n",
      "running train loss =   2040230.9375\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2168107.250000[  100/  200]\n",
      "\n",
      "running train loss =   2035079.125\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2073528.500000[  100/  200]\n",
      "\n",
      "running train loss =   2029866.3125\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2032892.375000[  100/  200]\n",
      "\n",
      "running train loss =   2024617.9375\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2030949.875000[  100/  200]\n",
      "\n",
      "running train loss =   2019373.875\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1986646.125000[  100/  200]\n",
      "\n",
      "running train loss =   2014084.25\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2018468.000000[  100/  200]\n",
      "\n",
      "running train loss =   2008785.1875\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1980827.875000[  100/  200]\n",
      "\n",
      "running train loss =   2003440.8125\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2033191.875000[  100/  200]\n",
      "\n",
      "running train loss =   1998109.0\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1960625.000000[  100/  200]\n",
      "\n",
      "running train loss =   1992726.125\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2034533.625000[  100/  200]\n",
      "\n",
      "running train loss =   1987323.125\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1972655.000000[  100/  200]\n",
      "\n",
      "running train loss =   1981897.5625\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2004373.000000[  100/  200]\n",
      "\n",
      "running train loss =   1976470.5625\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2002945.875000[  100/  200]\n",
      "\n",
      "running train loss =   1970980.1875\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1969808.750000[  100/  200]\n",
      "\n",
      "running train loss =   1965491.125\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1957297.875000[  100/  200]\n",
      "\n",
      "running train loss =   1959962.375\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1918309.625000[  100/  200]\n",
      "\n",
      "running train loss =   1954421.9375\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1925157.125000[  100/  200]\n",
      "\n",
      "running train loss =   1948862.25\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1947628.500000[  100/  200]\n",
      "\n",
      "running train loss =   1943293.6875\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1932128.125000[  100/  200]\n",
      "\n",
      "running train loss =   1937679.5\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1866075.250000[  100/  200]\n",
      "\n",
      "running train loss =   1932041.0625\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1911930.875000[  100/  200]\n",
      "\n",
      "running train loss =   1926411.25\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1930399.000000[  100/  200]\n",
      "\n",
      "running train loss =   1920728.75\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1928852.500000[  100/  200]\n",
      "\n",
      "running train loss =   1915045.875\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1850961.125000[  100/  200]\n",
      "\n",
      "running train loss =   1909317.75\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1920320.500000[  100/  200]\n",
      "\n",
      "running train loss =   1903615.625\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1941874.375000[  100/  200]\n",
      "\n",
      "running train loss =   1897857.8125\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1874418.250000[  100/  200]\n",
      "\n",
      "running train loss =   1892074.75\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1852991.000000[  100/  200]\n",
      "\n",
      "running train loss =   1886287.6875\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1866585.750000[  100/  200]\n",
      "\n",
      "running train loss =   1880500.375\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1861753.875000[  100/  200]\n",
      "\n",
      "running train loss =   1874670.1875\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1831821.500000[  100/  200]\n",
      "\n",
      "running train loss =   1868806.6875\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1866877.250000[  100/  200]\n",
      "\n",
      "running train loss =   1862949.125\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1907257.250000[  100/  200]\n",
      "\n",
      "running train loss =   1857091.375\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1909533.750000[  100/  200]\n",
      "\n",
      "running train loss =   1851192.125\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1904162.375000[  100/  200]\n",
      "\n",
      "running train loss =   1845275.4375\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1899961.250000[  100/  200]\n",
      "\n",
      "running train loss =   1839337.375\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1786230.500000[  100/  200]\n",
      "\n",
      "running train loss =   1833361.1875\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1780633.875000[  100/  200]\n",
      "\n",
      "running train loss =   1827388.4375\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1799511.250000[  100/  200]\n",
      "\n",
      "running train loss =   1821394.8125\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1857069.250000[  100/  200]\n",
      "\n",
      "running train loss =   1815426.3125\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1723594.875000[  100/  200]\n",
      "\n",
      "running train loss =   1809387.25\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1770603.000000[  100/  200]\n",
      "\n",
      "running train loss =   1803351.125\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1835246.125000[  100/  200]\n",
      "\n",
      "running train loss =   1797333.875\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1788750.500000[  100/  200]\n",
      "\n",
      "running train loss =   1791238.3125\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1836517.875000[  100/  200]\n",
      "\n",
      "running train loss =   1785193.5\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1794143.625000[  100/  200]\n",
      "\n",
      "running train loss =   1779083.3125\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1741801.625000[  100/  200]\n",
      "\n",
      "running train loss =   1772957.5625\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1746242.750000[  100/  200]\n",
      "\n",
      "running train loss =   1766851.9375\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1759723.500000[  100/  200]\n",
      "\n",
      "running train loss =   1760718.0\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1716890.250000[  100/  200]\n",
      "\n",
      "running train loss =   1754563.625\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1674381.250000[  100/  200]\n",
      "\n",
      "running train loss =   1748409.1875\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1812069.000000[  100/  200]\n",
      "\n",
      "running train loss =   1742257.0625\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1619402.875000[  100/  200]\n",
      "\n",
      "running train loss =   1736014.5\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1763770.500000[  100/  200]\n",
      "\n",
      "running train loss =   1729860.0625\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1664278.875000[  100/  200]\n",
      "\n",
      "running train loss =   1723646.9375\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1758437.000000[  100/  200]\n",
      "\n",
      "running train loss =   1717428.5\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1673265.250000[  100/  200]\n",
      "\n",
      "running train loss =   1711168.5625\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1649426.375000[  100/  200]\n",
      "\n",
      "running train loss =   1704929.75\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1742393.625000[  100/  200]\n",
      "\n",
      "running train loss =   1698702.75\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1731075.875000[  100/  200]\n",
      "\n",
      "running train loss =   1692438.5\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1617659.500000[  100/  200]\n",
      "\n",
      "running train loss =   1686110.375\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1670266.375000[  100/  200]\n",
      "\n",
      "running train loss =   1679857.8125\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1626490.500000[  100/  200]\n",
      "\n",
      "running train loss =   1673555.625\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1674991.250000[  100/  200]\n",
      "\n",
      "running train loss =   1667270.875\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1728449.750000[  100/  200]\n",
      "\n",
      "running train loss =   1660942.8125\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1656594.875000[  100/  200]\n",
      "\n",
      "running train loss =   1654609.1875\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1661516.500000[  100/  200]\n",
      "\n",
      "running train loss =   1648303.875\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1642381.875000[  100/  200]\n",
      "\n",
      "running train loss =   1641946.625\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1608228.500000[  100/  200]\n",
      "\n",
      "running train loss =   1635587.1875\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1557987.500000[  100/  200]\n",
      "\n",
      "running train loss =   1629205.375\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1626568.625000[  100/  200]\n",
      "\n",
      "running train loss =   1622881.25\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1651573.500000[  100/  200]\n",
      "\n",
      "running train loss =   1616504.75\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1644157.250000[  100/  200]\n",
      "\n",
      "running train loss =   1610132.375\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1656655.625000[  100/  200]\n",
      "\n",
      "running train loss =   1603736.9375\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1534994.375000[  100/  200]\n",
      "\n",
      "running train loss =   1597317.375\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1640624.000000[  100/  200]\n",
      "\n",
      "running train loss =   1590960.5625\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1651538.750000[  100/  200]\n",
      "\n",
      "running train loss =   1584570.875\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1618946.375000[  100/  200]\n",
      "\n",
      "running train loss =   1578123.125\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1579604.000000[  100/  200]\n",
      "\n",
      "running train loss =   1571721.125\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1634423.375000[  100/  200]\n",
      "\n",
      "running train loss =   1565311.1875\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1618997.625000[  100/  200]\n",
      "\n",
      "running train loss =   1558897.6875\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1582446.500000[  100/  200]\n",
      "\n",
      "running train loss =   1552441.875\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1584638.750000[  100/  200]\n",
      "\n",
      "running train loss =   1546000.3125\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1550786.500000[  100/  200]\n",
      "\n",
      "running train loss =   1539546.0\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1576788.125000[  100/  200]\n",
      "\n",
      "running train loss =   1533134.8125\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1569057.500000[  100/  200]\n",
      "\n",
      "running train loss =   1526687.625\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1467705.500000[  100/  200]\n",
      "\n",
      "running train loss =   1520206.125\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1512004.500000[  100/  200]\n",
      "\n",
      "running train loss =   1513775.125\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1545174.250000[  100/  200]\n",
      "\n",
      "running train loss =   1507332.5\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1587434.375000[  100/  200]\n",
      "\n",
      "running train loss =   1500879.25\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1473569.625000[  100/  200]\n",
      "\n",
      "running train loss =   1494369.75\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1456176.000000[  100/  200]\n",
      "\n",
      "running train loss =   1487906.75\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1454317.250000[  100/  200]\n",
      "\n",
      "running train loss =   1481433.25\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1482222.875000[  100/  200]\n",
      "\n",
      "running train loss =   1474993.25\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1388212.375000[  100/  200]\n",
      "\n",
      "running train loss =   1468468.125\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1465755.375000[  100/  200]\n",
      "\n",
      "running train loss =   1462018.9375\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1472090.125000[  100/  200]\n",
      "\n",
      "running train loss =   1455536.5\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1478372.000000[  100/  200]\n",
      "\n",
      "running train loss =   1449065.75\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1447065.625000[  100/  200]\n",
      "\n",
      "running train loss =   1442584.3125\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1426573.625000[  100/  200]\n",
      "\n",
      "running train loss =   1436075.6875\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1413397.250000[  100/  200]\n",
      "\n",
      "running train loss =   1429570.4375\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1486435.000000[  100/  200]\n",
      "\n",
      "running train loss =   1423137.1875\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1414793.500000[  100/  200]\n",
      "\n",
      "running train loss =   1416624.5\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1478071.625000[  100/  200]\n",
      "\n",
      "running train loss =   1410150.375\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1367780.625000[  100/  200]\n",
      "\n",
      "running train loss =   1403603.0625\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1423468.500000[  100/  200]\n",
      "\n",
      "running train loss =   1397141.3125\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1432039.000000[  100/  200]\n",
      "\n",
      "running train loss =   1390661.0\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1411316.625000[  100/  200]\n",
      "\n",
      "running train loss =   1384163.25\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1412193.000000[  100/  200]\n",
      "\n",
      "running train loss =   1377670.5\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1381901.750000[  100/  200]\n",
      "\n",
      "running train loss =   1371167.75\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1338803.625000[  100/  200]\n",
      "\n",
      "running train loss =   1364677.3125\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1401199.250000[  100/  200]\n",
      "\n",
      "running train loss =   1358244.6875\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1371774.125000[  100/  200]\n",
      "\n",
      "running train loss =   1351743.125\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1333951.750000[  100/  200]\n",
      "\n",
      "running train loss =   1345250.0625\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1362949.250000[  100/  200]\n",
      "\n",
      "running train loss =   1338788.1875\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1312942.125000[  100/  200]\n",
      "\n",
      "running train loss =   1332284.875\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1295866.125000[  100/  200]\n",
      "\n",
      "running train loss =   1325814.0\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1265315.250000[  100/  200]\n",
      "\n",
      "running train loss =   1319349.0\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1309109.000000[  100/  200]\n",
      "\n",
      "running train loss =   1312901.6875\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1346702.375000[  100/  200]\n",
      "\n",
      "running train loss =   1306454.875\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1254077.000000[  100/  200]\n",
      "\n",
      "running train loss =   1299970.3125\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1327502.125000[  100/  200]\n",
      "\n",
      "running train loss =   1293535.875\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1177157.625000[  100/  200]\n",
      "\n",
      "running train loss =   1287043.4375\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1277293.250000[  100/  200]\n",
      "\n",
      "running train loss =   1280655.75\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1285740.125000[  100/  200]\n",
      "\n",
      "running train loss =   1274208.0\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1286127.500000[  100/  200]\n",
      "\n",
      "running train loss =   1267796.5625\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1235925.875000[  100/  200]\n",
      "\n",
      "running train loss =   1261336.5625\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1283810.375000[  100/  200]\n",
      "\n",
      "running train loss =   1254947.5\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1269201.000000[  100/  200]\n",
      "\n",
      "running train loss =   1248516.1875\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1248111.625000[  100/  200]\n",
      "\n",
      "running train loss =   1242118.9375\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1290840.375000[  100/  200]\n",
      "\n",
      "running train loss =   1235715.5625\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1255093.750000[  100/  200]\n",
      "\n",
      "running train loss =   1229322.125\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1214505.000000[  100/  200]\n",
      "\n",
      "running train loss =   1222915.375\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1209241.250000[  100/  200]\n",
      "\n",
      "running train loss =   1216527.125\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1189095.250000[  100/  200]\n",
      "\n",
      "running train loss =   1210148.1875\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1246538.750000[  100/  200]\n",
      "\n",
      "running train loss =   1203818.3125\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1187442.375000[  100/  200]\n",
      "\n",
      "running train loss =   1197445.25\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1246799.750000[  100/  200]\n",
      "\n",
      "running train loss =   1191091.0625\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1239239.625000[  100/  200]\n",
      "\n",
      "running train loss =   1184751.6875\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1161460.750000[  100/  200]\n",
      "\n",
      "running train loss =   1178370.625\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1207993.000000[  100/  200]\n",
      "\n",
      "running train loss =   1172094.75\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1153978.750000[  100/  200]\n",
      "\n",
      "running train loss =   1165748.5\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1128510.500000[  100/  200]\n",
      "\n",
      "running train loss =   1159442.625\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1164710.125000[  100/  200]\n",
      "\n",
      "running train loss =   1153147.25\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1129314.000000[  100/  200]\n",
      "\n",
      "running train loss =   1146849.875\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1134240.500000[  100/  200]\n",
      "\n",
      "running train loss =   1140566.1875\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1149447.125000[  100/  200]\n",
      "\n",
      "running train loss =   1134307.625\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1094886.875000[  100/  200]\n",
      "\n",
      "running train loss =   1128018.1875\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1147054.500000[  100/  200]\n",
      "\n",
      "running train loss =   1121795.9375\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1077864.375000[  100/  200]\n",
      "\n",
      "running train loss =   1115545.375\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1102099.000000[  100/  200]\n",
      "\n",
      "running train loss =   1109318.5625\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1116433.125000[  100/  200]\n",
      "\n",
      "running train loss =   1103095.625\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1129910.500000[  100/  200]\n",
      "\n",
      "running train loss =   1096892.3125\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1047173.937500[  100/  200]\n",
      "\n",
      "running train loss =   1090642.78125\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1111150.500000[  100/  200]\n",
      "\n",
      "running train loss =   1084492.875\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1092244.750000[  100/  200]\n",
      "\n",
      "running train loss =   1078305.5\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1112416.625000[  100/  200]\n",
      "\n",
      "running train loss =   1072152.34375\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1026603.500000[  100/  200]\n",
      "\n",
      "running train loss =   1065954.6875\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1028815.125000[  100/  200]\n",
      "\n",
      "running train loss =   1059806.75\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1136805.750000[  100/  200]\n",
      "\n",
      "running train loss =   1053746.4375\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1067992.625000[  100/  200]\n",
      "\n",
      "running train loss =   1047583.875\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1114819.500000[  100/  200]\n",
      "\n",
      "running train loss =   1041502.3125\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1095305.500000[  100/  200]\n",
      "\n",
      "running train loss =   1035403.59375\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1019135.062500[  100/  200]\n",
      "\n",
      "running train loss =   1029293.90625\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1072157.500000[  100/  200]\n",
      "\n",
      "running train loss =   1023263.15625\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1038424.562500[  100/  200]\n",
      "\n",
      "running train loss =   1017187.3125\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1016282.875000[  100/  200]\n",
      "\n",
      "running train loss =   1011135.3125\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1005493.250000[  100/  200]\n",
      "\n",
      "running train loss =   1005104.25\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 998758.937500[  100/  200]\n",
      "\n",
      "running train loss =   999101.28125\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1021820.562500[  100/  200]\n",
      "\n",
      "running train loss =   993124.28125\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1024165.187500[  100/  200]\n",
      "\n",
      "running train loss =   987152.28125\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 999662.062500[  100/  200]\n",
      "\n",
      "running train loss =   981198.28125\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1024515.937500[  100/  200]\n",
      "\n",
      "running train loss =   975249.65625\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 976011.375000[  100/  200]\n",
      "\n",
      "running train loss =   969283.3125\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 935805.625000[  100/  200]\n",
      "\n",
      "running train loss =   963368.375\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 941549.687500[  100/  200]\n",
      "\n",
      "running train loss =   957476.125\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 918007.750000[  100/  200]\n",
      "\n",
      "running train loss =   951599.59375\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 979899.375000[  100/  200]\n",
      "\n",
      "running train loss =   945767.96875\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 918426.375000[  100/  200]\n",
      "\n",
      "running train loss =   939875.46875\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 946590.000000[  100/  200]\n",
      "\n",
      "running train loss =   934079.15625\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 886988.562500[  100/  200]\n",
      "\n",
      "running train loss =   928227.71875\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 918302.250000[  100/  200]\n",
      "\n",
      "running train loss =   922439.25\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 924354.312500[  100/  200]\n",
      "\n",
      "running train loss =   916680.34375\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 946193.750000[  100/  200]\n",
      "\n",
      "running train loss =   910921.6875\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 899542.000000[  100/  200]\n",
      "\n",
      "running train loss =   905166.8125\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 859675.625000[  100/  200]\n",
      "\n",
      "running train loss =   899408.53125\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 923912.562500[  100/  200]\n",
      "\n",
      "running train loss =   893760.28125\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 893868.875000[  100/  200]\n",
      "\n",
      "running train loss =   888056.4375\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 922194.562500[  100/  200]\n",
      "\n",
      "running train loss =   882374.40625\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 905892.625000[  100/  200]\n",
      "\n",
      "running train loss =   876740.46875\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 896515.812500[  100/  200]\n",
      "\n",
      "running train loss =   871068.1875\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 816548.500000[  100/  200]\n",
      "\n",
      "running train loss =   865419.28125\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 839504.375000[  100/  200]\n",
      "\n",
      "running train loss =   859855.21875\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 808585.625000[  100/  200]\n",
      "\n",
      "running train loss =   854260.5\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 876739.250000[  100/  200]\n",
      "\n",
      "running train loss =   848743.3125\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 819640.750000[  100/  200]\n",
      "\n",
      "running train loss =   843169.46875\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 811021.437500[  100/  200]\n",
      "\n",
      "running train loss =   837653.28125\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 887092.750000[  100/  200]\n",
      "\n",
      "running train loss =   832205.96875\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 784403.187500[  100/  200]\n",
      "\n",
      "running train loss =   826651.84375\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 757936.062500[  100/  200]\n",
      "\n",
      "running train loss =   821193.0625\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 792555.187500[  100/  200]\n",
      "\n",
      "running train loss =   815765.0625\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 788258.500000[  100/  200]\n",
      "\n",
      "running train loss =   810360.21875\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 814637.062500[  100/  200]\n",
      "\n",
      "running train loss =   804965.625\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 765176.250000[  100/  200]\n",
      "\n",
      "running train loss =   799559.09375\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 782626.312500[  100/  200]\n",
      "\n",
      "running train loss =   794227.40625\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 820102.937500[  100/  200]\n",
      "\n",
      "running train loss =   788904.21875\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 827878.312500[  100/  200]\n",
      "\n",
      "running train loss =   783588.0\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 778875.125000[  100/  200]\n",
      "\n",
      "running train loss =   778268.4375\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 766903.687500[  100/  200]\n",
      "\n",
      "running train loss =   772968.46875\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 814084.625000[  100/  200]\n",
      "\n",
      "running train loss =   767753.3125\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 737343.125000[  100/  200]\n",
      "\n",
      "running train loss =   762447.75\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 794507.750000[  100/  200]\n",
      "\n",
      "running train loss =   757292.78125\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 797910.312500[  100/  200]\n",
      "\n",
      "running train loss =   752099.4375\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 707242.375000[  100/  200]\n",
      "\n",
      "running train loss =   746869.59375\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 797208.312500[  100/  200]\n",
      "\n",
      "running train loss =   741787.15625\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 730372.750000[  100/  200]\n",
      "\n",
      "running train loss =   736611.75\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 759234.312500[  100/  200]\n",
      "\n",
      "running train loss =   731538.15625\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 760546.875000[  100/  200]\n",
      "\n",
      "running train loss =   726458.1875\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 710394.812500[  100/  200]\n",
      "\n",
      "running train loss =   721377.25\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 759630.062500[  100/  200]\n",
      "\n",
      "running train loss =   716371.46875\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 720991.437500[  100/  200]\n",
      "\n",
      "running train loss =   711344.09375\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 679322.375000[  100/  200]\n",
      "\n",
      "running train loss =   706309.0625\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 652847.062500[  100/  200]\n",
      "\n",
      "running train loss =   701329.21875\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 729296.500000[  100/  200]\n",
      "\n",
      "running train loss =   696440.21875\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 662834.500000[  100/  200]\n",
      "\n",
      "running train loss =   691472.71875\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 651982.750000[  100/  200]\n",
      "\n",
      "running train loss =   686562.5\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 660571.500000[  100/  200]\n",
      "\n",
      "running train loss =   681697.71875\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 746441.500000[  100/  200]\n",
      "\n",
      "running train loss =   676898.34375\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 658682.187500[  100/  200]\n",
      "\n",
      "running train loss =   672019.53125\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 711065.187500[  100/  200]\n",
      "\n",
      "running train loss =   667241.5625\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 684669.812500[  100/  200]\n",
      "\n",
      "running train loss =   662434.9375\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 692591.187500[  100/  200]\n",
      "\n",
      "running train loss =   657686.9375\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 659450.375000[  100/  200]\n",
      "\n",
      "running train loss =   652928.53125\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 614214.812500[  100/  200]\n",
      "\n",
      "running train loss =   648172.96875\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 694223.750000[  100/  200]\n",
      "\n",
      "running train loss =   643549.5\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 631864.250000[  100/  200]\n",
      "\n",
      "running train loss =   638849.96875\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 680914.375000[  100/  200]\n",
      "\n",
      "running train loss =   634240.4375\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 637139.500000[  100/  200]\n",
      "\n",
      "running train loss =   629580.9375\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 601706.625000[  100/  200]\n",
      "\n",
      "running train loss =   624989.125\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 648711.500000[  100/  200]\n",
      "\n",
      "running train loss =   620448.0\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 573204.187500[  100/  200]\n",
      "\n",
      "running train loss =   615834.21875\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 566143.312500[  100/  200]\n",
      "\n",
      "running train loss =   611327.3125\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 597709.125000[  100/  200]\n",
      "\n",
      "running train loss =   606869.15625\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 645640.062500[  100/  200]\n",
      "\n",
      "running train loss =   602416.4375\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 643796.625000[  100/  200]\n",
      "\n",
      "running train loss =   597977.84375\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 594893.500000[  100/  200]\n",
      "\n",
      "running train loss =   593510.28125\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 615109.750000[  100/  200]\n",
      "\n",
      "running train loss =   589123.625\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 560486.125000[  100/  200]\n",
      "\n",
      "running train loss =   584720.46875\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 574871.812500[  100/  200]\n",
      "\n",
      "running train loss =   580383.78125\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 556819.062500[  100/  200]\n",
      "\n",
      "running train loss =   576048.53125\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 567173.687500[  100/  200]\n",
      "\n",
      "running train loss =   571748.46875\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 585738.375000[  100/  200]\n",
      "\n",
      "running train loss =   567483.28125\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 589772.750000[  100/  200]\n",
      "\n",
      "running train loss =   563223.40625\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 560042.812500[  100/  200]\n",
      "\n",
      "running train loss =   558985.5\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 593527.937500[  100/  200]\n",
      "\n",
      "running train loss =   554832.109375\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 496865.187500[  100/  200]\n",
      "\n",
      "running train loss =   550576.90625\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 556864.625000[  100/  200]\n",
      "\n",
      "running train loss =   546457.0625\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 545024.250000[  100/  200]\n",
      "\n",
      "running train loss =   542309.59375\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 505142.281250[  100/  200]\n",
      "\n",
      "running train loss =   538188.609375\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 532012.250000[  100/  200]\n",
      "\n",
      "running train loss =   534117.1875\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 498758.875000[  100/  200]\n",
      "\n",
      "running train loss =   530049.40625\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 519896.406250[  100/  200]\n",
      "\n",
      "running train loss =   526041.015625\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 522391.093750[  100/  200]\n",
      "\n",
      "running train loss =   522045.25\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 518729.000000[  100/  200]\n",
      "\n",
      "running train loss =   518030.9375\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 530021.750000[  100/  200]\n",
      "\n",
      "running train loss =   514096.8125\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 527999.812500[  100/  200]\n",
      "\n",
      "running train loss =   510167.421875\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 507150.593750[  100/  200]\n",
      "\n",
      "running train loss =   506220.71875\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 551555.500000[  100/  200]\n",
      "\n",
      "running train loss =   502387.1875\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 541121.000000[  100/  200]\n",
      "\n",
      "running train loss =   498510.859375\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 537967.937500[  100/  200]\n",
      "\n",
      "running train loss =   494664.546875\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 499484.000000[  100/  200]\n",
      "\n",
      "running train loss =   490805.59375\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 498425.000000[  100/  200]\n",
      "\n",
      "running train loss =   487050.375\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 458665.843750[  100/  200]\n",
      "\n",
      "running train loss =   483253.375\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 418811.812500[  100/  200]\n",
      "\n",
      "running train loss =   479468.25\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 505833.031250[  100/  200]\n",
      "\n",
      "running train loss =   475868.46875\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 445627.562500[  100/  200]\n",
      "\n",
      "running train loss =   472132.09375\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 480561.843750[  100/  200]\n",
      "\n",
      "running train loss =   468493.65625\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 483426.093750[  100/  200]\n",
      "\n",
      "running train loss =   464862.984375\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 456156.843750[  100/  200]\n",
      "\n",
      "running train loss =   461205.859375\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 428643.906250[  100/  200]\n",
      "\n",
      "running train loss =   457611.15625\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 438999.843750[  100/  200]\n",
      "\n",
      "running train loss =   454050.40625\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 438766.968750[  100/  200]\n",
      "\n",
      "running train loss =   450513.5\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 471601.812500[  100/  200]\n",
      "\n",
      "running train loss =   447055.984375\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 389331.875000[  100/  200]\n",
      "\n",
      "running train loss =   443471.984375\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 434382.625000[  100/  200]\n",
      "\n",
      "running train loss =   440058.8125\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 457186.281250[  100/  200]\n",
      "\n",
      "running train loss =   436621.203125\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 438730.750000[  100/  200]\n",
      "\n",
      "running train loss =   433188.953125\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 423987.875000[  100/  200]\n",
      "\n",
      "running train loss =   429778.4375\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 388065.468750[  100/  200]\n",
      "\n",
      "running train loss =   426406.96875\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 436971.156250[  100/  200]\n",
      "\n",
      "running train loss =   423103.15625\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 400726.312500[  100/  200]\n",
      "\n",
      "running train loss =   419767.59375\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 416150.625000[  100/  200]\n",
      "\n",
      "running train loss =   416492.71875\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 410556.000000[  100/  200]\n",
      "\n",
      "running train loss =   413204.6875\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 403998.875000[  100/  200]\n",
      "\n",
      "running train loss =   409951.359375\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 397880.593750[  100/  200]\n",
      "\n",
      "running train loss =   406727.65625\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 351094.468750[  100/  200]\n",
      "\n",
      "running train loss =   403526.640625\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 438199.593750[  100/  200]\n",
      "\n",
      "running train loss =   400383.0\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 386167.843750[  100/  200]\n",
      "\n",
      "running train loss =   397203.203125\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 434566.437500[  100/  200]\n",
      "\n",
      "running train loss =   394136.375\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 388658.375000[  100/  200]\n",
      "\n",
      "running train loss =   391002.0625\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 406133.187500[  100/  200]\n",
      "\n",
      "running train loss =   387931.875\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 371429.718750[  100/  200]\n",
      "\n",
      "running train loss =   384857.140625\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 371017.625000[  100/  200]\n",
      "\n",
      "running train loss =   381816.125\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 403632.531250[  100/  200]\n",
      "\n",
      "running train loss =   378838.8125\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 390997.468750[  100/  200]\n",
      "\n",
      "running train loss =   375841.25\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 361016.875000[  100/  200]\n",
      "\n",
      "running train loss =   372870.140625\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 356569.750000[  100/  200]\n",
      "\n",
      "running train loss =   369950.53125\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 417229.468750[  100/  200]\n",
      "\n",
      "running train loss =   367090.453125\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 393175.687500[  100/  200]\n",
      "\n",
      "running train loss =   364188.09375\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 338810.468750[  100/  200]\n",
      "\n",
      "running train loss =   361281.28125\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 337087.250000[  100/  200]\n",
      "\n",
      "running train loss =   358421.9375\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 329986.531250[  100/  200]\n",
      "\n",
      "running train loss =   355623.921875\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 372566.593750[  100/  200]\n",
      "\n",
      "running train loss =   352866.140625\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 357079.468750[  100/  200]\n",
      "\n",
      "running train loss =   350104.4375\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 330994.375000[  100/  200]\n",
      "\n",
      "running train loss =   347328.5\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 339507.468750[  100/  200]\n",
      "\n",
      "running train loss =   344625.296875\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 379163.750000[  100/  200]\n",
      "\n",
      "running train loss =   341951.328125\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 335276.125000[  100/  200]\n",
      "\n",
      "running train loss =   339234.796875\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 344280.125000[  100/  200]\n",
      "\n",
      "running train loss =   336603.125\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 345210.156250[  100/  200]\n",
      "\n",
      "running train loss =   333975.890625\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 370927.750000[  100/  200]\n",
      "\n",
      "running train loss =   331396.015625\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 318393.531250[  100/  200]\n",
      "\n",
      "running train loss =   328768.609375\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 338869.406250[  100/  200]\n",
      "\n",
      "running train loss =   326208.28125\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 323397.031250[  100/  200]\n",
      "\n",
      "running train loss =   323654.5625\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 309690.750000[  100/  200]\n",
      "\n",
      "running train loss =   321126.5625\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 280094.156250[  100/  200]\n",
      "\n",
      "running train loss =   318611.84375\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 297227.718750[  100/  200]\n",
      "\n",
      "running train loss =   316198.46875\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 296054.281250[  100/  200]\n",
      "\n",
      "running train loss =   313733.375\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 299355.750000[  100/  200]\n",
      "\n",
      "running train loss =   311293.375\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 322628.312500[  100/  200]\n",
      "\n",
      "running train loss =   308916.0625\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 289214.937500[  100/  200]\n",
      "\n",
      "running train loss =   306495.109375\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 284289.468750[  100/  200]\n",
      "\n",
      "running train loss =   304158.203125\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 304717.125000[  100/  200]\n",
      "\n",
      "running train loss =   301815.0625\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 333843.312500[  100/  200]\n",
      "\n",
      "running train loss =   299538.5625\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 294552.406250[  100/  200]\n",
      "\n",
      "running train loss =   297181.453125\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 326959.468750[  100/  200]\n",
      "\n",
      "running train loss =   294938.609375\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 321063.531250[  100/  200]\n",
      "\n",
      "running train loss =   292712.8125\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 284053.750000[  100/  200]\n",
      "\n",
      "running train loss =   290417.6875\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 247247.406250[  100/  200]\n",
      "\n",
      "running train loss =   288197.0625\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 280342.968750[  100/  200]\n",
      "\n",
      "running train loss =   286056.4375\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 268715.750000[  100/  200]\n",
      "\n",
      "running train loss =   283850.921875\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 290021.750000[  100/  200]\n",
      "\n",
      "running train loss =   281750.15625\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 252637.421875[  100/  200]\n",
      "\n",
      "running train loss =   279565.4140625\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 289988.250000[  100/  200]\n",
      "\n",
      "running train loss =   277485.6875\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 249842.921875[  100/  200]\n",
      "\n",
      "running train loss =   275395.8046875\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 284640.437500[  100/  200]\n",
      "\n",
      "running train loss =   273349.7578125\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 237279.359375[  100/  200]\n",
      "\n",
      "running train loss =   271264.3984375\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 219554.421875[  100/  200]\n",
      "\n",
      "running train loss =   269211.1171875\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 273468.500000[  100/  200]\n",
      "\n",
      "running train loss =   267313.546875\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 253421.859375[  100/  200]\n",
      "\n",
      "running train loss =   265277.7890625\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 279450.625000[  100/  200]\n",
      "\n",
      "running train loss =   263350.5390625\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 276789.468750[  100/  200]\n",
      "\n",
      "running train loss =   261362.625\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 240640.156250[  100/  200]\n",
      "\n",
      "running train loss =   259421.53125\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 288032.718750[  100/  200]\n",
      "\n",
      "running train loss =   257573.9375\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 233409.421875[  100/  200]\n",
      "\n",
      "running train loss =   255601.7265625\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 266072.250000[  100/  200]\n",
      "\n",
      "running train loss =   253814.15625\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 292314.781250[  100/  200]\n",
      "\n",
      "running train loss =   251992.546875\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 234484.312500[  100/  200]\n",
      "\n",
      "running train loss =   250083.703125\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 282203.031250[  100/  200]\n",
      "\n",
      "running train loss =   248328.7578125\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 242563.125000[  100/  200]\n",
      "\n",
      "running train loss =   246482.09375\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 234976.718750[  100/  200]\n",
      "\n",
      "running train loss =   244695.078125\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 271035.750000[  100/  200]\n",
      "\n",
      "running train loss =   243005.6328125\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 271525.718750[  100/  200]\n",
      "\n",
      "running train loss =   241265.2890625\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 235904.984375[  100/  200]\n",
      "\n",
      "running train loss =   239497.890625\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 258623.687500[  100/  200]\n",
      "\n",
      "running train loss =   237818.4765625\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 247100.703125[  100/  200]\n",
      "\n",
      "running train loss =   236148.1953125\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 245527.593750[  100/  200]\n",
      "\n",
      "running train loss =   234496.734375\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 190787.359375[  100/  200]\n",
      "\n",
      "running train loss =   232788.6640625\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 222786.500000[  100/  200]\n",
      "\n",
      "running train loss =   231214.6015625\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 213466.156250[  100/  200]\n",
      "\n",
      "running train loss =   229591.3203125\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 243708.312500[  100/  200]\n",
      "\n",
      "running train loss =   228023.6953125\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 264288.500000[  100/  200]\n",
      "\n",
      "running train loss =   226489.5390625\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 240346.937500[  100/  200]\n",
      "\n",
      "running train loss =   224873.6171875\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 179724.578125[  100/  200]\n",
      "\n",
      "running train loss =   223325.8046875\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 223564.562500[  100/  200]\n",
      "\n",
      "running train loss =   221806.328125\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 228344.875000[  100/  200]\n",
      "\n",
      "running train loss =   220328.7578125\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 207708.312500[  100/  200]\n",
      "\n",
      "running train loss =   218842.84375\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 221631.843750[  100/  200]\n",
      "\n",
      "running train loss =   217324.0\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 205403.515625[  100/  200]\n",
      "\n",
      "running train loss =   215889.4609375\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 237383.921875[  100/  200]\n",
      "\n",
      "running train loss =   214479.7734375\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 209499.765625[  100/  200]\n",
      "\n",
      "running train loss =   213045.640625\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 214154.265625[  100/  200]\n",
      "\n",
      "running train loss =   211628.9453125\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 201629.234375[  100/  200]\n",
      "\n",
      "running train loss =   210250.1875\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 180406.578125[  100/  200]\n",
      "\n",
      "running train loss =   208834.8359375\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 175113.656250[  100/  200]\n",
      "\n",
      "running train loss =   207492.609375\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 208908.281250[  100/  200]\n",
      "\n",
      "running train loss =   206177.359375\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 229012.406250[  100/  200]\n",
      "\n",
      "running train loss =   204905.2421875\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 186110.375000[  100/  200]\n",
      "\n",
      "running train loss =   203520.5703125\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 233771.421875[  100/  200]\n",
      "\n",
      "running train loss =   202245.3984375\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 187067.062500[  100/  200]\n",
      "\n",
      "running train loss =   200964.703125\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 158960.312500[  100/  200]\n",
      "\n",
      "running train loss =   199680.1171875\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 202692.046875[  100/  200]\n",
      "\n",
      "running train loss =   198474.6640625\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 198372.546875[  100/  200]\n",
      "\n",
      "running train loss =   197224.2421875\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 225624.640625[  100/  200]\n",
      "\n",
      "running train loss =   196041.7578125\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 207468.015625[  100/  200]\n",
      "\n",
      "running train loss =   194798.3046875\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 179153.000000[  100/  200]\n",
      "\n",
      "running train loss =   193576.5390625\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 200549.015625[  100/  200]\n",
      "\n",
      "running train loss =   192455.6796875\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 193273.218750[  100/  200]\n",
      "\n",
      "running train loss =   191247.3125\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 166741.843750[  100/  200]\n",
      "\n",
      "running train loss =   190081.3125\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 194402.562500[  100/  200]\n",
      "\n",
      "running train loss =   188972.7734375\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 172505.578125[  100/  200]\n",
      "\n",
      "running train loss =   187867.546875\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 174303.125000[  100/  200]\n",
      "\n",
      "running train loss =   186760.8515625\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 190740.156250[  100/  200]\n",
      "\n",
      "running train loss =   185676.921875\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 205780.375000[  100/  200]\n",
      "\n",
      "running train loss =   184566.5703125\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 164297.703125[  100/  200]\n",
      "\n",
      "running train loss =   183502.5546875\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 195451.156250[  100/  200]\n",
      "\n",
      "running train loss =   182469.984375\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 202243.765625[  100/  200]\n",
      "\n",
      "running train loss =   181443.1015625\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 180956.718750[  100/  200]\n",
      "\n",
      "running train loss =   180400.546875\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 196160.843750[  100/  200]\n",
      "\n",
      "running train loss =   179425.6953125\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 206626.140625[  100/  200]\n",
      "\n",
      "running train loss =   178397.6640625\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 167409.281250[  100/  200]\n",
      "\n",
      "running train loss =   177403.7421875\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 182190.859375[  100/  200]\n",
      "\n",
      "running train loss =   176410.2734375\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147476.312500[  100/  200]\n",
      "\n",
      "running train loss =   175449.4296875\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 188637.500000[  100/  200]\n",
      "\n",
      "running train loss =   174525.4609375\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 184265.765625[  100/  200]\n",
      "\n",
      "running train loss =   173582.21875\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 193067.203125[  100/  200]\n",
      "\n",
      "running train loss =   172658.9765625\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 171544.046875[  100/  200]\n",
      "\n",
      "running train loss =   171723.296875\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 173848.187500[  100/  200]\n",
      "\n",
      "running train loss =   170822.546875\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 174390.093750[  100/  200]\n",
      "\n",
      "running train loss =   169949.1796875\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 180486.781250[  100/  200]\n",
      "\n",
      "running train loss =   169086.546875\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 165812.562500[  100/  200]\n",
      "\n",
      "running train loss =   168232.875\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 174162.562500[  100/  200]\n",
      "\n",
      "running train loss =   167311.828125\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143682.437500[  100/  200]\n",
      "\n",
      "running train loss =   166484.7890625\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 188561.906250[  100/  200]\n",
      "\n",
      "running train loss =   165679.6875\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 163852.515625[  100/  200]\n",
      "\n",
      "running train loss =   164831.453125\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154714.859375[  100/  200]\n",
      "\n",
      "running train loss =   164026.21875\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 182298.218750[  100/  200]\n",
      "\n",
      "running train loss =   163261.875\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 168412.593750[  100/  200]\n",
      "\n",
      "running train loss =   162425.8359375\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 171995.296875[  100/  200]\n",
      "\n",
      "running train loss =   161656.40625\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 153143.750000[  100/  200]\n",
      "\n",
      "running train loss =   160915.3828125\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 163017.578125[  100/  200]\n",
      "\n",
      "running train loss =   160123.5078125\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154255.859375[  100/  200]\n",
      "\n",
      "running train loss =   159403.046875\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 142237.828125[  100/  200]\n",
      "\n",
      "running train loss =   158641.484375\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 170831.234375[  100/  200]\n",
      "\n",
      "running train loss =   157953.8203125\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 174986.453125[  100/  200]\n",
      "\n",
      "running train loss =   157222.1953125\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148126.593750[  100/  200]\n",
      "\n",
      "running train loss =   156482.546875\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150496.312500[  100/  200]\n",
      "\n",
      "running train loss =   155789.9140625\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 144135.984375[  100/  200]\n",
      "\n",
      "running train loss =   155084.6796875\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 162826.687500[  100/  200]\n",
      "\n",
      "running train loss =   154429.65625\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150756.031250[  100/  200]\n",
      "\n",
      "running train loss =   153788.890625\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143015.312500[  100/  200]\n",
      "\n",
      "running train loss =   153095.0234375\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150839.328125[  100/  200]\n",
      "\n",
      "running train loss =   152480.671875\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 149709.937500[  100/  200]\n",
      "\n",
      "running train loss =   151773.4921875\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 165694.734375[  100/  200]\n",
      "\n",
      "running train loss =   151187.3359375\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148706.406250[  100/  200]\n",
      "\n",
      "running train loss =   150550.0078125\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135683.906250[  100/  200]\n",
      "\n",
      "running train loss =   149928.4453125\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140379.687500[  100/  200]\n",
      "\n",
      "running train loss =   149317.9140625\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 158962.625000[  100/  200]\n",
      "\n",
      "running train loss =   148748.8984375\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150020.906250[  100/  200]\n",
      "\n",
      "running train loss =   148145.6328125\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 138288.640625[  100/  200]\n",
      "\n",
      "running train loss =   147561.375\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 136655.343750[  100/  200]\n",
      "\n",
      "running train loss =   146992.640625\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125691.609375[  100/  200]\n",
      "\n",
      "running train loss =   146407.2890625\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150455.281250[  100/  200]\n",
      "\n",
      "running train loss =   145872.1328125\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 164835.281250[  100/  200]\n",
      "\n",
      "running train loss =   145326.25\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 163501.375000[  100/  200]\n",
      "\n",
      "running train loss =   144781.9765625\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129709.890625[  100/  200]\n",
      "\n",
      "running train loss =   144234.6015625\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135070.515625[  100/  200]\n",
      "\n",
      "running train loss =   143717.21875\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145489.781250[  100/  200]\n",
      "\n",
      "running train loss =   143215.1015625\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148649.000000[  100/  200]\n",
      "\n",
      "running train loss =   142653.828125\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140562.906250[  100/  200]\n",
      "\n",
      "running train loss =   142197.6953125\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 144348.250000[  100/  200]\n",
      "\n",
      "running train loss =   141655.0078125\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154104.531250[  100/  200]\n",
      "\n",
      "running train loss =   141201.13671875\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 159265.859375[  100/  200]\n",
      "\n",
      "running train loss =   140724.05078125\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121311.101562[  100/  200]\n",
      "\n",
      "running train loss =   140268.66796875\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135769.343750[  100/  200]\n",
      "\n",
      "running train loss =   139768.1640625\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147605.984375[  100/  200]\n",
      "\n",
      "running train loss =   139343.78125\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148730.718750[  100/  200]\n",
      "\n",
      "running train loss =   138850.6484375\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 136522.343750[  100/  200]\n",
      "\n",
      "running train loss =   138412.625\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143891.625000[  100/  200]\n",
      "\n",
      "running train loss =   137973.71875\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118033.406250[  100/  200]\n",
      "\n",
      "running train loss =   137543.2265625\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137385.031250[  100/  200]\n",
      "\n",
      "running train loss =   137097.125\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119969.210938[  100/  200]\n",
      "\n",
      "running train loss =   136684.55859375\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 131981.406250[  100/  200]\n",
      "\n",
      "running train loss =   136235.6484375\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129663.359375[  100/  200]\n",
      "\n",
      "running train loss =   135859.5234375\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148126.765625[  100/  200]\n",
      "\n",
      "running train loss =   135484.46875\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 139327.296875[  100/  200]\n",
      "\n",
      "running train loss =   135053.6171875\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150183.765625[  100/  200]\n",
      "\n",
      "running train loss =   134696.31640625\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114237.601562[  100/  200]\n",
      "\n",
      "running train loss =   134296.67578125\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145254.343750[  100/  200]\n",
      "\n",
      "running train loss =   133903.4609375\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140003.125000[  100/  200]\n",
      "\n",
      "running train loss =   133549.1484375\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126274.562500[  100/  200]\n",
      "\n",
      "running train loss =   133163.0234375\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120003.937500[  100/  200]\n",
      "\n",
      "running train loss =   132788.609375\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111138.351562[  100/  200]\n",
      "\n",
      "running train loss =   132432.94921875\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137372.890625[  100/  200]\n",
      "\n",
      "running train loss =   132126.51171875\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129084.843750[  100/  200]\n",
      "\n",
      "running train loss =   131751.515625\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135218.062500[  100/  200]\n",
      "\n",
      "running train loss =   131395.8671875\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134831.671875[  100/  200]\n",
      "\n",
      "running train loss =   131082.64453125\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112425.289062[  100/  200]\n",
      "\n",
      "running train loss =   130741.08203125\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147036.031250[  100/  200]\n",
      "\n",
      "running train loss =   130416.08203125\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127128.828125[  100/  200]\n",
      "\n",
      "running train loss =   130103.9140625\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148133.937500[  100/  200]\n",
      "\n",
      "running train loss =   129769.5078125\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130348.976562[  100/  200]\n",
      "\n",
      "running train loss =   129461.19921875\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119425.007812[  100/  200]\n",
      "\n",
      "running train loss =   129162.92578125\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127823.000000[  100/  200]\n",
      "\n",
      "running train loss =   128868.01953125\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134423.734375[  100/  200]\n",
      "\n",
      "running train loss =   128543.8359375\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119545.781250[  100/  200]\n",
      "\n",
      "running train loss =   128247.5078125\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125818.179688[  100/  200]\n",
      "\n",
      "running train loss =   127962.48828125\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105451.710938[  100/  200]\n",
      "\n",
      "running train loss =   127666.71484375\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112005.578125[  100/  200]\n",
      "\n",
      "running train loss =   127413.2421875\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100863.703125[  100/  200]\n",
      "\n",
      "running train loss =   127120.84375\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 149212.312500[  100/  200]\n",
      "\n",
      "running train loss =   126875.20703125\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118691.796875[  100/  200]\n",
      "\n",
      "running train loss =   126598.109375\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115454.710938[  100/  200]\n",
      "\n",
      "running train loss =   126339.13671875\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115344.148438[  100/  200]\n",
      "\n",
      "running train loss =   126075.19140625\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145412.125000[  100/  200]\n",
      "\n",
      "running train loss =   125815.11328125\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140534.453125[  100/  200]\n",
      "\n",
      "running train loss =   125569.3203125\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 131973.968750[  100/  200]\n",
      "\n",
      "running train loss =   125295.3984375\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 149081.687500[  100/  200]\n",
      "\n",
      "running train loss =   125097.6328125\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127443.578125[  100/  200]\n",
      "\n",
      "running train loss =   124847.078125\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145941.625000[  100/  200]\n",
      "\n",
      "running train loss =   124612.109375\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137792.250000[  100/  200]\n",
      "\n",
      "running train loss =   124379.015625\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127360.367188[  100/  200]\n",
      "\n",
      "running train loss =   124124.9453125\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 136402.625000[  100/  200]\n",
      "\n",
      "running train loss =   123900.015625\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119843.156250[  100/  200]\n",
      "\n",
      "running train loss =   123661.19140625\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118589.156250[  100/  200]\n",
      "\n",
      "running train loss =   123478.08203125\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133783.015625[  100/  200]\n",
      "\n",
      "running train loss =   123252.45703125\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117991.953125[  100/  200]\n",
      "\n",
      "running train loss =   123016.34765625\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115007.343750[  100/  200]\n",
      "\n",
      "running train loss =   122848.66796875\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112234.460938[  100/  200]\n",
      "\n",
      "running train loss =   122622.75390625\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115717.656250[  100/  200]\n",
      "\n",
      "running train loss =   122412.1328125\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110642.140625[  100/  200]\n",
      "\n",
      "running train loss =   122198.0703125\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127894.906250[  100/  200]\n",
      "\n",
      "running train loss =   122007.5625\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 138963.859375[  100/  200]\n",
      "\n",
      "running train loss =   121841.03125\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133492.796875[  100/  200]\n",
      "\n",
      "running train loss =   121619.07421875\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133803.125000[  100/  200]\n",
      "\n",
      "running train loss =   121425.50390625\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106675.820312[  100/  200]\n",
      "\n",
      "running train loss =   121255.81640625\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97886.062500[  100/  200]\n",
      "\n",
      "running train loss =   121074.1484375\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133672.421875[  100/  200]\n",
      "\n",
      "running train loss =   120900.640625\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121658.203125[  100/  200]\n",
      "\n",
      "running train loss =   120737.171875\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96135.203125[  100/  200]\n",
      "\n",
      "running train loss =   120523.859375\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116222.820312[  100/  200]\n",
      "\n",
      "running train loss =   120364.69921875\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114339.203125[  100/  200]\n",
      "\n",
      "running train loss =   120179.1171875\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120360.523438[  100/  200]\n",
      "\n",
      "running train loss =   120024.19140625\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89051.890625[  100/  200]\n",
      "\n",
      "running train loss =   119850.4296875\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130649.148438[  100/  200]\n",
      "\n",
      "running train loss =   119692.78515625\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111186.250000[  100/  200]\n",
      "\n",
      "running train loss =   119535.84375\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105639.093750[  100/  200]\n",
      "\n",
      "running train loss =   119365.6328125\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114706.796875[  100/  200]\n",
      "\n",
      "running train loss =   119227.59375\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125588.343750[  100/  200]\n",
      "\n",
      "running train loss =   119072.75\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119764.640625[  100/  200]\n",
      "\n",
      "running train loss =   118932.33984375\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133900.125000[  100/  200]\n",
      "\n",
      "running train loss =   118778.7734375\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127330.867188[  100/  200]\n",
      "\n",
      "running train loss =   118638.5390625\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 132810.156250[  100/  200]\n",
      "\n",
      "running train loss =   118464.08984375\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123580.648438[  100/  200]\n",
      "\n",
      "running train loss =   118339.11328125\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116233.632812[  100/  200]\n",
      "\n",
      "running train loss =   118186.90234375\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133331.640625[  100/  200]\n",
      "\n",
      "running train loss =   118066.2109375\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119880.757812[  100/  200]\n",
      "\n",
      "running train loss =   117906.5078125\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 131486.859375[  100/  200]\n",
      "\n",
      "running train loss =   117778.41796875\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112669.609375[  100/  200]\n",
      "\n",
      "running train loss =   117647.53515625\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109588.218750[  100/  200]\n",
      "\n",
      "running train loss =   117530.078125\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110301.500000[  100/  200]\n",
      "\n",
      "running train loss =   117381.94921875\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121690.859375[  100/  200]\n",
      "\n",
      "running train loss =   117253.875\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116735.320312[  100/  200]\n",
      "\n",
      "running train loss =   117134.98828125\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94908.109375[  100/  200]\n",
      "\n",
      "running train loss =   117013.9375\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107255.578125[  100/  200]\n",
      "\n",
      "running train loss =   116874.08984375\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118772.343750[  100/  200]\n",
      "\n",
      "running train loss =   116761.80859375\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115163.078125[  100/  200]\n",
      "\n",
      "running train loss =   116640.3359375\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117811.007812[  100/  200]\n",
      "\n",
      "running train loss =   116542.37890625\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126521.148438[  100/  200]\n",
      "\n",
      "running train loss =   116410.2890625\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111373.406250[  100/  200]\n",
      "\n",
      "running train loss =   116310.43359375\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99776.156250[  100/  200]\n",
      "\n",
      "running train loss =   116187.4375\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134346.093750[  100/  200]\n",
      "\n",
      "running train loss =   116062.59765625\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 132352.578125[  100/  200]\n",
      "\n",
      "running train loss =   115948.04296875\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108167.281250[  100/  200]\n",
      "\n",
      "running train loss =   115841.65234375\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140756.750000[  100/  200]\n",
      "\n",
      "running train loss =   115751.08984375\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130970.789062[  100/  200]\n",
      "\n",
      "running train loss =   115641.1953125\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104407.328125[  100/  200]\n",
      "\n",
      "running train loss =   115527.1328125\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143505.015625[  100/  200]\n",
      "\n",
      "running train loss =   115425.37890625\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117860.882812[  100/  200]\n",
      "\n",
      "running train loss =   115318.953125\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113212.398438[  100/  200]\n",
      "\n",
      "running train loss =   115235.6875\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130823.203125[  100/  200]\n",
      "\n",
      "running train loss =   115119.90234375\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140670.781250[  100/  200]\n",
      "\n",
      "running train loss =   115021.33203125\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111973.703125[  100/  200]\n",
      "\n",
      "running train loss =   114932.23046875\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108529.062500[  100/  200]\n",
      "\n",
      "running train loss =   114841.515625\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120781.132812[  100/  200]\n",
      "\n",
      "running train loss =   114727.23828125\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105765.156250[  100/  200]\n",
      "\n",
      "running train loss =   114631.578125\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101791.781250[  100/  200]\n",
      "\n",
      "running train loss =   114537.99609375\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126020.281250[  100/  200]\n",
      "\n",
      "running train loss =   114456.88671875\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101137.968750[  100/  200]\n",
      "\n",
      "running train loss =   114366.7578125\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99426.132812[  100/  200]\n",
      "\n",
      "running train loss =   114275.2109375\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100426.328125[  100/  200]\n",
      "\n",
      "running train loss =   114181.2890625\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118638.343750[  100/  200]\n",
      "\n",
      "running train loss =   114088.56640625\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125309.523438[  100/  200]\n",
      "\n",
      "running train loss =   114004.90234375\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134011.687500[  100/  200]\n",
      "\n",
      "running train loss =   113925.953125\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114231.523438[  100/  200]\n",
      "\n",
      "running train loss =   113838.65234375\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113213.250000[  100/  200]\n",
      "\n",
      "running train loss =   113753.921875\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108032.062500[  100/  200]\n",
      "\n",
      "running train loss =   113660.70703125\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128307.148438[  100/  200]\n",
      "\n",
      "running train loss =   113600.87109375\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111102.562500[  100/  200]\n",
      "\n",
      "running train loss =   113494.765625\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100106.421875[  100/  200]\n",
      "\n",
      "running train loss =   113425.1953125\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108576.617188[  100/  200]\n",
      "\n",
      "running train loss =   113344.91796875\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115655.796875[  100/  200]\n",
      "\n",
      "running train loss =   113260.55859375\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111244.757812[  100/  200]\n",
      "\n",
      "running train loss =   113179.95703125\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105000.937500[  100/  200]\n",
      "\n",
      "running train loss =   113105.2578125\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93669.468750[  100/  200]\n",
      "\n",
      "running train loss =   113030.3046875\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107134.132812[  100/  200]\n",
      "\n",
      "running train loss =   112945.8359375\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114957.289062[  100/  200]\n",
      "\n",
      "running train loss =   112878.98046875\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94749.460938[  100/  200]\n",
      "\n",
      "running train loss =   112794.77734375\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133556.421875[  100/  200]\n",
      "\n",
      "running train loss =   112734.16015625\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115035.140625[  100/  200]\n",
      "\n",
      "running train loss =   112652.59375\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130223.523438[  100/  200]\n",
      "\n",
      "running train loss =   112578.7109375\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99728.523438[  100/  200]\n",
      "\n",
      "running train loss =   112508.3671875\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115152.179688[  100/  200]\n",
      "\n",
      "running train loss =   112426.03125\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114883.960938[  100/  200]\n",
      "\n",
      "running train loss =   112357.32421875\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94738.101562[  100/  200]\n",
      "\n",
      "running train loss =   112291.87890625\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103636.476562[  100/  200]\n",
      "\n",
      "running train loss =   112220.21484375\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122459.273438[  100/  200]\n",
      "\n",
      "running train loss =   112152.81640625\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117730.257812[  100/  200]\n",
      "\n",
      "running train loss =   112079.1171875\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107643.843750[  100/  200]\n",
      "\n",
      "running train loss =   112009.265625\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105022.273438[  100/  200]\n",
      "\n",
      "running train loss =   111950.05859375\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103327.046875[  100/  200]\n",
      "\n",
      "running train loss =   111871.40234375\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120454.000000[  100/  200]\n",
      "\n",
      "running train loss =   111807.984375\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116519.656250[  100/  200]\n",
      "\n",
      "running train loss =   111746.2890625\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125984.718750[  100/  200]\n",
      "\n",
      "running train loss =   111675.5078125\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115491.539062[  100/  200]\n",
      "\n",
      "running train loss =   111607.8984375\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103893.273438[  100/  200]\n",
      "\n",
      "running train loss =   111545.08984375\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120368.453125[  100/  200]\n",
      "\n",
      "running train loss =   111483.7109375\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108728.093750[  100/  200]\n",
      "\n",
      "running train loss =   111411.61328125\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105474.203125[  100/  200]\n",
      "\n",
      "running train loss =   111353.1171875\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117262.820312[  100/  200]\n",
      "\n",
      "running train loss =   111295.28515625\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109242.476562[  100/  200]\n",
      "\n",
      "running train loss =   111218.9765625\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119497.382812[  100/  200]\n",
      "\n",
      "running train loss =   111160.3359375\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125821.468750[  100/  200]\n",
      "\n",
      "running train loss =   111102.75390625\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93128.992188[  100/  200]\n",
      "\n",
      "running train loss =   111035.6796875\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112568.367188[  100/  200]\n",
      "\n",
      "running train loss =   110974.01171875\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125109.289062[  100/  200]\n",
      "\n",
      "running train loss =   110912.9140625\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114996.500000[  100/  200]\n",
      "\n",
      "running train loss =   110850.87109375\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120096.109375[  100/  200]\n",
      "\n",
      "running train loss =   110790.078125\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111707.210938[  100/  200]\n",
      "\n",
      "running train loss =   110733.76171875\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130704.007812[  100/  200]\n",
      "\n",
      "running train loss =   110672.28515625\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128320.937500[  100/  200]\n",
      "\n",
      "running train loss =   110609.5625\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91340.523438[  100/  200]\n",
      "\n",
      "running train loss =   110550.48828125\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128374.859375[  100/  200]\n",
      "\n",
      "running train loss =   110493.23046875\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114739.179688[  100/  200]\n",
      "\n",
      "running train loss =   110429.828125\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105585.429688[  100/  200]\n",
      "\n",
      "running train loss =   110372.51953125\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104996.312500[  100/  200]\n",
      "\n",
      "running train loss =   110312.984375\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115271.109375[  100/  200]\n",
      "\n",
      "running train loss =   110256.68359375\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114337.367188[  100/  200]\n",
      "\n",
      "running train loss =   110199.19921875\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117495.250000[  100/  200]\n",
      "\n",
      "running train loss =   110140.69921875\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118903.937500[  100/  200]\n",
      "\n",
      "running train loss =   110080.27734375\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102499.242188[  100/  200]\n",
      "\n",
      "running train loss =   110023.04296875\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110383.820312[  100/  200]\n",
      "\n",
      "running train loss =   109974.140625\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101197.578125[  100/  200]\n",
      "\n",
      "running train loss =   109908.35546875\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115603.671875[  100/  200]\n",
      "\n",
      "running train loss =   109855.4453125\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113895.773438[  100/  200]\n",
      "\n",
      "running train loss =   109798.87109375\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99665.296875[  100/  200]\n",
      "\n",
      "running train loss =   109742.609375\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99524.171875[  100/  200]\n",
      "\n",
      "running train loss =   109679.32421875\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113153.773438[  100/  200]\n",
      "\n",
      "running train loss =   109626.8125\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111337.820312[  100/  200]\n",
      "\n",
      "running train loss =   109574.859375\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110901.906250[  100/  200]\n",
      "\n",
      "running train loss =   109518.65625\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121887.343750[  100/  200]\n",
      "\n",
      "running train loss =   109460.046875\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106214.382812[  100/  200]\n",
      "\n",
      "running train loss =   109408.3515625\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93564.437500[  100/  200]\n",
      "\n",
      "running train loss =   109348.73828125\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106424.546875[  100/  200]\n",
      "\n",
      "running train loss =   109300.07421875\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128136.171875[  100/  200]\n",
      "\n",
      "running train loss =   109240.95703125\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112713.773438[  100/  200]\n",
      "\n",
      "running train loss =   109189.85546875\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111098.921875[  100/  200]\n",
      "\n",
      "running train loss =   109130.6015625\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106610.539062[  100/  200]\n",
      "\n",
      "running train loss =   109076.984375\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117549.570312[  100/  200]\n",
      "\n",
      "running train loss =   109020.82421875\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118920.781250[  100/  200]\n",
      "\n",
      "running train loss =   108972.6875\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106697.921875[  100/  200]\n",
      "\n",
      "running train loss =   108912.03125\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104970.523438[  100/  200]\n",
      "\n",
      "running train loss =   108857.80859375\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100663.718750[  100/  200]\n",
      "\n",
      "running train loss =   108805.03125\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100691.500000[  100/  200]\n",
      "\n",
      "running train loss =   108755.328125\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110051.906250[  100/  200]\n",
      "\n",
      "running train loss =   108696.52734375\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109451.031250[  100/  200]\n",
      "\n",
      "running train loss =   108646.81640625\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107900.593750[  100/  200]\n",
      "\n",
      "running train loss =   108588.5703125\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80012.171875[  100/  200]\n",
      "\n",
      "running train loss =   108536.84375\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122657.156250[  100/  200]\n",
      "\n",
      "running train loss =   108483.28125\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109339.687500[  100/  200]\n",
      "\n",
      "running train loss =   108427.8671875\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91763.539062[  100/  200]\n",
      "\n",
      "running train loss =   108376.83984375\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113833.281250[  100/  200]\n",
      "\n",
      "running train loss =   108324.125\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113492.796875[  100/  200]\n",
      "\n",
      "running train loss =   108270.3671875\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102336.320312[  100/  200]\n",
      "\n",
      "running train loss =   108214.7890625\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119403.796875[  100/  200]\n",
      "\n",
      "running train loss =   108163.6328125\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115032.460938[  100/  200]\n",
      "\n",
      "running train loss =   108110.984375\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110522.320312[  100/  200]\n",
      "\n",
      "running train loss =   108058.0703125\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114206.523438[  100/  200]\n",
      "\n",
      "running train loss =   108002.703125\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111450.726562[  100/  200]\n",
      "\n",
      "running train loss =   107950.671875\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85248.468750[  100/  200]\n",
      "\n",
      "running train loss =   107896.85546875\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105506.710938[  100/  200]\n",
      "\n",
      "running train loss =   107843.2421875\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108660.820312[  100/  200]\n",
      "\n",
      "running train loss =   107792.859375\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101921.468750[  100/  200]\n",
      "\n",
      "running train loss =   107736.99609375\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98282.703125[  100/  200]\n",
      "\n",
      "running train loss =   107686.06640625\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107571.148438[  100/  200]\n",
      "\n",
      "running train loss =   107631.9140625\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101290.867188[  100/  200]\n",
      "\n",
      "running train loss =   107581.078125\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105318.289062[  100/  200]\n",
      "\n",
      "running train loss =   107527.25390625\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120685.148438[  100/  200]\n",
      "\n",
      "running train loss =   107478.05078125\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92408.000000[  100/  200]\n",
      "\n",
      "running train loss =   107422.80859375\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109669.937500[  100/  200]\n",
      "\n",
      "running train loss =   107370.9296875\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107610.492188[  100/  200]\n",
      "\n",
      "running train loss =   107321.49609375\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102428.101562[  100/  200]\n",
      "\n",
      "running train loss =   107261.9609375\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103394.148438[  100/  200]\n",
      "\n",
      "running train loss =   107210.078125\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109115.523438[  100/  200]\n",
      "\n",
      "running train loss =   107161.41015625\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109913.187500[  100/  200]\n",
      "\n",
      "running train loss =   107107.76953125\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96521.007812[  100/  200]\n",
      "\n",
      "running train loss =   107053.85546875\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124965.976562[  100/  200]\n",
      "\n",
      "running train loss =   107002.23828125\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110304.593750[  100/  200]\n",
      "\n",
      "running train loss =   106947.7734375\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109021.937500[  100/  200]\n",
      "\n",
      "running train loss =   106894.703125\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101569.820312[  100/  200]\n",
      "\n",
      "running train loss =   106840.671875\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94008.359375[  100/  200]\n",
      "\n",
      "running train loss =   106787.40625\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89224.367188[  100/  200]\n",
      "\n",
      "running train loss =   106736.421875\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106017.742188[  100/  200]\n",
      "\n",
      "running train loss =   106685.2109375\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106561.117188[  100/  200]\n",
      "\n",
      "running train loss =   106631.86328125\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116586.031250[  100/  200]\n",
      "\n",
      "running train loss =   106577.98046875\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97791.773438[  100/  200]\n",
      "\n",
      "running train loss =   106523.8515625\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102877.398438[  100/  200]\n",
      "\n",
      "running train loss =   106472.1640625\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86581.476562[  100/  200]\n",
      "\n",
      "running train loss =   106416.5546875\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111168.960938[  100/  200]\n",
      "\n",
      "running train loss =   106366.26171875\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124662.867188[  100/  200]\n",
      "\n",
      "running train loss =   106316.47265625\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113732.773438[  100/  200]\n",
      "\n",
      "running train loss =   106261.6171875\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87203.656250[  100/  200]\n",
      "\n",
      "running train loss =   106206.62890625\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116745.148438[  100/  200]\n",
      "\n",
      "running train loss =   106154.484375\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107403.921875[  100/  200]\n",
      "\n",
      "running train loss =   106101.4296875\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104865.757812[  100/  200]\n",
      "\n",
      "running train loss =   106048.0\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87450.609375[  100/  200]\n",
      "\n",
      "running train loss =   105992.515625\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104691.148438[  100/  200]\n",
      "\n",
      "running train loss =   105944.25390625\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105196.750000[  100/  200]\n",
      "\n",
      "running train loss =   105889.796875\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108388.890625[  100/  200]\n",
      "\n",
      "running train loss =   105835.296875\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114181.031250[  100/  200]\n",
      "\n",
      "running train loss =   105782.96875\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114477.000000[  100/  200]\n",
      "\n",
      "running train loss =   105729.14453125\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100247.398438[  100/  200]\n",
      "\n",
      "running train loss =   105674.62890625\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102066.367188[  100/  200]\n",
      "\n",
      "running train loss =   105622.2890625\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116061.953125[  100/  200]\n",
      "\n",
      "running train loss =   105569.1953125\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99084.250000[  100/  200]\n",
      "\n",
      "running train loss =   105513.85546875\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97714.898438[  100/  200]\n",
      "\n",
      "running train loss =   105460.49609375\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89578.976562[  100/  200]\n",
      "\n",
      "running train loss =   105406.72265625\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108326.578125[  100/  200]\n",
      "\n",
      "running train loss =   105354.08984375\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104261.929688[  100/  200]\n",
      "\n",
      "running train loss =   105300.4453125\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103713.500000[  100/  200]\n",
      "\n",
      "running train loss =   105247.58984375\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104037.609375[  100/  200]\n",
      "\n",
      "running train loss =   105192.05078125\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96119.789062[  100/  200]\n",
      "\n",
      "running train loss =   105137.6640625\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103946.093750[  100/  200]\n",
      "\n",
      "running train loss =   105085.2734375\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119919.179688[  100/  200]\n",
      "\n",
      "running train loss =   105031.66015625\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103398.718750[  100/  200]\n",
      "\n",
      "running train loss =   104977.453125\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104357.851562[  100/  200]\n",
      "\n",
      "running train loss =   104922.01171875\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101459.328125[  100/  200]\n",
      "\n",
      "running train loss =   104868.4140625\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99159.546875[  100/  200]\n",
      "\n",
      "running train loss =   104813.7578125\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104202.406250[  100/  200]\n",
      "\n",
      "running train loss =   104761.16796875\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97759.867188[  100/  200]\n",
      "\n",
      "running train loss =   104704.46484375\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100747.703125[  100/  200]\n",
      "\n",
      "running train loss =   104651.921875\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101528.351562[  100/  200]\n",
      "\n",
      "running train loss =   104596.265625\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122655.289062[  100/  200]\n",
      "\n",
      "running train loss =   104543.24609375\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106154.476562[  100/  200]\n",
      "\n",
      "running train loss =   104487.48828125\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86945.101562[  100/  200]\n",
      "\n",
      "running train loss =   104430.3515625\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95575.781250[  100/  200]\n",
      "\n",
      "running train loss =   104377.1640625\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102406.656250[  100/  200]\n",
      "\n",
      "running train loss =   104326.43359375\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90213.210938[  100/  200]\n",
      "\n",
      "running train loss =   104265.32421875\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102352.820312[  100/  200]\n",
      "\n",
      "running train loss =   104213.28515625\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111813.257812[  100/  200]\n",
      "\n",
      "running train loss =   104159.0234375\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92458.343750[  100/  200]\n",
      "\n",
      "running train loss =   104101.37109375\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97556.476562[  100/  200]\n",
      "\n",
      "running train loss =   104047.51953125\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105926.398438[  100/  200]\n",
      "\n",
      "running train loss =   103994.07421875\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112860.242188[  100/  200]\n",
      "\n",
      "running train loss =   103939.8125\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101588.562500[  100/  200]\n",
      "\n",
      "running train loss =   103881.92578125\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94815.390625[  100/  200]\n",
      "\n",
      "running train loss =   103824.875\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88541.398438[  100/  200]\n",
      "\n",
      "running train loss =   103768.7578125\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121570.859375[  100/  200]\n",
      "\n",
      "running train loss =   103716.89453125\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112308.828125[  100/  200]\n",
      "\n",
      "running train loss =   103659.62890625\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92688.289062[  100/  200]\n",
      "\n",
      "running train loss =   103603.36328125\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92758.117188[  100/  200]\n",
      "\n",
      "running train loss =   103545.73828125\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99731.421875[  100/  200]\n",
      "\n",
      "running train loss =   103491.984375\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108368.960938[  100/  200]\n",
      "\n",
      "running train loss =   103436.93359375\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98925.703125[  100/  200]\n",
      "\n",
      "running train loss =   103378.97265625\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96657.289062[  100/  200]\n",
      "\n",
      "running train loss =   103324.875\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113750.023438[  100/  200]\n",
      "\n",
      "running train loss =   103267.6953125\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110081.796875[  100/  200]\n",
      "\n",
      "running train loss =   103210.34765625\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108069.710938[  100/  200]\n",
      "\n",
      "running train loss =   103155.9921875\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98208.109375[  100/  200]\n",
      "\n",
      "running train loss =   103099.03515625\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117837.507812[  100/  200]\n",
      "\n",
      "running train loss =   103044.86328125\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103838.226562[  100/  200]\n",
      "\n",
      "running train loss =   102985.72265625\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125069.656250[  100/  200]\n",
      "\n",
      "running train loss =   102931.86328125\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102482.703125[  100/  200]\n",
      "\n",
      "running train loss =   102871.1015625\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107127.851562[  100/  200]\n",
      "\n",
      "running train loss =   102815.640625\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80548.804688[  100/  200]\n",
      "\n",
      "running train loss =   102755.55078125\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87050.820312[  100/  200]\n",
      "\n",
      "running train loss =   102698.359375\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96126.312500[  100/  200]\n",
      "\n",
      "running train loss =   102643.265625\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102522.882812[  100/  200]\n",
      "\n",
      "running train loss =   102590.28125\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96327.968750[  100/  200]\n",
      "\n",
      "running train loss =   102529.57421875\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122092.820312[  100/  200]\n",
      "\n",
      "running train loss =   102477.82421875\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121306.632812[  100/  200]\n",
      "\n",
      "running train loss =   102420.15234375\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76303.742188[  100/  200]\n",
      "\n",
      "running train loss =   102354.81640625\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85992.156250[  100/  200]\n",
      "\n",
      "running train loss =   102299.0\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98631.742188[  100/  200]\n",
      "\n",
      "running train loss =   102244.77734375\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110547.343750[  100/  200]\n",
      "\n",
      "running train loss =   102187.4765625\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95893.351562[  100/  200]\n",
      "\n",
      "running train loss =   102128.76171875\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94931.343750[  100/  200]\n",
      "\n",
      "running train loss =   102071.2421875\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106552.812500[  100/  200]\n",
      "\n",
      "running train loss =   102016.55078125\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100444.093750[  100/  200]\n",
      "\n",
      "running train loss =   101954.65625\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94935.078125[  100/  200]\n",
      "\n",
      "running train loss =   101896.8046875\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97023.656250[  100/  200]\n",
      "\n",
      "running train loss =   101837.65625\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113735.617188[  100/  200]\n",
      "\n",
      "running train loss =   101784.7578125\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106459.757812[  100/  200]\n",
      "\n",
      "running train loss =   101725.68359375\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95789.507812[  100/  200]\n",
      "\n",
      "running train loss =   101664.67578125\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92035.500000[  100/  200]\n",
      "\n",
      "running train loss =   101605.75390625\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100728.210938[  100/  200]\n",
      "\n",
      "running train loss =   101548.421875\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94337.007812[  100/  200]\n",
      "\n",
      "running train loss =   101489.86328125\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88421.109375[  100/  200]\n",
      "\n",
      "running train loss =   101431.0390625\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104272.773438[  100/  200]\n",
      "\n",
      "running train loss =   101373.24609375\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98039.656250[  100/  200]\n",
      "\n",
      "running train loss =   101314.65625\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95819.632812[  100/  200]\n",
      "\n",
      "running train loss =   101256.08984375\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111194.796875[  100/  200]\n",
      "\n",
      "running train loss =   101198.4453125\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92553.492188[  100/  200]\n",
      "\n",
      "running train loss =   101137.47265625\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105269.890625[  100/  200]\n",
      "\n",
      "running train loss =   101085.50390625\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102864.601562[  100/  200]\n",
      "\n",
      "running train loss =   101020.8046875\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96842.101562[  100/  200]\n",
      "\n",
      "running train loss =   100960.7890625\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100467.742188[  100/  200]\n",
      "\n",
      "running train loss =   100901.58203125\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108320.921875[  100/  200]\n",
      "\n",
      "running train loss =   100845.0078125\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102975.250000[  100/  200]\n",
      "\n",
      "running train loss =   100784.2734375\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94866.156250[  100/  200]\n",
      "\n",
      "running train loss =   100723.44921875\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105516.046875[  100/  200]\n",
      "\n",
      "running train loss =   100672.02734375\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117984.406250[  100/  200]\n",
      "\n",
      "running train loss =   100610.03125\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110014.390625[  100/  200]\n",
      "\n",
      "running train loss =   100547.7265625\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92529.453125[  100/  200]\n",
      "\n",
      "running train loss =   100484.33984375\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114342.757812[  100/  200]\n",
      "\n",
      "running train loss =   100428.48046875\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93019.179688[  100/  200]\n",
      "\n",
      "running train loss =   100366.859375\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99419.867188[  100/  200]\n",
      "\n",
      "running train loss =   100308.30859375\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113157.968750[  100/  200]\n",
      "\n",
      "running train loss =   100250.54296875\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108846.687500[  100/  200]\n",
      "\n",
      "running train loss =   100187.1640625\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81039.148438[  100/  200]\n",
      "\n",
      "running train loss =   100126.99609375\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101619.359375[  100/  200]\n",
      "\n",
      "running train loss =   100072.453125\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110409.703125[  100/  200]\n",
      "\n",
      "running train loss =   100009.125\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86227.203125[  100/  200]\n",
      "\n",
      "running train loss =   99944.1796875\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95166.773438[  100/  200]\n",
      "\n",
      "running train loss =   99886.4609375\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95406.468750[  100/  200]\n",
      "\n",
      "running train loss =   99827.64453125\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110926.687500[  100/  200]\n",
      "\n",
      "running train loss =   99769.10546875\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89435.796875[  100/  200]\n",
      "\n",
      "running train loss =   99703.7890625\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103616.250000[  100/  200]\n",
      "\n",
      "running train loss =   99647.69921875\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102965.546875[  100/  200]\n",
      "\n",
      "running train loss =   99586.0546875\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90534.796875[  100/  200]\n",
      "\n",
      "running train loss =   99525.3046875\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98661.359375[  100/  200]\n",
      "\n",
      "running train loss =   99462.76953125\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104853.273438[  100/  200]\n",
      "\n",
      "running train loss =   99405.453125\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104837.617188[  100/  200]\n",
      "\n",
      "running train loss =   99344.3203125\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90527.671875[  100/  200]\n",
      "\n",
      "running train loss =   99281.265625\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93075.437500[  100/  200]\n",
      "\n",
      "running train loss =   99218.73046875\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111667.523438[  100/  200]\n",
      "\n",
      "running train loss =   99160.35546875\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105016.296875[  100/  200]\n",
      "\n",
      "running train loss =   99101.8046875\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115001.453125[  100/  200]\n",
      "\n",
      "running train loss =   99040.59765625\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100365.976562[  100/  200]\n",
      "\n",
      "running train loss =   98975.82421875\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98765.601562[  100/  200]\n",
      "\n",
      "running train loss =   98913.0703125\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89252.156250[  100/  200]\n",
      "\n",
      "running train loss =   98852.6484375\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86402.109375[  100/  200]\n",
      "\n",
      "running train loss =   98788.640625\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102974.320312[  100/  200]\n",
      "\n",
      "running train loss =   98731.56640625\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102773.789062[  100/  200]\n",
      "\n",
      "running train loss =   98670.79296875\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76716.210938[  100/  200]\n",
      "\n",
      "running train loss =   98602.68359375\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107590.078125[  100/  200]\n",
      "\n",
      "running train loss =   98545.9453125\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97656.296875[  100/  200]\n",
      "\n",
      "running train loss =   98481.5546875\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102499.593750[  100/  200]\n",
      "\n",
      "running train loss =   98423.015625\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113037.898438[  100/  200]\n",
      "\n",
      "running train loss =   98362.96484375\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110451.179688[  100/  200]\n",
      "\n",
      "running train loss =   98299.68359375\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107771.046875[  100/  200]\n",
      "\n",
      "running train loss =   98236.70703125\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92520.203125[  100/  200]\n",
      "\n",
      "running train loss =   98173.01171875\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2528201.000000[  100/  500]\n",
      "\n",
      "running train loss =   2435955.7\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2506845.000000[  100/  500]\n",
      "\n",
      "running train loss =   2434212.0\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2370177.250000[  100/  500]\n",
      "\n",
      "running train loss =   2432442.8\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2556431.750000[  100/  500]\n",
      "\n",
      "running train loss =   2430592.55\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2452201.750000[  100/  500]\n",
      "\n",
      "running train loss =   2428638.15\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2430681.750000[  100/  500]\n",
      "\n",
      "running train loss =   2426546.4\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2464728.000000[  100/  500]\n",
      "\n",
      "running train loss =   2424302.85\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2351145.500000[  100/  500]\n",
      "\n",
      "running train loss =   2421893.5\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2546006.750000[  100/  500]\n",
      "\n",
      "running train loss =   2419300.15\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2379827.250000[  100/  500]\n",
      "\n",
      "running train loss =   2416520.05\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2384322.500000[  100/  500]\n",
      "\n",
      "running train loss =   2413552.05\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2431325.000000[  100/  500]\n",
      "\n",
      "running train loss =   2410420.5\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2424174.000000[  100/  500]\n",
      "\n",
      "running train loss =   2407096.6\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2417120.750000[  100/  500]\n",
      "\n",
      "running train loss =   2403591.15\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2451814.500000[  100/  500]\n",
      "\n",
      "running train loss =   2399890.75\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2377015.500000[  100/  500]\n",
      "\n",
      "running train loss =   2396023.3\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2353679.750000[  100/  500]\n",
      "\n",
      "running train loss =   2391961.5\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2401236.750000[  100/  500]\n",
      "\n",
      "running train loss =   2387691.1\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2353622.250000[  100/  500]\n",
      "\n",
      "running train loss =   2383200.4\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2440775.250000[  100/  500]\n",
      "\n",
      "running train loss =   2378514.35\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2434665.500000[  100/  500]\n",
      "\n",
      "running train loss =   2373631.25\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2424344.000000[  100/  500]\n",
      "\n",
      "running train loss =   2368518.6\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2449157.500000[  100/  500]\n",
      "\n",
      "running train loss =   2363194.65\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2446265.500000[  100/  500]\n",
      "\n",
      "running train loss =   2357663.95\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2344551.000000[  100/  500]\n",
      "\n",
      "running train loss =   2351947.1\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2391112.000000[  100/  500]\n",
      "\n",
      "running train loss =   2345989.6\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2275222.500000[  100/  500]\n",
      "\n",
      "running train loss =   2339819.85\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2310556.750000[  100/  500]\n",
      "\n",
      "running train loss =   2333461.45\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2425805.500000[  100/  500]\n",
      "\n",
      "running train loss =   2326868.4\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2320984.250000[  100/  500]\n",
      "\n",
      "running train loss =   2320055.85\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2378014.750000[  100/  500]\n",
      "\n",
      "running train loss =   2313034.9\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2364837.000000[  100/  500]\n",
      "\n",
      "running train loss =   2305776.15\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2239859.750000[  100/  500]\n",
      "\n",
      "running train loss =   2298314.05\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2325567.500000[  100/  500]\n",
      "\n",
      "running train loss =   2290648.8\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2241492.250000[  100/  500]\n",
      "\n",
      "running train loss =   2282753.95\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2284301.000000[  100/  500]\n",
      "\n",
      "running train loss =   2274626.25\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2199464.250000[  100/  500]\n",
      "\n",
      "running train loss =   2266325.4\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2321995.000000[  100/  500]\n",
      "\n",
      "running train loss =   2257817.1\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2266724.500000[  100/  500]\n",
      "\n",
      "running train loss =   2249063.05\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2234696.750000[  100/  500]\n",
      "\n",
      "running train loss =   2240140.65\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2122410.000000[  100/  500]\n",
      "\n",
      "running train loss =   2230984.35\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2226772.000000[  100/  500]\n",
      "\n",
      "running train loss =   2221656.7\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2150696.250000[  100/  500]\n",
      "\n",
      "running train loss =   2212073.125\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2267270.500000[  100/  500]\n",
      "\n",
      "running train loss =   2202359.7\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2310185.250000[  100/  500]\n",
      "\n",
      "running train loss =   2192408.25\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2168344.250000[  100/  500]\n",
      "\n",
      "running train loss =   2182266.85\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2094984.125000[  100/  500]\n",
      "\n",
      "running train loss =   2171897.375\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2100718.750000[  100/  500]\n",
      "\n",
      "running train loss =   2161422.65\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2062363.250000[  100/  500]\n",
      "\n",
      "running train loss =   2150720.75\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2159411.000000[  100/  500]\n",
      "\n",
      "running train loss =   2139812.3\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2061284.750000[  100/  500]\n",
      "\n",
      "running train loss =   2128742.5\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2124739.250000[  100/  500]\n",
      "\n",
      "running train loss =   2117547.525\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2144967.000000[  100/  500]\n",
      "\n",
      "running train loss =   2106128.525\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2139764.750000[  100/  500]\n",
      "\n",
      "running train loss =   2094542.825\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2041373.750000[  100/  500]\n",
      "\n",
      "running train loss =   2082766.9\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2005682.750000[  100/  500]\n",
      "\n",
      "running train loss =   2070850.525\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1966872.625000[  100/  500]\n",
      "\n",
      "running train loss =   2058764.075\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2107815.250000[  100/  500]\n",
      "\n",
      "running train loss =   2046540.775\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2114885.500000[  100/  500]\n",
      "\n",
      "running train loss =   2034126.825\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1966675.875000[  100/  500]\n",
      "\n",
      "running train loss =   2021592.275\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2038426.875000[  100/  500]\n",
      "\n",
      "running train loss =   2008912.65\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2124697.750000[  100/  500]\n",
      "\n",
      "running train loss =   1996068.25\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1906230.375000[  100/  500]\n",
      "\n",
      "running train loss =   1983034.175\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1945961.125000[  100/  500]\n",
      "\n",
      "running train loss =   1969987.3\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1938644.750000[  100/  500]\n",
      "\n",
      "running train loss =   1956709.15\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1927815.625000[  100/  500]\n",
      "\n",
      "running train loss =   1943333.95\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1934716.375000[  100/  500]\n",
      "\n",
      "running train loss =   1929804.95\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1865519.500000[  100/  500]\n",
      "\n",
      "running train loss =   1916201.9\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1879780.750000[  100/  500]\n",
      "\n",
      "running train loss =   1902450.4\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1929235.000000[  100/  500]\n",
      "\n",
      "running train loss =   1888568.75\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1878255.875000[  100/  500]\n",
      "\n",
      "running train loss =   1874564.525\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1826408.375000[  100/  500]\n",
      "\n",
      "running train loss =   1860424.75\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1860390.875000[  100/  500]\n",
      "\n",
      "running train loss =   1846284.25\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1781206.250000[  100/  500]\n",
      "\n",
      "running train loss =   1831925.85\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1941498.500000[  100/  500]\n",
      "\n",
      "running train loss =   1817505.725\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1750717.250000[  100/  500]\n",
      "\n",
      "running train loss =   1802946.1\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1879349.000000[  100/  500]\n",
      "\n",
      "running train loss =   1788358.225\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1864157.625000[  100/  500]\n",
      "\n",
      "running train loss =   1773555.7\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1672742.875000[  100/  500]\n",
      "\n",
      "running train loss =   1758710.775\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1741677.875000[  100/  500]\n",
      "\n",
      "running train loss =   1743822.425\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1844248.000000[  100/  500]\n",
      "\n",
      "running train loss =   1728849.675\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1737291.375000[  100/  500]\n",
      "\n",
      "running train loss =   1713778.7\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1699810.250000[  100/  500]\n",
      "\n",
      "running train loss =   1698635.075\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1646716.000000[  100/  500]\n",
      "\n",
      "running train loss =   1683402.95\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1664951.625000[  100/  500]\n",
      "\n",
      "running train loss =   1668128.925\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1686883.500000[  100/  500]\n",
      "\n",
      "running train loss =   1652758.75\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1648377.625000[  100/  500]\n",
      "\n",
      "running train loss =   1637420.975\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1531862.875000[  100/  500]\n",
      "\n",
      "running train loss =   1621954.275\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1668906.375000[  100/  500]\n",
      "\n",
      "running train loss =   1606533.7\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1533243.000000[  100/  500]\n",
      "\n",
      "running train loss =   1590978.95\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1516565.500000[  100/  500]\n",
      "\n",
      "running train loss =   1575367.475\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1677855.000000[  100/  500]\n",
      "\n",
      "running train loss =   1559836.6\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1570095.625000[  100/  500]\n",
      "\n",
      "running train loss =   1544152.35\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1648409.625000[  100/  500]\n",
      "\n",
      "running train loss =   1528540.725\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1582852.375000[  100/  500]\n",
      "\n",
      "running train loss =   1512817.475\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1529220.125000[  100/  500]\n",
      "\n",
      "running train loss =   1497098.6\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1563800.000000[  100/  500]\n",
      "\n",
      "running train loss =   1481400.35\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1377666.875000[  100/  500]\n",
      "\n",
      "running train loss =   1465565.725\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1464543.250000[  100/  500]\n",
      "\n",
      "running train loss =   1449893.475\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1471669.250000[  100/  500]\n",
      "\n",
      "running train loss =   1434137.225\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1463408.500000[  100/  500]\n",
      "\n",
      "running train loss =   1418338.775\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1431181.500000[  100/  500]\n",
      "\n",
      "running train loss =   1402572.1\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1484115.625000[  100/  500]\n",
      "\n",
      "running train loss =   1386818.975\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1411497.250000[  100/  500]\n",
      "\n",
      "running train loss =   1371031.075\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1255828.375000[  100/  500]\n",
      "\n",
      "running train loss =   1355197.625\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1242301.500000[  100/  500]\n",
      "\n",
      "running train loss =   1339509.65\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1272172.125000[  100/  500]\n",
      "\n",
      "running train loss =   1323760.15\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1379510.875000[  100/  500]\n",
      "\n",
      "running train loss =   1308151.55\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1293502.375000[  100/  500]\n",
      "\n",
      "running train loss =   1292465.025\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1283289.875000[  100/  500]\n",
      "\n",
      "running train loss =   1276770.4\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1362018.750000[  100/  500]\n",
      "\n",
      "running train loss =   1261238.9\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1305655.500000[  100/  500]\n",
      "\n",
      "running train loss =   1245635.525\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1261612.375000[  100/  500]\n",
      "\n",
      "running train loss =   1230149.025\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1187766.500000[  100/  500]\n",
      "\n",
      "running train loss =   1214568.825\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1220256.250000[  100/  500]\n",
      "\n",
      "running train loss =   1199186.675\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1167549.125000[  100/  500]\n",
      "\n",
      "running train loss =   1183808.025\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1158293.875000[  100/  500]\n",
      "\n",
      "running train loss =   1168448.95\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1141493.250000[  100/  500]\n",
      "\n",
      "running train loss =   1153156.275\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1168711.875000[  100/  500]\n",
      "\n",
      "running train loss =   1137966.6\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1113736.500000[  100/  500]\n",
      "\n",
      "running train loss =   1122778.125\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1167994.875000[  100/  500]\n",
      "\n",
      "running train loss =   1107709.05\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1027076.937500[  100/  500]\n",
      "\n",
      "running train loss =   1092619.5875\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1133479.875000[  100/  500]\n",
      "\n",
      "running train loss =   1077738.6875\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1095651.125000[  100/  500]\n",
      "\n",
      "running train loss =   1062852.4375\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1017830.937500[  100/  500]\n",
      "\n",
      "running train loss =   1048021.775\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1084029.875000[  100/  500]\n",
      "\n",
      "running train loss =   1033354.225\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1000576.312500[  100/  500]\n",
      "\n",
      "running train loss =   1018627.75\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1022976.562500[  100/  500]\n",
      "\n",
      "running train loss =   1004101.675\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1061252.625000[  100/  500]\n",
      "\n",
      "running train loss =   989674.95\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 977803.125000[  100/  500]\n",
      "\n",
      "running train loss =   975186.6125\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1065106.250000[  100/  500]\n",
      "\n",
      "running train loss =   961016.3125\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 945922.187500[  100/  500]\n",
      "\n",
      "running train loss =   946727.2875\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1013888.625000[  100/  500]\n",
      "\n",
      "running train loss =   932644.8625\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 955670.375000[  100/  500]\n",
      "\n",
      "running train loss =   918684.1625\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 903179.062500[  100/  500]\n",
      "\n",
      "running train loss =   904736.6875\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 860529.812500[  100/  500]\n",
      "\n",
      "running train loss =   890911.475\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 845938.562500[  100/  500]\n",
      "\n",
      "running train loss =   877163.95\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 864641.625000[  100/  500]\n",
      "\n",
      "running train loss =   863724.4125\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 835857.812500[  100/  500]\n",
      "\n",
      "running train loss =   850206.7375\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 826752.312500[  100/  500]\n",
      "\n",
      "running train loss =   836912.825\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 778133.687500[  100/  500]\n",
      "\n",
      "running train loss =   823638.3625\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 879160.812500[  100/  500]\n",
      "\n",
      "running train loss =   810649.7\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 764062.875000[  100/  500]\n",
      "\n",
      "running train loss =   797525.3125\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 821311.375000[  100/  500]\n",
      "\n",
      "running train loss =   784707.225\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 806184.000000[  100/  500]\n",
      "\n",
      "running train loss =   771988.9875\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 824819.500000[  100/  500]\n",
      "\n",
      "running train loss =   759340.725\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 788547.062500[  100/  500]\n",
      "\n",
      "running train loss =   746857.525\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 717164.937500[  100/  500]\n",
      "\n",
      "running train loss =   734471.75\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 802521.750000[  100/  500]\n",
      "\n",
      "running train loss =   722300.55\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 662690.375000[  100/  500]\n",
      "\n",
      "running train loss =   710143.85\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 690105.250000[  100/  500]\n",
      "\n",
      "running train loss =   698191.2\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 702632.187500[  100/  500]\n",
      "\n",
      "running train loss =   686363.9625\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 669581.375000[  100/  500]\n",
      "\n",
      "running train loss =   674711.6875\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 656056.312500[  100/  500]\n",
      "\n",
      "running train loss =   663159.0625\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 594110.500000[  100/  500]\n",
      "\n",
      "running train loss =   651684.9625\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 640599.125000[  100/  500]\n",
      "\n",
      "running train loss =   640496.85\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 647683.125000[  100/  500]\n",
      "\n",
      "running train loss =   629305.7\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 617037.250000[  100/  500]\n",
      "\n",
      "running train loss =   618337.4625\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 625628.312500[  100/  500]\n",
      "\n",
      "running train loss =   607493.275\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 674682.000000[  100/  500]\n",
      "\n",
      "running train loss =   596798.075\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 582821.562500[  100/  500]\n",
      "\n",
      "running train loss =   586167.51875\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 551404.437500[  100/  500]\n",
      "\n",
      "running train loss =   575691.2875\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 598312.187500[  100/  500]\n",
      "\n",
      "running train loss =   565536.46875\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 573981.687500[  100/  500]\n",
      "\n",
      "running train loss =   555300.4125\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 500977.468750[  100/  500]\n",
      "\n",
      "running train loss =   545296.45625\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 493781.125000[  100/  500]\n",
      "\n",
      "running train loss =   535361.0\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 507507.968750[  100/  500]\n",
      "\n",
      "running train loss =   525720.4375\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 539548.937500[  100/  500]\n",
      "\n",
      "running train loss =   516216.65\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 459788.687500[  100/  500]\n",
      "\n",
      "running train loss =   506728.45\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 510441.281250[  100/  500]\n",
      "\n",
      "running train loss =   497471.15625\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 493947.187500[  100/  500]\n",
      "\n",
      "running train loss =   488241.925\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 475578.468750[  100/  500]\n",
      "\n",
      "running train loss =   479289.2875\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 490942.406250[  100/  500]\n",
      "\n",
      "running train loss =   470453.5\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 379775.437500[  100/  500]\n",
      "\n",
      "running train loss =   461638.7375\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 450911.687500[  100/  500]\n",
      "\n",
      "running train loss =   453168.53125\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 465687.281250[  100/  500]\n",
      "\n",
      "running train loss =   444706.0875\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 516415.437500[  100/  500]\n",
      "\n",
      "running train loss =   436423.34375\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 410688.468750[  100/  500]\n",
      "\n",
      "running train loss =   428224.50625\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 421627.718750[  100/  500]\n",
      "\n",
      "running train loss =   420344.38125\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 427564.843750[  100/  500]\n",
      "\n",
      "running train loss =   412426.76875\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 420279.468750[  100/  500]\n",
      "\n",
      "running train loss =   404769.65625\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 402153.718750[  100/  500]\n",
      "\n",
      "running train loss =   397134.25625\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 351315.718750[  100/  500]\n",
      "\n",
      "running train loss =   389725.55625\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 385090.687500[  100/  500]\n",
      "\n",
      "running train loss =   382500.4625\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 328239.125000[  100/  500]\n",
      "\n",
      "running train loss =   375223.175\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 404661.437500[  100/  500]\n",
      "\n",
      "running train loss =   368361.11875\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 411753.375000[  100/  500]\n",
      "\n",
      "running train loss =   361423.575\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 370056.156250[  100/  500]\n",
      "\n",
      "running train loss =   354667.09375\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 312021.312500[  100/  500]\n",
      "\n",
      "running train loss =   347978.01875\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 398579.875000[  100/  500]\n",
      "\n",
      "running train loss =   341603.46875\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 352556.281250[  100/  500]\n",
      "\n",
      "running train loss =   335259.45625\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 346354.562500[  100/  500]\n",
      "\n",
      "running train loss =   329037.48125\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 315482.031250[  100/  500]\n",
      "\n",
      "running train loss =   322976.2875\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 359066.437500[  100/  500]\n",
      "\n",
      "running train loss =   317008.35\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 289913.437500[  100/  500]\n",
      "\n",
      "running train loss =   311222.7875\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 307831.656250[  100/  500]\n",
      "\n",
      "running train loss =   305622.53125\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 320174.531250[  100/  500]\n",
      "\n",
      "running train loss =   299954.625\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 292211.718750[  100/  500]\n",
      "\n",
      "running train loss =   294561.29375\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 263327.968750[  100/  500]\n",
      "\n",
      "running train loss =   289284.6375\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 262069.843750[  100/  500]\n",
      "\n",
      "running train loss =   284076.728125\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 219133.015625[  100/  500]\n",
      "\n",
      "running train loss =   278935.778125\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 278492.812500[  100/  500]\n",
      "\n",
      "running train loss =   274090.58125\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 272158.062500[  100/  500]\n",
      "\n",
      "running train loss =   269270.5625\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 275109.312500[  100/  500]\n",
      "\n",
      "running train loss =   264527.68125\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 240095.437500[  100/  500]\n",
      "\n",
      "running train loss =   259861.753125\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 252308.453125[  100/  500]\n",
      "\n",
      "running train loss =   255449.25625\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 230259.359375[  100/  500]\n",
      "\n",
      "running train loss =   251017.06875\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 263358.093750[  100/  500]\n",
      "\n",
      "running train loss =   246782.275\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 236297.015625[  100/  500]\n",
      "\n",
      "running train loss =   242631.0375\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 234297.000000[  100/  500]\n",
      "\n",
      "running train loss =   238552.734375\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 242149.421875[  100/  500]\n",
      "\n",
      "running train loss =   234631.690625\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 259509.265625[  100/  500]\n",
      "\n",
      "running train loss =   230873.165625\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 209992.765625[  100/  500]\n",
      "\n",
      "running train loss =   227118.309375\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 233448.796875[  100/  500]\n",
      "\n",
      "running train loss =   223521.3125\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 234321.718750[  100/  500]\n",
      "\n",
      "running train loss =   219911.89375\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 266727.968750[  100/  500]\n",
      "\n",
      "running train loss =   216449.378125\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 198539.765625[  100/  500]\n",
      "\n",
      "running train loss =   213106.55\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 246165.515625[  100/  500]\n",
      "\n",
      "running train loss =   209952.028125\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 228236.578125[  100/  500]\n",
      "\n",
      "running train loss =   206698.025\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 204412.781250[  100/  500]\n",
      "\n",
      "running train loss =   203619.0125\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 183991.125000[  100/  500]\n",
      "\n",
      "running train loss =   200644.228125\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 190321.000000[  100/  500]\n",
      "\n",
      "running train loss =   197776.878125\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 177515.625000[  100/  500]\n",
      "\n",
      "running train loss =   194978.5375\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 215838.734375[  100/  500]\n",
      "\n",
      "running train loss =   192331.375\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 219973.921875[  100/  500]\n",
      "\n",
      "running train loss =   189628.103125\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 213223.500000[  100/  500]\n",
      "\n",
      "running train loss =   187037.28125\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 196615.546875[  100/  500]\n",
      "\n",
      "running train loss =   184577.665625\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 196528.875000[  100/  500]\n",
      "\n",
      "running train loss =   182198.36875\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 167575.484375[  100/  500]\n",
      "\n",
      "running train loss =   179791.465625\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 175685.359375[  100/  500]\n",
      "\n",
      "running train loss =   177588.03125\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 178341.921875[  100/  500]\n",
      "\n",
      "running train loss =   175286.85\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 168148.359375[  100/  500]\n",
      "\n",
      "running train loss =   173339.859375\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150530.406250[  100/  500]\n",
      "\n",
      "running train loss =   171209.896875\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 160907.375000[  100/  500]\n",
      "\n",
      "running train loss =   169172.115625\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 173731.406250[  100/  500]\n",
      "\n",
      "running train loss =   167289.66875\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 168367.500000[  100/  500]\n",
      "\n",
      "running train loss =   165440.853125\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 160548.781250[  100/  500]\n",
      "\n",
      "running train loss =   163648.8625\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 170133.765625[  100/  500]\n",
      "\n",
      "running train loss =   161896.821875\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 155496.953125[  100/  500]\n",
      "\n",
      "running train loss =   160192.846875\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 166503.203125[  100/  500]\n",
      "\n",
      "running train loss =   158619.0484375\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 200711.078125[  100/  500]\n",
      "\n",
      "running train loss =   157028.725\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 170922.656250[  100/  500]\n",
      "\n",
      "running train loss =   155463.3859375\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 183816.125000[  100/  500]\n",
      "\n",
      "running train loss =   154001.078125\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145576.109375[  100/  500]\n",
      "\n",
      "running train loss =   152585.684375\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148059.312500[  100/  500]\n",
      "\n",
      "running train loss =   151261.959375\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113881.648438[  100/  500]\n",
      "\n",
      "running train loss =   149873.9578125\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107901.812500[  100/  500]\n",
      "\n",
      "running train loss =   148593.146875\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 151624.734375[  100/  500]\n",
      "\n",
      "running train loss =   147415.9875\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 138461.453125[  100/  500]\n",
      "\n",
      "running train loss =   146223.3390625\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128276.250000[  100/  500]\n",
      "\n",
      "running train loss =   145077.86875\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 146621.437500[  100/  500]\n",
      "\n",
      "running train loss =   143956.3171875\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 138267.328125[  100/  500]\n",
      "\n",
      "running train loss =   142918.1578125\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118676.289062[  100/  500]\n",
      "\n",
      "running train loss =   141846.5890625\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 144245.156250[  100/  500]\n",
      "\n",
      "running train loss =   140875.4796875\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 142918.812500[  100/  500]\n",
      "\n",
      "running train loss =   139940.36875\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 159295.531250[  100/  500]\n",
      "\n",
      "running train loss =   139000.1\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147695.015625[  100/  500]\n",
      "\n",
      "running train loss =   138091.2796875\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150589.656250[  100/  500]\n",
      "\n",
      "running train loss =   137260.7109375\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120315.976562[  100/  500]\n",
      "\n",
      "running train loss =   136374.1515625\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134037.265625[  100/  500]\n",
      "\n",
      "running train loss =   135644.54375\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134602.875000[  100/  500]\n",
      "\n",
      "running train loss =   134901.5234375\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122779.640625[  100/  500]\n",
      "\n",
      "running train loss =   134113.5625\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122048.773438[  100/  500]\n",
      "\n",
      "running train loss =   133418.971875\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123418.156250[  100/  500]\n",
      "\n",
      "running train loss =   132715.9109375\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 133049.593750[  100/  500]\n",
      "\n",
      "running train loss =   132046.1390625\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 149243.046875[  100/  500]\n",
      "\n",
      "running train loss =   131408.15625\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116517.070312[  100/  500]\n",
      "\n",
      "running train loss =   130837.4359375\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128327.742188[  100/  500]\n",
      "\n",
      "running train loss =   130217.36875\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126781.023438[  100/  500]\n",
      "\n",
      "running train loss =   129655.546875\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127997.406250[  100/  500]\n",
      "\n",
      "running train loss =   129095.6828125\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122979.296875[  100/  500]\n",
      "\n",
      "running train loss =   128560.3515625\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114616.179688[  100/  500]\n",
      "\n",
      "running train loss =   128067.1734375\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135883.218750[  100/  500]\n",
      "\n",
      "running train loss =   127576.2671875\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88626.828125[  100/  500]\n",
      "\n",
      "running train loss =   127103.453125\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140387.921875[  100/  500]\n",
      "\n",
      "running train loss =   126610.5859375\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95723.703125[  100/  500]\n",
      "\n",
      "running train loss =   126193.9078125\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128574.328125[  100/  500]\n",
      "\n",
      "running train loss =   125752.1375\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 139915.031250[  100/  500]\n",
      "\n",
      "running train loss =   125346.3453125\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137761.046875[  100/  500]\n",
      "\n",
      "running train loss =   124968.525\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119536.757812[  100/  500]\n",
      "\n",
      "running train loss =   124560.0421875\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 144382.593750[  100/  500]\n",
      "\n",
      "running train loss =   124219.0734375\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 150787.203125[  100/  500]\n",
      "\n",
      "running train loss =   123821.19375\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126745.882812[  100/  500]\n",
      "\n",
      "running train loss =   123504.809375\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137349.000000[  100/  500]\n",
      "\n",
      "running train loss =   123154.4984375\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92684.656250[  100/  500]\n",
      "\n",
      "running train loss =   122822.5296875\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123106.281250[  100/  500]\n",
      "\n",
      "running train loss =   122523.5078125\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120169.429688[  100/  500]\n",
      "\n",
      "running train loss =   122204.6140625\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113261.312500[  100/  500]\n",
      "\n",
      "running train loss =   121931.6203125\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130559.710938[  100/  500]\n",
      "\n",
      "running train loss =   121638.0109375\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122228.296875[  100/  500]\n",
      "\n",
      "running train loss =   121366.1109375\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109703.921875[  100/  500]\n",
      "\n",
      "running train loss =   121095.0765625\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106293.781250[  100/  500]\n",
      "\n",
      "running train loss =   120837.0515625\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113966.796875[  100/  500]\n",
      "\n",
      "running train loss =   120559.5796875\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126216.656250[  100/  500]\n",
      "\n",
      "running train loss =   120317.5890625\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 132397.562500[  100/  500]\n",
      "\n",
      "running train loss =   120095.3234375\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140684.453125[  100/  500]\n",
      "\n",
      "running train loss =   119867.2203125\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105806.648438[  100/  500]\n",
      "\n",
      "running train loss =   119639.4953125\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99314.132812[  100/  500]\n",
      "\n",
      "running train loss =   119402.36875\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123070.921875[  100/  500]\n",
      "\n",
      "running train loss =   119194.8140625\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135543.046875[  100/  500]\n",
      "\n",
      "running train loss =   118995.415625\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92993.398438[  100/  500]\n",
      "\n",
      "running train loss =   118778.3828125\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124823.796875[  100/  500]\n",
      "\n",
      "running train loss =   118591.2671875\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115708.132812[  100/  500]\n",
      "\n",
      "running train loss =   118381.16875\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117052.078125[  100/  500]\n",
      "\n",
      "running train loss =   118195.409375\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99869.281250[  100/  500]\n",
      "\n",
      "running train loss =   118019.2765625\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119539.867188[  100/  500]\n",
      "\n",
      "running train loss =   117814.7265625\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118230.156250[  100/  500]\n",
      "\n",
      "running train loss =   117650.121875\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116860.359375[  100/  500]\n",
      "\n",
      "running train loss =   117452.6875\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119491.179688[  100/  500]\n",
      "\n",
      "running train loss =   117284.7796875\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 136088.406250[  100/  500]\n",
      "\n",
      "running train loss =   117122.7359375\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129773.046875[  100/  500]\n",
      "\n",
      "running train loss =   116944.946875\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104125.570312[  100/  500]\n",
      "\n",
      "running train loss =   116785.63125\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102811.320312[  100/  500]\n",
      "\n",
      "running train loss =   116623.7171875\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92987.968750[  100/  500]\n",
      "\n",
      "running train loss =   116462.815625\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121013.562500[  100/  500]\n",
      "\n",
      "running train loss =   116293.0203125\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 141116.656250[  100/  500]\n",
      "\n",
      "running train loss =   116151.6234375\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 146274.718750[  100/  500]\n",
      "\n",
      "running train loss =   115986.06875\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123390.203125[  100/  500]\n",
      "\n",
      "running train loss =   115834.3828125\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101884.859375[  100/  500]\n",
      "\n",
      "running train loss =   115682.528125\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108756.179688[  100/  500]\n",
      "\n",
      "running train loss =   115546.75\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112756.062500[  100/  500]\n",
      "\n",
      "running train loss =   115392.790625\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121736.132812[  100/  500]\n",
      "\n",
      "running train loss =   115236.2171875\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116615.226562[  100/  500]\n",
      "\n",
      "running train loss =   115091.10625\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107634.773438[  100/  500]\n",
      "\n",
      "running train loss =   114950.6609375\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123292.468750[  100/  500]\n",
      "\n",
      "running train loss =   114803.409375\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95646.882812[  100/  500]\n",
      "\n",
      "running train loss =   114663.4125\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135601.656250[  100/  500]\n",
      "\n",
      "running train loss =   114517.3140625\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118021.398438[  100/  500]\n",
      "\n",
      "running train loss =   114375.159375\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128203.312500[  100/  500]\n",
      "\n",
      "running train loss =   114246.7375\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107569.812500[  100/  500]\n",
      "\n",
      "running train loss =   114094.8640625\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125110.593750[  100/  500]\n",
      "\n",
      "running train loss =   113957.1296875\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 127399.273438[  100/  500]\n",
      "\n",
      "running train loss =   113816.775\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126104.343750[  100/  500]\n",
      "\n",
      "running train loss =   113676.40625\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92949.523438[  100/  500]\n",
      "\n",
      "running train loss =   113544.0828125\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124826.000000[  100/  500]\n",
      "\n",
      "running train loss =   113403.18125\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111637.156250[  100/  500]\n",
      "\n",
      "running train loss =   113255.996875\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110945.960938[  100/  500]\n",
      "\n",
      "running train loss =   113118.1046875\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100217.539062[  100/  500]\n",
      "\n",
      "running train loss =   112986.165625\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107083.109375[  100/  500]\n",
      "\n",
      "running train loss =   112841.5390625\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106350.320312[  100/  500]\n",
      "\n",
      "running train loss =   112709.259375\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118690.578125[  100/  500]\n",
      "\n",
      "running train loss =   112576.3890625\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112978.320312[  100/  500]\n",
      "\n",
      "running train loss =   112429.2671875\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117098.726562[  100/  500]\n",
      "\n",
      "running train loss =   112293.6609375\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99826.382812[  100/  500]\n",
      "\n",
      "running train loss =   112148.5890625\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123729.632812[  100/  500]\n",
      "\n",
      "running train loss =   112017.328125\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117088.289062[  100/  500]\n",
      "\n",
      "running train loss =   111873.54375\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130756.289062[  100/  500]\n",
      "\n",
      "running train loss =   111741.740625\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126540.640625[  100/  500]\n",
      "\n",
      "running train loss =   111596.371875\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107715.242188[  100/  500]\n",
      "\n",
      "running train loss =   111455.4546875\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92617.500000[  100/  500]\n",
      "\n",
      "running train loss =   111315.6296875\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112708.359375[  100/  500]\n",
      "\n",
      "running train loss =   111175.9234375\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119615.828125[  100/  500]\n",
      "\n",
      "running train loss =   111035.2953125\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113773.406250[  100/  500]\n",
      "\n",
      "running train loss =   110897.73125\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122861.960938[  100/  500]\n",
      "\n",
      "running train loss =   110755.209375\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143078.093750[  100/  500]\n",
      "\n",
      "running train loss =   110617.0515625\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103354.218750[  100/  500]\n",
      "\n",
      "running train loss =   110473.5796875\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103940.578125[  100/  500]\n",
      "\n",
      "running train loss =   110320.1828125\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121136.257812[  100/  500]\n",
      "\n",
      "running train loss =   110184.0203125\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110331.523438[  100/  500]\n",
      "\n",
      "running train loss =   110040.2875\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120819.523438[  100/  500]\n",
      "\n",
      "running train loss =   109892.003125\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93647.726562[  100/  500]\n",
      "\n",
      "running train loss =   109746.571875\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 110893.757812[  100/  500]\n",
      "\n",
      "running train loss =   109609.4515625\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106275.953125[  100/  500]\n",
      "\n",
      "running train loss =   109457.5578125\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91342.148438[  100/  500]\n",
      "\n",
      "running train loss =   109316.4984375\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113365.218750[  100/  500]\n",
      "\n",
      "running train loss =   109172.4453125\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78760.492188[  100/  500]\n",
      "\n",
      "running train loss =   109015.0953125\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116020.289062[  100/  500]\n",
      "\n",
      "running train loss =   108876.328125\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81200.679688[  100/  500]\n",
      "\n",
      "running train loss =   108722.29375\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92088.828125[  100/  500]\n",
      "\n",
      "running train loss =   108574.3890625\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112552.640625[  100/  500]\n",
      "\n",
      "running train loss =   108431.4640625\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108737.921875[  100/  500]\n",
      "\n",
      "running train loss =   108278.8890625\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90615.757812[  100/  500]\n",
      "\n",
      "running train loss =   108123.728125\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117356.671875[  100/  500]\n",
      "\n",
      "running train loss =   107976.953125\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109358.359375[  100/  500]\n",
      "\n",
      "running train loss =   107821.6\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80043.109375[  100/  500]\n",
      "\n",
      "running train loss =   107672.4078125\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114290.117188[  100/  500]\n",
      "\n",
      "running train loss =   107525.0734375\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114294.460938[  100/  500]\n",
      "\n",
      "running train loss =   107374.0765625\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99792.359375[  100/  500]\n",
      "\n",
      "running train loss =   107210.7515625\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129116.460938[  100/  500]\n",
      "\n",
      "running train loss =   107062.946875\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75446.578125[  100/  500]\n",
      "\n",
      "running train loss =   106905.4515625\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96518.203125[  100/  500]\n",
      "\n",
      "running train loss =   106751.3640625\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99285.617188[  100/  500]\n",
      "\n",
      "running train loss =   106595.9734375\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101936.523438[  100/  500]\n",
      "\n",
      "running train loss =   106443.31875\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111908.421875[  100/  500]\n",
      "\n",
      "running train loss =   106281.7578125\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94098.117188[  100/  500]\n",
      "\n",
      "running train loss =   106123.865625\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116242.570312[  100/  500]\n",
      "\n",
      "running train loss =   105970.2328125\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101832.726562[  100/  500]\n",
      "\n",
      "running train loss =   105807.425\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107719.976562[  100/  500]\n",
      "\n",
      "running train loss =   105655.384375\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100411.781250[  100/  500]\n",
      "\n",
      "running train loss =   105493.4984375\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104179.460938[  100/  500]\n",
      "\n",
      "running train loss =   105340.384375\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88777.718750[  100/  500]\n",
      "\n",
      "running train loss =   105167.8140625\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105811.242188[  100/  500]\n",
      "\n",
      "running train loss =   105012.1703125\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107937.210938[  100/  500]\n",
      "\n",
      "running train loss =   104853.971875\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113613.531250[  100/  500]\n",
      "\n",
      "running train loss =   104694.6671875\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115240.289062[  100/  500]\n",
      "\n",
      "running train loss =   104534.2875\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103869.406250[  100/  500]\n",
      "\n",
      "running train loss =   104368.2125\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100248.953125[  100/  500]\n",
      "\n",
      "running train loss =   104204.0078125\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107930.328125[  100/  500]\n",
      "\n",
      "running train loss =   104046.678125\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95031.023438[  100/  500]\n",
      "\n",
      "running train loss =   103877.271875\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100005.320312[  100/  500]\n",
      "\n",
      "running train loss =   103710.190625\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97863.187500[  100/  500]\n",
      "\n",
      "running train loss =   103555.9203125\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105864.078125[  100/  500]\n",
      "\n",
      "running train loss =   103386.5640625\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 137093.578125[  100/  500]\n",
      "\n",
      "running train loss =   103220.9703125\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90543.289062[  100/  500]\n",
      "\n",
      "running train loss =   103049.0640625\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 109129.382812[  100/  500]\n",
      "\n",
      "running train loss =   102884.609375\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123925.390625[  100/  500]\n",
      "\n",
      "running train loss =   102717.309375\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121585.601562[  100/  500]\n",
      "\n",
      "running train loss =   102561.734375\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120157.796875[  100/  500]\n",
      "\n",
      "running train loss =   102389.2875\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105791.679688[  100/  500]\n",
      "\n",
      "running train loss =   102215.71875\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104261.148438[  100/  500]\n",
      "\n",
      "running train loss =   102044.8015625\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98588.867188[  100/  500]\n",
      "\n",
      "running train loss =   101881.30625\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83948.109375[  100/  500]\n",
      "\n",
      "running train loss =   101705.059375\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95394.359375[  100/  500]\n",
      "\n",
      "running train loss =   101540.771875\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103190.640625[  100/  500]\n",
      "\n",
      "running train loss =   101376.1984375\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83291.781250[  100/  500]\n",
      "\n",
      "running train loss =   101197.1125\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102994.312500[  100/  500]\n",
      "\n",
      "running train loss =   101025.828125\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103270.523438[  100/  500]\n",
      "\n",
      "running train loss =   100852.9765625\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107279.000000[  100/  500]\n",
      "\n",
      "running train loss =   100681.9046875\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89310.593750[  100/  500]\n",
      "\n",
      "running train loss =   100515.2953125\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123094.367188[  100/  500]\n",
      "\n",
      "running train loss =   100338.73125\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106137.101562[  100/  500]\n",
      "\n",
      "running train loss =   100163.4796875\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94170.421875[  100/  500]\n",
      "\n",
      "running train loss =   99992.65625\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99877.929688[  100/  500]\n",
      "\n",
      "running train loss =   99817.6859375\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83779.843750[  100/  500]\n",
      "\n",
      "running train loss =   99639.040625\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98962.851562[  100/  500]\n",
      "\n",
      "running train loss =   99462.0046875\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85862.296875[  100/  500]\n",
      "\n",
      "running train loss =   99289.053125\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91206.617188[  100/  500]\n",
      "\n",
      "running train loss =   99113.590625\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97138.632812[  100/  500]\n",
      "\n",
      "running train loss =   98935.8453125\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100045.968750[  100/  500]\n",
      "\n",
      "running train loss =   98756.4171875\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100012.640625[  100/  500]\n",
      "\n",
      "running train loss =   98589.690625\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88341.476562[  100/  500]\n",
      "\n",
      "running train loss =   98400.9890625\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79231.476562[  100/  500]\n",
      "\n",
      "running train loss =   98229.4765625\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85360.929688[  100/  500]\n",
      "\n",
      "running train loss =   98048.4578125\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107082.632812[  100/  500]\n",
      "\n",
      "running train loss =   97868.996875\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98008.343750[  100/  500]\n",
      "\n",
      "running train loss =   97688.2953125\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120139.421875[  100/  500]\n",
      "\n",
      "running train loss =   97513.7734375\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86322.929688[  100/  500]\n",
      "\n",
      "running train loss =   97325.175\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118614.773438[  100/  500]\n",
      "\n",
      "running train loss =   97154.3640625\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 82695.914062[  100/  500]\n",
      "\n",
      "running train loss =   96967.6515625\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96996.976562[  100/  500]\n",
      "\n",
      "running train loss =   96791.990625\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87234.648438[  100/  500]\n",
      "\n",
      "running train loss =   96604.828125\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94935.921875[  100/  500]\n",
      "\n",
      "running train loss =   96427.3703125\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97247.781250[  100/  500]\n",
      "\n",
      "running train loss =   96237.5625\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101631.562500[  100/  500]\n",
      "\n",
      "running train loss =   96063.790625\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 82510.125000[  100/  500]\n",
      "\n",
      "running train loss =   95869.5953125\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77551.281250[  100/  500]\n",
      "\n",
      "running train loss =   95694.378125\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62183.335938[  100/  500]\n",
      "\n",
      "running train loss =   95509.003125\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86017.789062[  100/  500]\n",
      "\n",
      "running train loss =   95328.64375\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107071.570312[  100/  500]\n",
      "\n",
      "running train loss =   95150.9609375\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87002.257812[  100/  500]\n",
      "\n",
      "running train loss =   94958.390625\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105812.640625[  100/  500]\n",
      "\n",
      "running train loss =   94775.39375\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83300.039062[  100/  500]\n",
      "\n",
      "running train loss =   94589.546875\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87207.718750[  100/  500]\n",
      "\n",
      "running train loss =   94402.0015625\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113452.531250[  100/  500]\n",
      "\n",
      "running train loss =   94216.284375\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90791.867188[  100/  500]\n",
      "\n",
      "running train loss =   94026.7171875\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84472.859375[  100/  500]\n",
      "\n",
      "running train loss =   93842.3640625\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76717.062500[  100/  500]\n",
      "\n",
      "running train loss =   93662.46875\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95201.406250[  100/  500]\n",
      "\n",
      "running train loss =   93483.475\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101880.781250[  100/  500]\n",
      "\n",
      "running train loss =   93287.3234375\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104012.109375[  100/  500]\n",
      "\n",
      "running train loss =   93109.0953125\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80000.898438[  100/  500]\n",
      "\n",
      "running train loss =   92915.04375\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85249.101562[  100/  500]\n",
      "\n",
      "running train loss =   92728.69375\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95622.710938[  100/  500]\n",
      "\n",
      "running train loss =   92543.2484375\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99976.906250[  100/  500]\n",
      "\n",
      "running train loss =   92362.4015625\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126419.187500[  100/  500]\n",
      "\n",
      "running train loss =   92169.2640625\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76570.640625[  100/  500]\n",
      "\n",
      "running train loss =   91972.015625\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106852.750000[  100/  500]\n",
      "\n",
      "running train loss =   91795.1453125\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100204.812500[  100/  500]\n",
      "\n",
      "running train loss =   91600.378125\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73978.531250[  100/  500]\n",
      "\n",
      "running train loss =   91408.4234375\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89545.703125[  100/  500]\n",
      "\n",
      "running train loss =   91224.3484375\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87113.250000[  100/  500]\n",
      "\n",
      "running train loss =   91037.0109375\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88483.320312[  100/  500]\n",
      "\n",
      "running train loss =   90846.3671875\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89227.757812[  100/  500]\n",
      "\n",
      "running train loss =   90666.0765625\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85635.000000[  100/  500]\n",
      "\n",
      "running train loss =   90475.1515625\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 125736.179688[  100/  500]\n",
      "\n",
      "running train loss =   90282.55703125\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79216.070312[  100/  500]\n",
      "\n",
      "running train loss =   90092.5140625\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88160.703125[  100/  500]\n",
      "\n",
      "running train loss =   89896.79375\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86275.929688[  100/  500]\n",
      "\n",
      "running train loss =   89719.70625\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96457.382812[  100/  500]\n",
      "\n",
      "running train loss =   89534.6703125\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 102884.468750[  100/  500]\n",
      "\n",
      "running train loss =   89330.115625\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87515.617188[  100/  500]\n",
      "\n",
      "running train loss =   89142.590625\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85002.953125[  100/  500]\n",
      "\n",
      "running train loss =   88953.8734375\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111826.007812[  100/  500]\n",
      "\n",
      "running train loss =   88775.696875\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92487.820312[  100/  500]\n",
      "\n",
      "running train loss =   88575.659375\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 107424.078125[  100/  500]\n",
      "\n",
      "running train loss =   88375.484375\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80188.492188[  100/  500]\n",
      "\n",
      "running train loss =   88185.978125\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105329.648438[  100/  500]\n",
      "\n",
      "running train loss =   88020.009375\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80158.328125[  100/  500]\n",
      "\n",
      "running train loss =   87811.934375\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89460.976562[  100/  500]\n",
      "\n",
      "running train loss =   87615.496875\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105008.093750[  100/  500]\n",
      "\n",
      "running train loss =   87433.94140625\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91878.242188[  100/  500]\n",
      "\n",
      "running train loss =   87235.9\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85718.757812[  100/  500]\n",
      "\n",
      "running train loss =   87045.46328125\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78027.140625[  100/  500]\n",
      "\n",
      "running train loss =   86849.6609375\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79800.539062[  100/  500]\n",
      "\n",
      "running train loss =   86666.3796875\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77053.007812[  100/  500]\n",
      "\n",
      "running train loss =   86467.6234375\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67646.210938[  100/  500]\n",
      "\n",
      "running train loss =   86272.04375\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90536.132812[  100/  500]\n",
      "\n",
      "running train loss =   86093.290625\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86379.218750[  100/  500]\n",
      "\n",
      "running train loss =   85899.64375\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97478.992188[  100/  500]\n",
      "\n",
      "running train loss =   85704.9390625\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86530.976562[  100/  500]\n",
      "\n",
      "running train loss =   85508.525\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100070.617188[  100/  500]\n",
      "\n",
      "running train loss =   85321.4328125\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94183.851562[  100/  500]\n",
      "\n",
      "running train loss =   85129.215625\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72007.343750[  100/  500]\n",
      "\n",
      "running train loss =   84931.790625\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70811.242188[  100/  500]\n",
      "\n",
      "running train loss =   84742.09296875\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 71359.828125[  100/  500]\n",
      "\n",
      "running train loss =   84552.7171875\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78436.109375[  100/  500]\n",
      "\n",
      "running train loss =   84368.8234375\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79688.531250[  100/  500]\n",
      "\n",
      "running train loss =   84165.7078125\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89270.429688[  100/  500]\n",
      "\n",
      "running train loss =   83971.334375\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76671.187500[  100/  500]\n",
      "\n",
      "running train loss =   83783.85\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95505.031250[  100/  500]\n",
      "\n",
      "running train loss =   83591.3828125\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105021.601562[  100/  500]\n",
      "\n",
      "running train loss =   83395.6078125\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 82166.726562[  100/  500]\n",
      "\n",
      "running train loss =   83199.3375\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84592.179688[  100/  500]\n",
      "\n",
      "running train loss =   83011.5484375\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85793.617188[  100/  500]\n",
      "\n",
      "running train loss =   82818.2359375\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77674.320312[  100/  500]\n",
      "\n",
      "running train loss =   82623.0140625\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101306.859375[  100/  500]\n",
      "\n",
      "running train loss =   82431.9546875\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75864.281250[  100/  500]\n",
      "\n",
      "running train loss =   82232.3984375\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83210.882812[  100/  500]\n",
      "\n",
      "running train loss =   82048.3109375\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78253.679688[  100/  500]\n",
      "\n",
      "running train loss =   81857.1234375\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87552.828125[  100/  500]\n",
      "\n",
      "running train loss =   81661.3953125\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75493.375000[  100/  500]\n",
      "\n",
      "running train loss =   81466.509375\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75706.109375[  100/  500]\n",
      "\n",
      "running train loss =   81272.6875\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92808.406250[  100/  500]\n",
      "\n",
      "running train loss =   81093.271875\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77315.398438[  100/  500]\n",
      "\n",
      "running train loss =   80886.309375\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77759.046875[  100/  500]\n",
      "\n",
      "running train loss =   80695.903125\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84195.367188[  100/  500]\n",
      "\n",
      "running train loss =   80500.0875\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99840.742188[  100/  500]\n",
      "\n",
      "running train loss =   80318.69140625\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78904.953125[  100/  500]\n",
      "\n",
      "running train loss =   80117.61796875\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75811.414062[  100/  500]\n",
      "\n",
      "running train loss =   79924.475\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77088.296875[  100/  500]\n",
      "\n",
      "running train loss =   79736.05859375\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75429.492188[  100/  500]\n",
      "\n",
      "running train loss =   79542.3703125\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74329.562500[  100/  500]\n",
      "\n",
      "running train loss =   79346.165625\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76040.843750[  100/  500]\n",
      "\n",
      "running train loss =   79161.90625\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92628.460938[  100/  500]\n",
      "\n",
      "running train loss =   78970.6375\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73236.531250[  100/  500]\n",
      "\n",
      "running train loss =   78768.2484375\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73108.984375[  100/  500]\n",
      "\n",
      "running train loss =   78579.8125\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86014.648438[  100/  500]\n",
      "\n",
      "running train loss =   78390.0140625\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85655.359375[  100/  500]\n",
      "\n",
      "running train loss =   78199.29140625\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92931.921875[  100/  500]\n",
      "\n",
      "running train loss =   78007.846875\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63727.468750[  100/  500]\n",
      "\n",
      "running train loss =   77807.27890625\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74891.781250[  100/  500]\n",
      "\n",
      "running train loss =   77623.4796875\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74450.796875[  100/  500]\n",
      "\n",
      "running train loss =   77424.3234375\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 71372.656250[  100/  500]\n",
      "\n",
      "running train loss =   77239.0671875\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99473.679688[  100/  500]\n",
      "\n",
      "running train loss =   77045.30234375\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72698.000000[  100/  500]\n",
      "\n",
      "running train loss =   76850.6953125\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 83997.226562[  100/  500]\n",
      "\n",
      "running train loss =   76661.7734375\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65724.382812[  100/  500]\n",
      "\n",
      "running train loss =   76470.3609375\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67865.507812[  100/  500]\n",
      "\n",
      "running train loss =   76283.75546875\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76340.781250[  100/  500]\n",
      "\n",
      "running train loss =   76088.5765625\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75388.750000[  100/  500]\n",
      "\n",
      "running train loss =   75894.5890625\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65184.699219[  100/  500]\n",
      "\n",
      "running train loss =   75713.0109375\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 59963.429688[  100/  500]\n",
      "\n",
      "running train loss =   75524.6828125\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72228.851562[  100/  500]\n",
      "\n",
      "running train loss =   75329.65\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70750.906250[  100/  500]\n",
      "\n",
      "running train loss =   75131.49296875\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61304.355469[  100/  500]\n",
      "\n",
      "running train loss =   74951.90234375\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63514.550781[  100/  500]\n",
      "\n",
      "running train loss =   74749.61171875\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78258.671875[  100/  500]\n",
      "\n",
      "running train loss =   74565.7953125\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81623.429688[  100/  500]\n",
      "\n",
      "running train loss =   74378.3453125\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73431.359375[  100/  500]\n",
      "\n",
      "running train loss =   74190.75390625\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66969.781250[  100/  500]\n",
      "\n",
      "running train loss =   73999.74765625\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98838.257812[  100/  500]\n",
      "\n",
      "running train loss =   73813.81796875\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67636.671875[  100/  500]\n",
      "\n",
      "running train loss =   73619.565625\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77746.687500[  100/  500]\n",
      "\n",
      "running train loss =   73427.6875\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 71388.187500[  100/  500]\n",
      "\n",
      "running train loss =   73236.609375\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 95798.101562[  100/  500]\n",
      "\n",
      "running train loss =   73063.0296875\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68174.007812[  100/  500]\n",
      "\n",
      "running train loss =   72859.7703125\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67493.859375[  100/  500]\n",
      "\n",
      "running train loss =   72675.8078125\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 82264.054688[  100/  500]\n",
      "\n",
      "running train loss =   72490.209375\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67663.679688[  100/  500]\n",
      "\n",
      "running train loss =   72302.30859375\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69234.671875[  100/  500]\n",
      "\n",
      "running train loss =   72114.47109375\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74260.398438[  100/  500]\n",
      "\n",
      "running train loss =   71930.3390625\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68935.929688[  100/  500]\n",
      "\n",
      "running train loss =   71737.52109375\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75594.617188[  100/  500]\n",
      "\n",
      "running train loss =   71556.15625\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61594.878906[  100/  500]\n",
      "\n",
      "running train loss =   71362.9234375\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67954.445312[  100/  500]\n",
      "\n",
      "running train loss =   71179.6546875\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73113.546875[  100/  500]\n",
      "\n",
      "running train loss =   70993.50546875\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84306.523438[  100/  500]\n",
      "\n",
      "running train loss =   70810.20078125\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77149.226562[  100/  500]\n",
      "\n",
      "running train loss =   70619.446875\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77100.882812[  100/  500]\n",
      "\n",
      "running train loss =   70444.03671875\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55551.199219[  100/  500]\n",
      "\n",
      "running train loss =   70253.2734375\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60568.753906[  100/  500]\n",
      "\n",
      "running train loss =   70063.29765625\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62448.191406[  100/  500]\n",
      "\n",
      "running train loss =   69878.25625\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57330.191406[  100/  500]\n",
      "\n",
      "running train loss =   69693.79609375\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66576.937500[  100/  500]\n",
      "\n",
      "running train loss =   69504.25703125\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68711.421875[  100/  500]\n",
      "\n",
      "running train loss =   69323.67265625\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68403.242188[  100/  500]\n",
      "\n",
      "running train loss =   69141.75546875\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74116.867188[  100/  500]\n",
      "\n",
      "running train loss =   68968.67890625\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 82003.273438[  100/  500]\n",
      "\n",
      "running train loss =   68768.44609375\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 51713.324219[  100/  500]\n",
      "\n",
      "running train loss =   68586.94296875\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58521.800781[  100/  500]\n",
      "\n",
      "running train loss =   68399.50703125\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68984.281250[  100/  500]\n",
      "\n",
      "running train loss =   68219.21796875\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 71942.281250[  100/  500]\n",
      "\n",
      "running train loss =   68039.53828125\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 50308.843750[  100/  500]\n",
      "\n",
      "running train loss =   67854.74140625\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58522.039062[  100/  500]\n",
      "\n",
      "running train loss =   67671.2359375\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66490.304688[  100/  500]\n",
      "\n",
      "running train loss =   67493.43515625\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74733.023438[  100/  500]\n",
      "\n",
      "running train loss =   67311.475\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77859.789062[  100/  500]\n",
      "\n",
      "running train loss =   67126.77109375\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55395.320312[  100/  500]\n",
      "\n",
      "running train loss =   66936.2609375\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 53660.976562[  100/  500]\n",
      "\n",
      "running train loss =   66764.84375\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67055.304688[  100/  500]\n",
      "\n",
      "running train loss =   66584.796875\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79755.148438[  100/  500]\n",
      "\n",
      "running train loss =   66399.62890625\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 59243.378906[  100/  500]\n",
      "\n",
      "running train loss =   66227.79140625\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70764.859375[  100/  500]\n",
      "\n",
      "running train loss =   66045.08125\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61821.898438[  100/  500]\n",
      "\n",
      "running train loss =   65867.3828125\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74668.859375[  100/  500]\n",
      "\n",
      "running train loss =   65679.53515625\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62049.546875[  100/  500]\n",
      "\n",
      "running train loss =   65497.79453125\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57955.039062[  100/  500]\n",
      "\n",
      "running train loss =   65325.903125\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74417.976562[  100/  500]\n",
      "\n",
      "running train loss =   65142.7078125\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 64223.355469[  100/  500]\n",
      "\n",
      "running train loss =   64965.12109375\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77969.742188[  100/  500]\n",
      "\n",
      "running train loss =   64787.065625\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63873.039062[  100/  500]\n",
      "\n",
      "running train loss =   64602.37421875\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61682.269531[  100/  500]\n",
      "\n",
      "running train loss =   64424.5296875\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65232.601562[  100/  500]\n",
      "\n",
      "running train loss =   64262.71328125\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60978.593750[  100/  500]\n",
      "\n",
      "running train loss =   64072.83671875\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 59525.781250[  100/  500]\n",
      "\n",
      "running train loss =   63897.27421875\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69157.953125[  100/  500]\n",
      "\n",
      "running train loss =   63719.56953125\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40063.128906[  100/  500]\n",
      "\n",
      "running train loss =   63537.03046875\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 64352.753906[  100/  500]\n",
      "\n",
      "running train loss =   63367.60078125\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69016.421875[  100/  500]\n",
      "\n",
      "running train loss =   63185.0375\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58615.628906[  100/  500]\n",
      "\n",
      "running train loss =   63012.60625\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66699.398438[  100/  500]\n",
      "\n",
      "running train loss =   62837.903125\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69846.242188[  100/  500]\n",
      "\n",
      "running train loss =   62670.1390625\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56186.980469[  100/  500]\n",
      "\n",
      "running train loss =   62478.38984375\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68727.304688[  100/  500]\n",
      "\n",
      "running train loss =   62312.96328125\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57153.304688[  100/  500]\n",
      "\n",
      "running train loss =   62136.13125\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60829.953125[  100/  500]\n",
      "\n",
      "running train loss =   61967.209375\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61241.289062[  100/  500]\n",
      "\n",
      "running train loss =   61786.05390625\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63664.246094[  100/  500]\n",
      "\n",
      "running train loss =   61619.5671875\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56459.511719[  100/  500]\n",
      "\n",
      "running train loss =   61429.63125\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63662.148438[  100/  500]\n",
      "\n",
      "running train loss =   61269.15390625\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60582.265625[  100/  500]\n",
      "\n",
      "running train loss =   61096.4\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56346.960938[  100/  500]\n",
      "\n",
      "running train loss =   60919.0390625\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55703.761719[  100/  500]\n",
      "\n",
      "running train loss =   60747.8703125\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60133.746094[  100/  500]\n",
      "\n",
      "running train loss =   60577.06015625\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57594.945312[  100/  500]\n",
      "\n",
      "running train loss =   60398.940625\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61671.230469[  100/  500]\n",
      "\n",
      "running train loss =   60229.54609375\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 50925.214844[  100/  500]\n",
      "\n",
      "running train loss =   60053.39921875\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62428.261719[  100/  500]\n",
      "\n",
      "running train loss =   59890.55234375\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 64260.636719[  100/  500]\n",
      "\n",
      "running train loss =   59721.31328125\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66640.148438[  100/  500]\n",
      "\n",
      "running train loss =   59553.10234375\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56672.199219[  100/  500]\n",
      "\n",
      "running train loss =   59381.25546875\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57064.300781[  100/  500]\n",
      "\n",
      "running train loss =   59207.65625\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46580.035156[  100/  500]\n",
      "\n",
      "running train loss =   59034.23984375\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63059.511719[  100/  500]\n",
      "\n",
      "running train loss =   58871.6296875\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72667.929688[  100/  500]\n",
      "\n",
      "running train loss =   58709.1109375\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65197.675781[  100/  500]\n",
      "\n",
      "running train loss =   58529.9921875\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56530.765625[  100/  500]\n",
      "\n",
      "running train loss =   58364.25859375\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69466.062500[  100/  500]\n",
      "\n",
      "running train loss =   58198.69453125\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65166.824219[  100/  500]\n",
      "\n",
      "running train loss =   58025.78125\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68766.656250[  100/  500]\n",
      "\n",
      "running train loss =   57869.69765625\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38738.382812[  100/  500]\n",
      "\n",
      "running train loss =   57689.0328125\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39666.453125[  100/  500]\n",
      "\n",
      "running train loss =   57522.30390625\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56889.746094[  100/  500]\n",
      "\n",
      "running train loss =   57359.94375\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58779.964844[  100/  500]\n",
      "\n",
      "running train loss =   57192.69453125\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54304.546875[  100/  500]\n",
      "\n",
      "running train loss =   57031.03125\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62068.941406[  100/  500]\n",
      "\n",
      "running train loss =   56872.49375\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47796.460938[  100/  500]\n",
      "\n",
      "running train loss =   56696.3890625\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46621.996094[  100/  500]\n",
      "\n",
      "running train loss =   56529.07421875\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47261.316406[  100/  500]\n",
      "\n",
      "running train loss =   56365.0140625\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 59147.933594[  100/  500]\n",
      "\n",
      "running train loss =   56205.46171875\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61003.488281[  100/  500]\n",
      "\n",
      "running train loss =   56040.43828125\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52424.851562[  100/  500]\n",
      "\n",
      "running train loss =   55871.0875\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62991.035156[  100/  500]\n",
      "\n",
      "running train loss =   55713.39765625\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56430.128906[  100/  500]\n",
      "\n",
      "running train loss =   55549.35078125\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67009.726562[  100/  500]\n",
      "\n",
      "running train loss =   55389.05625\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 53660.558594[  100/  500]\n",
      "\n",
      "running train loss =   55224.74375\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46790.761719[  100/  500]\n",
      "\n",
      "running train loss =   55059.7875\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72361.015625[  100/  500]\n",
      "\n",
      "running train loss =   54903.96796875\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52017.179688[  100/  500]\n",
      "\n",
      "running train loss =   54737.440625\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54744.929688[  100/  500]\n",
      "\n",
      "running train loss =   54579.50078125\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61722.281250[  100/  500]\n",
      "\n",
      "running train loss =   54416.59921875\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57138.394531[  100/  500]\n",
      "\n",
      "running train loss =   54255.9625\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54054.160156[  100/  500]\n",
      "\n",
      "running train loss =   54100.89375\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58271.683594[  100/  500]\n",
      "\n",
      "running train loss =   53931.50390625\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61304.453125[  100/  500]\n",
      "\n",
      "running train loss =   53779.23515625\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56038.210938[  100/  500]\n",
      "\n",
      "running train loss =   53624.51953125\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55122.199219[  100/  500]\n",
      "\n",
      "running train loss =   53456.14296875\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43472.726562[  100/  500]\n",
      "\n",
      "running train loss =   53294.0015625\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42551.859375[  100/  500]\n",
      "\n",
      "running train loss =   53141.52734375\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56866.609375[  100/  500]\n",
      "\n",
      "running train loss =   52986.12578125\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54366.023438[  100/  500]\n",
      "\n",
      "running train loss =   52833.5265625\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41011.496094[  100/  500]\n",
      "\n",
      "running train loss =   52668.6671875\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 59176.750000[  100/  500]\n",
      "\n",
      "running train loss =   52516.12109375\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63281.750000[  100/  500]\n",
      "\n",
      "running train loss =   52361.775\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48067.640625[  100/  500]\n",
      "\n",
      "running train loss =   52197.534375\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60128.429688[  100/  500]\n",
      "\n",
      "running train loss =   52042.459375\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46762.683594[  100/  500]\n",
      "\n",
      "running train loss =   51893.17265625\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57558.058594[  100/  500]\n",
      "\n",
      "running train loss =   51733.3078125\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48487.003906[  100/  500]\n",
      "\n",
      "running train loss =   51575.64453125\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65591.343750[  100/  500]\n",
      "\n",
      "running train loss =   51426.10859375\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52267.410156[  100/  500]\n",
      "\n",
      "running train loss =   51274.83984375\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56365.718750[  100/  500]\n",
      "\n",
      "running train loss =   51118.1484375\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46297.988281[  100/  500]\n",
      "\n",
      "running train loss =   50958.53046875\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48165.941406[  100/  500]\n",
      "\n",
      "running train loss =   50808.87265625\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48294.410156[  100/  500]\n",
      "\n",
      "running train loss =   50657.4125\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46050.101562[  100/  500]\n",
      "\n",
      "running train loss =   50501.115625\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62559.808594[  100/  500]\n",
      "\n",
      "running train loss =   50354.09296875\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68759.500000[  100/  500]\n",
      "\n",
      "running train loss =   50201.1890625\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40206.230469[  100/  500]\n",
      "\n",
      "running train loss =   50044.24765625\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56423.980469[  100/  500]\n",
      "\n",
      "running train loss =   49898.52109375\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48521.656250[  100/  500]\n",
      "\n",
      "running train loss =   49755.1671875\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55906.953125[  100/  500]\n",
      "\n",
      "running train loss =   49591.82578125\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 50317.730469[  100/  500]\n",
      "\n",
      "running train loss =   49446.609375\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46964.085938[  100/  500]\n",
      "\n",
      "running train loss =   49303.22734375\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44431.148438[  100/  500]\n",
      "\n",
      "running train loss =   49144.65703125\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42436.046875[  100/  500]\n",
      "\n",
      "running train loss =   48997.53828125\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37336.273438[  100/  500]\n",
      "\n",
      "running train loss =   48848.80703125\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56792.171875[  100/  500]\n",
      "\n",
      "running train loss =   48701.534375\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43927.703125[  100/  500]\n",
      "\n",
      "running train loss =   48552.00703125\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46856.269531[  100/  500]\n",
      "\n",
      "running train loss =   48405.90546875\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58328.500000[  100/  500]\n",
      "\n",
      "running train loss =   48257.53828125\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49650.421875[  100/  500]\n",
      "\n",
      "running train loss =   48106.42578125\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46935.308594[  100/  500]\n",
      "\n",
      "running train loss =   47963.5890625\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44951.628906[  100/  500]\n",
      "\n",
      "running train loss =   47821.771875\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47827.414062[  100/  500]\n",
      "\n",
      "running train loss =   47677.48203125\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54113.074219[  100/  500]\n",
      "\n",
      "running train loss =   47529.41328125\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36076.406250[  100/  500]\n",
      "\n",
      "running train loss =   47379.17890625\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49215.585938[  100/  500]\n",
      "\n",
      "running train loss =   47237.803125\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52055.531250[  100/  500]\n",
      "\n",
      "running train loss =   47097.0375\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 45089.761719[  100/  500]\n",
      "\n",
      "running train loss =   46950.265625\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 50812.000000[  100/  500]\n",
      "\n",
      "running train loss =   46806.82578125\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48425.906250[  100/  500]\n",
      "\n",
      "running train loss =   46662.44921875\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40135.730469[  100/  500]\n",
      "\n",
      "running train loss =   46520.621875\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49478.660156[  100/  500]\n",
      "\n",
      "running train loss =   46378.87265625\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49123.558594[  100/  500]\n",
      "\n",
      "running train loss =   46233.88203125\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46816.886719[  100/  500]\n",
      "\n",
      "running train loss =   46091.81640625\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44747.886719[  100/  500]\n",
      "\n",
      "running train loss =   45949.125\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61249.410156[  100/  500]\n",
      "\n",
      "running train loss =   45814.9140625\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49380.058594[  100/  500]\n",
      "\n",
      "running train loss =   45668.6546875\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32296.937500[  100/  500]\n",
      "\n",
      "running train loss =   45524.9421875\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61520.949219[  100/  500]\n",
      "\n",
      "running train loss =   45393.7625\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49536.210938[  100/  500]\n",
      "\n",
      "running train loss =   45251.08203125\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39153.394531[  100/  500]\n",
      "\n",
      "running train loss =   45112.59453125\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44439.343750[  100/  500]\n",
      "\n",
      "running train loss =   44970.309375\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47356.253906[  100/  500]\n",
      "\n",
      "running train loss =   44839.096875\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39315.085938[  100/  500]\n",
      "\n",
      "running train loss =   44691.39609375\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43972.796875[  100/  500]\n",
      "\n",
      "running train loss =   44554.02734375\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 45326.503906[  100/  500]\n",
      "\n",
      "running train loss =   44422.38359375\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48397.144531[  100/  500]\n",
      "\n",
      "running train loss =   44281.7625\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48502.230469[  100/  500]\n",
      "\n",
      "running train loss =   44147.10859375\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43513.753906[  100/  500]\n",
      "\n",
      "running train loss =   44006.95625\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42711.093750[  100/  500]\n",
      "\n",
      "running train loss =   43872.6453125\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37282.628906[  100/  500]\n",
      "\n",
      "running train loss =   43742.0140625\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46283.511719[  100/  500]\n",
      "\n",
      "running train loss =   43606.4859375\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40142.464844[  100/  500]\n",
      "\n",
      "running train loss =   43467.26875\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39346.859375[  100/  500]\n",
      "\n",
      "running train loss =   43330.93828125\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43203.796875[  100/  500]\n",
      "\n",
      "running train loss =   43202.212109375\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42004.988281[  100/  500]\n",
      "\n",
      "running train loss =   43060.671875\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38349.933594[  100/  500]\n",
      "\n",
      "running train loss =   42936.37890625\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41658.238281[  100/  500]\n",
      "\n",
      "running train loss =   42798.54609375\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41783.828125[  100/  500]\n",
      "\n",
      "running train loss =   42666.63359375\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47752.734375[  100/  500]\n",
      "\n",
      "running train loss =   42535.8359375\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43295.703125[  100/  500]\n",
      "\n",
      "running train loss =   42400.28671875\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42819.335938[  100/  500]\n",
      "\n",
      "running train loss =   42270.5421875\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48553.796875[  100/  500]\n",
      "\n",
      "running train loss =   42143.66015625\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44419.214844[  100/  500]\n",
      "\n",
      "running train loss =   42010.07109375\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38346.242188[  100/  500]\n",
      "\n",
      "running train loss =   41885.4421875\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32903.886719[  100/  500]\n",
      "\n",
      "running train loss =   41754.09765625\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54973.628906[  100/  500]\n",
      "\n",
      "running train loss =   41626.565625\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40093.734375[  100/  500]\n",
      "\n",
      "running train loss =   41485.4671875\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31206.025391[  100/  500]\n",
      "\n",
      "running train loss =   41362.540234375\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44258.378906[  100/  500]\n",
      "\n",
      "running train loss =   41228.875\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43421.015625[  100/  500]\n",
      "\n",
      "running train loss =   41104.0109375\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48692.550781[  100/  500]\n",
      "\n",
      "running train loss =   40978.85859375\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33107.179688[  100/  500]\n",
      "\n",
      "running train loss =   40845.52578125\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42993.031250[  100/  500]\n",
      "\n",
      "running train loss =   40717.89453125\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37018.148438[  100/  500]\n",
      "\n",
      "running train loss =   40596.6234375\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34627.500000[  100/  500]\n",
      "\n",
      "running train loss =   40469.10703125\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36590.261719[  100/  500]\n",
      "\n",
      "running train loss =   40342.37421875\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40693.289062[  100/  500]\n",
      "\n",
      "running train loss =   40216.3234375\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30211.832031[  100/  500]\n",
      "\n",
      "running train loss =   40089.56875\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36566.226562[  100/  500]\n",
      "\n",
      "running train loss =   39963.90234375\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40788.972656[  100/  500]\n",
      "\n",
      "running train loss =   39841.109765625\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29846.435547[  100/  500]\n",
      "\n",
      "running train loss =   39716.853515625\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40179.281250[  100/  500]\n",
      "\n",
      "running train loss =   39593.990625\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35052.828125[  100/  500]\n",
      "\n",
      "running train loss =   39473.621484375\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37910.421875[  100/  500]\n",
      "\n",
      "running train loss =   39343.2015625\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47003.710938[  100/  500]\n",
      "\n",
      "running train loss =   39223.70234375\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39651.933594[  100/  500]\n",
      "\n",
      "running train loss =   39099.1828125\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38249.441406[  100/  500]\n",
      "\n",
      "running train loss =   38982.31875\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40649.281250[  100/  500]\n",
      "\n",
      "running train loss =   38858.5359375\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44567.683594[  100/  500]\n",
      "\n",
      "running train loss =   38733.72421875\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57928.906250[  100/  500]\n",
      "\n",
      "running train loss =   38626.570703125\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42118.585938[  100/  500]\n",
      "\n",
      "running train loss =   38489.923828125\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34983.375000[  100/  500]\n",
      "\n",
      "running train loss =   38373.766796875\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46482.445312[  100/  500]\n",
      "\n",
      "running train loss =   38252.9453125\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34408.007812[  100/  500]\n",
      "\n",
      "running train loss =   38129.6734375\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38211.199219[  100/  500]\n",
      "\n",
      "running train loss =   38013.8546875\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37242.652344[  100/  500]\n",
      "\n",
      "running train loss =   37893.4375\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29520.972656[  100/  500]\n",
      "\n",
      "running train loss =   37772.564453125\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37855.925781[  100/  500]\n",
      "\n",
      "running train loss =   37657.2390625\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38795.050781[  100/  500]\n",
      "\n",
      "running train loss =   37537.390625\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35249.277344[  100/  500]\n",
      "\n",
      "running train loss =   37419.69140625\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49311.238281[  100/  500]\n",
      "\n",
      "running train loss =   37306.962109375\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40327.136719[  100/  500]\n",
      "\n",
      "running train loss =   37182.8609375\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37373.445312[  100/  500]\n",
      "\n",
      "running train loss =   37068.11328125\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36876.062500[  100/  500]\n",
      "\n",
      "running train loss =   36955.756640625\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37234.164062[  100/  500]\n",
      "\n",
      "running train loss =   36834.93828125\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43073.808594[  100/  500]\n",
      "\n",
      "running train loss =   36723.15\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41026.675781[  100/  500]\n",
      "\n",
      "running train loss =   36604.990625\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37585.179688[  100/  500]\n",
      "\n",
      "running train loss =   36489.934765625\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40076.296875[  100/  500]\n",
      "\n",
      "running train loss =   36373.242578125\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29470.783203[  100/  500]\n",
      "\n",
      "running train loss =   36258.685546875\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40013.109375[  100/  500]\n",
      "\n",
      "running train loss =   36153.416796875\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33483.238281[  100/  500]\n",
      "\n",
      "running train loss =   36030.869140625\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37179.316406[  100/  500]\n",
      "\n",
      "running train loss =   35920.432421875\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38978.167969[  100/  500]\n",
      "\n",
      "running train loss =   35805.555859375\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40202.121094[  100/  500]\n",
      "\n",
      "running train loss =   35693.553515625\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36555.121094[  100/  500]\n",
      "\n",
      "running train loss =   35578.821875\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32700.539062[  100/  500]\n",
      "\n",
      "running train loss =   35468.912890625\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37363.261719[  100/  500]\n",
      "\n",
      "running train loss =   35357.319921875\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33405.871094[  100/  500]\n",
      "\n",
      "running train loss =   35244.96484375\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26994.931641[  100/  500]\n",
      "\n",
      "running train loss =   35133.59609375\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23911.199219[  100/  500]\n",
      "\n",
      "running train loss =   35022.741015625\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35355.468750[  100/  500]\n",
      "\n",
      "running train loss =   34910.45625\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33702.734375[  100/  500]\n",
      "\n",
      "running train loss =   34802.20859375\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36969.070312[  100/  500]\n",
      "\n",
      "running train loss =   34694.32265625\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30766.685547[  100/  500]\n",
      "\n",
      "running train loss =   34582.115234375\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40161.000000[  100/  500]\n",
      "\n",
      "running train loss =   34470.33046875\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35637.660156[  100/  500]\n",
      "\n",
      "running train loss =   34364.1359375\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34546.593750[  100/  500]\n",
      "\n",
      "running train loss =   34254.43828125\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35299.488281[  100/  500]\n",
      "\n",
      "running train loss =   34149.288671875\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38271.070312[  100/  500]\n",
      "\n",
      "running train loss =   34042.614453125\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33936.507812[  100/  500]\n",
      "\n",
      "running train loss =   33934.30703125\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29300.988281[  100/  500]\n",
      "\n",
      "running train loss =   33828.487890625\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33487.984375[  100/  500]\n",
      "\n",
      "running train loss =   33721.239453125\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35269.929688[  100/  500]\n",
      "\n",
      "running train loss =   33610.509765625\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31230.085938[  100/  500]\n",
      "\n",
      "running train loss =   33501.508984375\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31207.759766[  100/  500]\n",
      "\n",
      "running train loss =   33402.11484375\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27398.648438[  100/  500]\n",
      "\n",
      "running train loss =   33295.58046875\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29603.695312[  100/  500]\n",
      "\n",
      "running train loss =   33184.8640625\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28570.230469[  100/  500]\n",
      "\n",
      "running train loss =   33081.47890625\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31506.009766[  100/  500]\n",
      "\n",
      "running train loss =   32978.645703125\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42333.148438[  100/  500]\n",
      "\n",
      "running train loss =   32875.1578125\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30726.199219[  100/  500]\n",
      "\n",
      "running train loss =   32766.019140625\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40332.789062[  100/  500]\n",
      "\n",
      "running train loss =   32668.047265625\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28012.394531[  100/  500]\n",
      "\n",
      "running train loss =   32561.962890625\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30015.759766[  100/  500]\n",
      "\n",
      "running train loss =   32457.52734375\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33446.664062[  100/  500]\n",
      "\n",
      "running train loss =   32355.824609375\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31421.658203[  100/  500]\n",
      "\n",
      "running train loss =   32253.030859375\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24385.615234[  100/  500]\n",
      "\n",
      "running train loss =   32155.5625\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26479.113281[  100/  500]\n",
      "\n",
      "running train loss =   32047.01328125\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30120.009766[  100/  500]\n",
      "\n",
      "running train loss =   31948.01328125\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26523.966797[  100/  500]\n",
      "\n",
      "running train loss =   31847.123828125\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29154.669922[  100/  500]\n",
      "\n",
      "running train loss =   31743.84453125\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26430.177734[  100/  500]\n",
      "\n",
      "running train loss =   31647.408984375\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29326.835938[  100/  500]\n",
      "\n",
      "running train loss =   31546.8875\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33714.152344[  100/  500]\n",
      "\n",
      "running train loss =   31450.216015625\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29179.626953[  100/  500]\n",
      "\n",
      "running train loss =   31344.9921875\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29154.630859[  100/  500]\n",
      "\n",
      "running train loss =   31244.244921875\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29321.039062[  100/  500]\n",
      "\n",
      "running train loss =   31148.362890625\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22983.134766[  100/  500]\n",
      "\n",
      "running train loss =   31047.31171875\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35089.015625[  100/  500]\n",
      "\n",
      "running train loss =   30954.04921875\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32828.171875[  100/  500]\n",
      "\n",
      "running train loss =   30852.658203125\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40258.351562[  100/  500]\n",
      "\n",
      "running train loss =   30755.923046875\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25133.960938[  100/  500]\n",
      "\n",
      "running train loss =   30655.772265625\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29946.470703[  100/  500]\n",
      "\n",
      "running train loss =   30558.53203125\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30758.234375[  100/  500]\n",
      "\n",
      "running train loss =   30461.404296875\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28091.744141[  100/  500]\n",
      "\n",
      "running train loss =   30366.006640625\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30635.804688[  100/  500]\n",
      "\n",
      "running train loss =   30270.998046875\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32651.890625[  100/  500]\n",
      "\n",
      "running train loss =   30175.798828125\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29554.607422[  100/  500]\n",
      "\n",
      "running train loss =   30079.79765625\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30580.687500[  100/  500]\n",
      "\n",
      "running train loss =   29986.166796875\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36202.132812[  100/  500]\n",
      "\n",
      "running train loss =   29895.52421875\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26426.650391[  100/  500]\n",
      "\n",
      "running train loss =   29792.64765625\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28979.220703[  100/  500]\n",
      "\n",
      "running train loss =   29700.319140625\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36835.761719[  100/  500]\n",
      "\n",
      "running train loss =   29608.80234375\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32298.410156[  100/  500]\n",
      "\n",
      "running train loss =   29515.5625\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26363.667969[  100/  500]\n",
      "\n",
      "running train loss =   29418.14609375\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31125.320312[  100/  500]\n",
      "\n",
      "running train loss =   29326.94453125\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31127.636719[  100/  500]\n",
      "\n",
      "running train loss =   29235.579296875\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26712.640625[  100/  500]\n",
      "\n",
      "running train loss =   29142.528125\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29294.294922[  100/  500]\n",
      "\n",
      "running train loss =   29049.958203125\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31725.189453[  100/  500]\n",
      "\n",
      "running train loss =   28961.65859375\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35960.531250[  100/  500]\n",
      "\n",
      "running train loss =   28866.600390625\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37216.320312[  100/  500]\n",
      "\n",
      "running train loss =   28778.714453125\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27436.658203[  100/  500]\n",
      "\n",
      "running train loss =   28683.1359375\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30993.832031[  100/  500]\n",
      "\n",
      "running train loss =   28589.976171875\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26384.589844[  100/  500]\n",
      "\n",
      "running train loss =   28508.085546875\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29116.953125[  100/  500]\n",
      "\n",
      "running train loss =   28414.344140625\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29838.462891[  100/  500]\n",
      "\n",
      "running train loss =   28326.076171875\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20779.638672[  100/  500]\n",
      "\n",
      "running train loss =   28237.2609375\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28266.164062[  100/  500]\n",
      "\n",
      "running train loss =   28146.51875\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29858.310547[  100/  500]\n",
      "\n",
      "running train loss =   28059.0828125\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33246.593750[  100/  500]\n",
      "\n",
      "running train loss =   27971.545703125\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32223.462891[  100/  500]\n",
      "\n",
      "running train loss =   27882.105859375\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29773.976562[  100/  500]\n",
      "\n",
      "running train loss =   27791.459765625\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20450.681641[  100/  500]\n",
      "\n",
      "running train loss =   27701.08671875\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27586.255859[  100/  500]\n",
      "\n",
      "running train loss =   27622.93515625\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23973.560547[  100/  500]\n",
      "\n",
      "running train loss =   27531.453515625\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34423.671875[  100/  500]\n",
      "\n",
      "running train loss =   27446.774609375\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27353.892578[  100/  500]\n",
      "\n",
      "running train loss =   27356.52421875\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26587.556641[  100/  500]\n",
      "\n",
      "running train loss =   27270.180859375\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25205.107422[  100/  500]\n",
      "\n",
      "running train loss =   27185.918359375\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37852.796875[  100/  500]\n",
      "\n",
      "running train loss =   27104.0984375\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19636.566406[  100/  500]\n",
      "\n",
      "running train loss =   27018.462890625\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27842.037109[  100/  500]\n",
      "\n",
      "running train loss =   26932.10859375\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19780.623047[  100/  500]\n",
      "\n",
      "running train loss =   26850.19296875\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20732.126953[  100/  500]\n",
      "\n",
      "running train loss =   26764.153515625\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21783.367188[  100/  500]\n",
      "\n",
      "running train loss =   26678.58359375\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22691.302734[  100/  500]\n",
      "\n",
      "running train loss =   26597.150390625\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25691.966797[  100/  500]\n",
      "\n",
      "running train loss =   26513.99453125\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23817.050781[  100/  500]\n",
      "\n",
      "running train loss =   26426.89140625\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22412.539062[  100/  500]\n",
      "\n",
      "running train loss =   26345.199609375\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25658.681641[  100/  500]\n",
      "\n",
      "running train loss =   26262.466796875\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26423.136719[  100/  500]\n",
      "\n",
      "running train loss =   26181.881640625\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26494.105469[  100/  500]\n",
      "\n",
      "running train loss =   26102.658203125\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33500.136719[  100/  500]\n",
      "\n",
      "running train loss =   26024.3671875\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25797.224609[  100/  500]\n",
      "\n",
      "running train loss =   25935.4265625\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29970.580078[  100/  500]\n",
      "\n",
      "running train loss =   25855.739453125\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20990.904297[  100/  500]\n",
      "\n",
      "running train loss =   25770.116796875\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22336.679688[  100/  500]\n",
      "\n",
      "running train loss =   25692.81328125\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28119.900391[  100/  500]\n",
      "\n",
      "running train loss =   25613.86640625\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30955.783203[  100/  500]\n",
      "\n",
      "running train loss =   25533.793359375\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24811.740234[  100/  500]\n",
      "\n",
      "running train loss =   25452.073828125\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21834.505859[  100/  500]\n",
      "\n",
      "running train loss =   25370.19921875\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30011.492188[  100/  500]\n",
      "\n",
      "running train loss =   25299.910546875\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28512.689453[  100/  500]\n",
      "\n",
      "running train loss =   25215.9421875\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21718.039062[  100/  500]\n",
      "\n",
      "running train loss =   25137.77265625\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30857.082031[  100/  500]\n",
      "\n",
      "running train loss =   25059.9734375\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21266.312500[  100/  500]\n",
      "\n",
      "running train loss =   24975.853125\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24862.654297[  100/  500]\n",
      "\n",
      "running train loss =   24909.962109375\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26685.408203[  100/  500]\n",
      "\n",
      "running train loss =   24824.190234375\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22682.457031[  100/  500]\n",
      "\n",
      "running train loss =   24747.6578125\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21012.367188[  100/  500]\n",
      "\n",
      "running train loss =   24670.667578125\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24955.134766[  100/  500]\n",
      "\n",
      "running train loss =   24591.473828125\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30605.544922[  100/  500]\n",
      "\n",
      "running train loss =   24514.9\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19945.775391[  100/  500]\n",
      "\n",
      "running train loss =   24438.72421875\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30032.599609[  100/  500]\n",
      "\n",
      "running train loss =   24367.1328125\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22517.730469[  100/  500]\n",
      "\n",
      "running train loss =   24289.9609375\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23737.226562[  100/  500]\n",
      "\n",
      "running train loss =   24216.359375\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30087.648438[  100/  500]\n",
      "\n",
      "running train loss =   24139.300390625\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23494.261719[  100/  500]\n",
      "\n",
      "running train loss =   24061.0171875\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23185.619141[  100/  500]\n",
      "\n",
      "running train loss =   23987.429296875\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28650.435547[  100/  500]\n",
      "\n",
      "running train loss =   23913.5220703125\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21873.292969[  100/  500]\n",
      "\n",
      "running train loss =   23838.2546875\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22754.283203[  100/  500]\n",
      "\n",
      "running train loss =   23764.04296875\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26003.025391[  100/  500]\n",
      "\n",
      "running train loss =   23692.278125\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23340.095703[  100/  500]\n",
      "\n",
      "running train loss =   23615.565234375\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24472.490234[  100/  500]\n",
      "\n",
      "running train loss =   23543.691796875\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18578.685547[  100/  500]\n",
      "\n",
      "running train loss =   23471.43203125\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21412.117188[  100/  500]\n",
      "\n",
      "running train loss =   23398.694921875\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22033.214844[  100/  500]\n",
      "\n",
      "running train loss =   23329.425\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24361.107422[  100/  500]\n",
      "\n",
      "running train loss =   23254.395703125\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18459.855469[  100/  500]\n",
      "\n",
      "running train loss =   23182.77109375\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20528.242188[  100/  500]\n",
      "\n",
      "running train loss =   23110.1390625\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30014.994141[  100/  500]\n",
      "\n",
      "running train loss =   23039.238671875\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25842.525391[  100/  500]\n",
      "\n",
      "running train loss =   22968.284765625\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26673.865234[  100/  500]\n",
      "\n",
      "running train loss =   22895.458984375\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19415.769531[  100/  500]\n",
      "\n",
      "running train loss =   22823.768359375\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17485.910156[  100/  500]\n",
      "\n",
      "running train loss =   22752.38125\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14744.830078[  100/  500]\n",
      "\n",
      "running train loss =   22682.062890625\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26273.921875[  100/  500]\n",
      "\n",
      "running train loss =   22616.9390625\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28090.585938[  100/  500]\n",
      "\n",
      "running train loss =   22552.753125\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26027.199219[  100/  500]\n",
      "\n",
      "running train loss =   22476.9109375\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29585.605469[  100/  500]\n",
      "\n",
      "running train loss =   22403.840625\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15659.178711[  100/  500]\n",
      "\n",
      "running train loss =   22338.5814453125\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19601.546875[  100/  500]\n",
      "\n",
      "running train loss =   22269.168359375\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18480.728516[  100/  500]\n",
      "\n",
      "running train loss =   22198.65703125\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25356.699219[  100/  500]\n",
      "\n",
      "running train loss =   22130.7990234375\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21865.000000[  100/  500]\n",
      "\n",
      "running train loss =   22063.2314453125\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18711.841797[  100/  500]\n",
      "\n",
      "running train loss =   22000.0328125\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20277.000000[  100/  500]\n",
      "\n",
      "running train loss =   21926.39765625\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19148.117188[  100/  500]\n",
      "\n",
      "running train loss =   21856.9453125\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19518.294922[  100/  500]\n",
      "\n",
      "running train loss =   21791.579296875\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20206.855469[  100/  500]\n",
      "\n",
      "running train loss =   21723.728125\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21820.761719[  100/  500]\n",
      "\n",
      "running train loss =   21658.940625\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18397.615234[  100/  500]\n",
      "\n",
      "running train loss =   21591.344921875\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22935.960938[  100/  500]\n",
      "\n",
      "running train loss =   21525.52734375\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29704.287109[  100/  500]\n",
      "\n",
      "running train loss =   21461.987890625\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22728.880859[  100/  500]\n",
      "\n",
      "running train loss =   21393.003125\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20090.787109[  100/  500]\n",
      "\n",
      "running train loss =   21325.768359375\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19110.398438[  100/  500]\n",
      "\n",
      "running train loss =   21262.8203125\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17200.195312[  100/  500]\n",
      "\n",
      "running train loss =   21196.68359375\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20357.318359[  100/  500]\n",
      "\n",
      "running train loss =   21131.230859375\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26472.582031[  100/  500]\n",
      "\n",
      "running train loss =   21069.8515625\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16911.041016[  100/  500]\n",
      "\n",
      "running train loss =   21000.388671875\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24105.609375[  100/  500]\n",
      "\n",
      "running train loss =   20936.250390625\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24754.107422[  100/  500]\n",
      "\n",
      "running train loss =   20877.710546875\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19531.734375[  100/  500]\n",
      "\n",
      "running train loss =   20807.6900390625\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20701.494141[  100/  500]\n",
      "\n",
      "running train loss =   20746.819140625\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20042.345703[  100/  500]\n",
      "\n",
      "running train loss =   20683.4671875\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24693.455078[  100/  500]\n",
      "\n",
      "running train loss =   20624.98359375\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18549.529297[  100/  500]\n",
      "\n",
      "running train loss =   20560.05078125\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24692.304688[  100/  500]\n",
      "\n",
      "running train loss =   20495.3572265625\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19188.828125[  100/  500]\n",
      "\n",
      "running train loss =   20430.598828125\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23038.636719[  100/  500]\n",
      "\n",
      "running train loss =   20368.80625\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17176.542969[  100/  500]\n",
      "\n",
      "running train loss =   20304.584765625\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19781.500000[  100/  500]\n",
      "\n",
      "running train loss =   20243.801953125\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19640.742188[  100/  500]\n",
      "\n",
      "running train loss =   20184.104296875\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19672.578125[  100/  500]\n",
      "\n",
      "running train loss =   20121.223046875\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20798.029297[  100/  500]\n",
      "\n",
      "running train loss =   20060.709765625\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20713.564453[  100/  500]\n",
      "\n",
      "running train loss =   19999.960546875\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20159.289062[  100/  500]\n",
      "\n",
      "running train loss =   19939.6453125\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16748.804688[  100/  500]\n",
      "\n",
      "running train loss =   19876.4115234375\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18178.000000[  100/  500]\n",
      "\n",
      "running train loss =   19814.79921875\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16913.349609[  100/  500]\n",
      "\n",
      "running train loss =   19759.430078125\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16778.921875[  100/  500]\n",
      "\n",
      "running train loss =   19699.69375\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21228.474609[  100/  500]\n",
      "\n",
      "running train loss =   19642.4708984375\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21390.181641[  100/  500]\n",
      "\n",
      "running train loss =   19578.8654296875\n",
      "\n",
      "Done!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2506379.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   2433205.55\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2450760.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2429566.625\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2499742.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2425662.3\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2603197.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2421273.7\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2401919.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2416266.95\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2326849.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2410599.95\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2432267.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2404190.475\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2450034.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2396993.7\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2395213.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2388925.625\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2331475.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2379984.55\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2457923.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2370153.175\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2393285.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2359449.375\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2230892.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   2347845.05\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2399005.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2335385.75\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2361370.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2322064.725\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2245378.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   2307840.1\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2253561.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2292744.725\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2237618.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2276772.375\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2131914.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   2259949.8\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2228073.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2242299.7375\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2181443.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2223807.375\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2236949.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   2204444.6125\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2053861.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   2184260.725\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2172300.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   2163337.15\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2155686.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2141611.5125\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2125936.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   2119121.1\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2071642.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   2095959.4625\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1996662.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   2072060.9\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1986603.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   2047497.7375\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1938919.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   2022330.075\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2055471.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   1996567.0875\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2001783.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   1970179.1625\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1867404.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   1943222.8875\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1833072.625000[  100/ 1000]\n",
      "\n",
      "running train loss =   1915768.6875\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1923899.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1887837.6625\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1883971.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   1859390.95\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1834740.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   1830474.8\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1827363.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   1801141.8\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1889569.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   1771499.8875\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1667228.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1741386.1375\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1729154.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   1711029.325\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1761510.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   1680298.125\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1579125.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   1649312.4625\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1699528.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   1618169.175\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1540364.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   1586643.675\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1609521.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   1555059.375\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1570653.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   1523358.4\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1540119.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1491455.325\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1381683.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1459453.8125\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1428991.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   1427535.6375\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1403446.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   1395532.9\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1354267.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1363529.9375\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1316012.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   1331613.7625\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1329938.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   1299747.4875\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1277940.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   1268087.1875\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1310955.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   1236505.2875\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1248194.625000[  100/ 1000]\n",
      "\n",
      "running train loss =   1205123.0\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1267555.625000[  100/ 1000]\n",
      "\n",
      "running train loss =   1173939.225\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1211365.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   1143076.6625\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1165763.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1112281.35\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1087448.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1081917.6625\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1028000.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1051874.79375\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1078068.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   1022076.79375\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1059561.625000[  100/ 1000]\n",
      "\n",
      "running train loss =   992696.56875\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1008556.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   963679.19375\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 1019118.187500[  100/ 1000]\n",
      "\n",
      "running train loss =   935018.13125\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 889184.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   906889.21875\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 880805.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   879145.24375\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 925513.625000[  100/ 1000]\n",
      "\n",
      "running train loss =   851812.75625\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 851423.062500[  100/ 1000]\n",
      "\n",
      "running train loss =   825025.975\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 888353.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   798700.2625\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 660038.250000[  100/ 1000]\n",
      "\n",
      "running train loss =   772780.46875\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 779837.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   747532.99375\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 673974.062500[  100/ 1000]\n",
      "\n",
      "running train loss =   722839.90625\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 717946.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   698600.04375\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 702914.187500[  100/ 1000]\n",
      "\n",
      "running train loss =   674888.975\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 627313.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   651697.15625\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 612700.062500[  100/ 1000]\n",
      "\n",
      "running train loss =   629195.24375\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 626868.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   607197.79375\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 585172.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   585816.73125\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 627944.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   564994.125\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 497555.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   544670.35625\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 608297.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   525085.665625\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 537323.187500[  100/ 1000]\n",
      "\n",
      "running train loss =   505953.215625\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 440161.093750[  100/ 1000]\n",
      "\n",
      "running train loss =   487480.503125\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 451889.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   469655.05\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 507074.281250[  100/ 1000]\n",
      "\n",
      "running train loss =   452282.0\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 395730.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   435537.50625\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 419240.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   419529.303125\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 397100.843750[  100/ 1000]\n",
      "\n",
      "running train loss =   403845.36875\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 365649.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   389017.89375\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 301059.031250[  100/ 1000]\n",
      "\n",
      "running train loss =   374567.41875\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 356573.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   360802.60625\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 346820.906250[  100/ 1000]\n",
      "\n",
      "running train loss =   347467.35\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 378008.718750[  100/ 1000]\n",
      "\n",
      "running train loss =   334764.503125\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 332999.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   322705.76875\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 297774.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   311008.6484375\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 325960.093750[  100/ 1000]\n",
      "\n",
      "running train loss =   299859.4375\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 319049.125000[  100/ 1000]\n",
      "\n",
      "running train loss =   289399.628125\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 276838.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   279137.928125\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 229210.515625[  100/ 1000]\n",
      "\n",
      "running train loss =   269454.959375\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 243076.312500[  100/ 1000]\n",
      "\n",
      "running train loss =   260456.8203125\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 197944.718750[  100/ 1000]\n",
      "\n",
      "running train loss =   251523.4984375\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 259911.343750[  100/ 1000]\n",
      "\n",
      "running train loss =   243527.2296875\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 235799.421875[  100/ 1000]\n",
      "\n",
      "running train loss =   235647.3\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 253880.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   228262.925\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 261450.140625[  100/ 1000]\n",
      "\n",
      "running train loss =   221186.5515625\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 208039.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   214553.5328125\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 200037.640625[  100/ 1000]\n",
      "\n",
      "running train loss =   208394.2828125\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 229613.296875[  100/ 1000]\n",
      "\n",
      "running train loss =   202349.5046875\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 207485.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   196834.2765625\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 207280.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   191720.2453125\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 211946.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   186773.415625\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 192376.984375[  100/ 1000]\n",
      "\n",
      "running train loss =   182176.9546875\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154452.828125[  100/ 1000]\n",
      "\n",
      "running train loss =   177761.9828125\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 166891.296875[  100/ 1000]\n",
      "\n",
      "running train loss =   173794.5171875\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 218851.062500[  100/ 1000]\n",
      "\n",
      "running train loss =   169980.778125\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 171926.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   166444.934375\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 166678.312500[  100/ 1000]\n",
      "\n",
      "running train loss =   163090.73828125\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148154.281250[  100/ 1000]\n",
      "\n",
      "running train loss =   160073.3828125\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 164377.406250[  100/ 1000]\n",
      "\n",
      "running train loss =   157131.73359375\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 173803.859375[  100/ 1000]\n",
      "\n",
      "running train loss =   154463.25625\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 149277.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   151902.37109375\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154336.609375[  100/ 1000]\n",
      "\n",
      "running train loss =   149619.8953125\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 143301.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   147459.78046875\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 132666.843750[  100/ 1000]\n",
      "\n",
      "running train loss =   145418.0859375\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 160340.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   143507.4578125\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 157501.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   141788.8609375\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106911.507812[  100/ 1000]\n",
      "\n",
      "running train loss =   140148.06015625\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 113261.156250[  100/ 1000]\n",
      "\n",
      "running train loss =   138646.58203125\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 119841.421875[  100/ 1000]\n",
      "\n",
      "running train loss =   137233.84921875\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121959.007812[  100/ 1000]\n",
      "\n",
      "running train loss =   135929.434375\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 154115.015625[  100/ 1000]\n",
      "\n",
      "running train loss =   134725.17890625\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147094.015625[  100/ 1000]\n",
      "\n",
      "running train loss =   133609.91640625\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 131148.921875[  100/ 1000]\n",
      "\n",
      "running train loss =   132534.99375\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106937.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   131559.484375\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 161922.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   130654.08828125\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134271.093750[  100/ 1000]\n",
      "\n",
      "running train loss =   129837.59140625\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 141068.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   129021.3546875\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 151041.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   128246.89765625\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 138484.859375[  100/ 1000]\n",
      "\n",
      "running train loss =   127573.2734375\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 115068.343750[  100/ 1000]\n",
      "\n",
      "running train loss =   126927.20703125\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124739.953125[  100/ 1000]\n",
      "\n",
      "running train loss =   126273.384375\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148274.046875[  100/ 1000]\n",
      "\n",
      "running train loss =   125729.95546875\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122132.648438[  100/ 1000]\n",
      "\n",
      "running train loss =   125174.740625\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123139.273438[  100/ 1000]\n",
      "\n",
      "running train loss =   124656.95625\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122588.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   124154.74609375\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112870.468750[  100/ 1000]\n",
      "\n",
      "running train loss =   123673.41953125\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104469.742188[  100/ 1000]\n",
      "\n",
      "running train loss =   123220.35859375\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 118164.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   122801.5453125\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114422.789062[  100/ 1000]\n",
      "\n",
      "running train loss =   122394.1203125\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 146829.265625[  100/ 1000]\n",
      "\n",
      "running train loss =   121974.0015625\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140466.859375[  100/ 1000]\n",
      "\n",
      "running train loss =   121576.2625\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134557.093750[  100/ 1000]\n",
      "\n",
      "running train loss =   121205.95234375\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101342.882812[  100/ 1000]\n",
      "\n",
      "running train loss =   120858.79140625\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 144222.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   120484.6375\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 130223.906250[  100/ 1000]\n",
      "\n",
      "running train loss =   120131.46328125\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126912.703125[  100/ 1000]\n",
      "\n",
      "running train loss =   119779.21171875\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120805.453125[  100/ 1000]\n",
      "\n",
      "running train loss =   119447.47109375\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 140520.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   119106.571875\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94394.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   118772.15078125\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129886.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   118438.2890625\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 114939.906250[  100/ 1000]\n",
      "\n",
      "running train loss =   118115.88515625\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 129684.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   117772.40546875\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 120167.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   117454.3703125\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112048.921875[  100/ 1000]\n",
      "\n",
      "running train loss =   117122.34609375\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 152625.578125[  100/ 1000]\n",
      "\n",
      "running train loss =   116795.87421875\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121537.890625[  100/ 1000]\n",
      "\n",
      "running train loss =   116461.5828125\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 93643.351562[  100/ 1000]\n",
      "\n",
      "running train loss =   116126.29921875\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 128514.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   115799.27890625\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 147821.953125[  100/ 1000]\n",
      "\n",
      "running train loss =   115459.328125\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96536.953125[  100/ 1000]\n",
      "\n",
      "running train loss =   115128.415625\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 135750.671875[  100/ 1000]\n",
      "\n",
      "running train loss =   114784.06953125\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 139459.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   114432.73125\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 100298.468750[  100/ 1000]\n",
      "\n",
      "running train loss =   114099.575\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98778.226562[  100/ 1000]\n",
      "\n",
      "running train loss =   113744.3390625\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 117250.679688[  100/ 1000]\n",
      "\n",
      "running train loss =   113393.928125\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122498.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   113041.78984375\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124878.242188[  100/ 1000]\n",
      "\n",
      "running train loss =   112687.28515625\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123604.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   112319.1015625\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 134247.406250[  100/ 1000]\n",
      "\n",
      "running train loss =   111965.27890625\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104113.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   111587.52421875\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 148521.843750[  100/ 1000]\n",
      "\n",
      "running train loss =   111229.10390625\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 145645.187500[  100/ 1000]\n",
      "\n",
      "running train loss =   110853.76640625\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 106540.078125[  100/ 1000]\n",
      "\n",
      "running train loss =   110456.66328125\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126013.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   110100.8421875\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104311.148438[  100/ 1000]\n",
      "\n",
      "running train loss =   109695.74609375\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81776.117188[  100/ 1000]\n",
      "\n",
      "running train loss =   109313.5640625\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 108883.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   108928.03203125\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97343.867188[  100/ 1000]\n",
      "\n",
      "running train loss =   108534.5875\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 126873.078125[  100/ 1000]\n",
      "\n",
      "running train loss =   108127.759375\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111731.062500[  100/ 1000]\n",
      "\n",
      "running train loss =   107721.61015625\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97948.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   107335.7046875\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88287.796875[  100/ 1000]\n",
      "\n",
      "running train loss =   106903.7015625\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 116440.406250[  100/ 1000]\n",
      "\n",
      "running train loss =   106497.74375\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 121177.312500[  100/ 1000]\n",
      "\n",
      "running train loss =   106089.20703125\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99954.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   105669.02890625\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 132851.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   105247.025390625\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96461.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   104821.25234375\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 123631.359375[  100/ 1000]\n",
      "\n",
      "running train loss =   104434.7625\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87523.406250[  100/ 1000]\n",
      "\n",
      "running train loss =   103973.09375\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 105920.890625[  100/ 1000]\n",
      "\n",
      "running train loss =   103551.95078125\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79164.023438[  100/ 1000]\n",
      "\n",
      "running train loss =   103114.91953125\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111091.718750[  100/ 1000]\n",
      "\n",
      "running train loss =   102682.06328125\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101330.539062[  100/ 1000]\n",
      "\n",
      "running train loss =   102246.075\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87393.257812[  100/ 1000]\n",
      "\n",
      "running train loss =   101794.99296875\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69945.703125[  100/ 1000]\n",
      "\n",
      "running train loss =   101363.37578125\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 124108.320312[  100/ 1000]\n",
      "\n",
      "running train loss =   100922.87578125\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99375.203125[  100/ 1000]\n",
      "\n",
      "running train loss =   100468.94609375\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98306.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   100018.3\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96538.523438[  100/ 1000]\n",
      "\n",
      "running train loss =   99558.8359375\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 92353.492188[  100/ 1000]\n",
      "\n",
      "running train loss =   99107.834375\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122179.421875[  100/ 1000]\n",
      "\n",
      "running train loss =   98653.85234375\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 94072.710938[  100/ 1000]\n",
      "\n",
      "running train loss =   98192.85390625\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 104311.578125[  100/ 1000]\n",
      "\n",
      "running train loss =   97741.05234375\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 86969.843750[  100/ 1000]\n",
      "\n",
      "running train loss =   97280.27421875\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80919.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   96817.88984375\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 91327.429688[  100/ 1000]\n",
      "\n",
      "running train loss =   96353.09296875\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89044.781250[  100/ 1000]\n",
      "\n",
      "running train loss =   95887.85859375\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 112195.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   95417.178125\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 101502.242188[  100/ 1000]\n",
      "\n",
      "running train loss =   94954.3203125\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 90548.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   94472.66171875\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89225.117188[  100/ 1000]\n",
      "\n",
      "running train loss =   94004.98515625\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 122321.242188[  100/ 1000]\n",
      "\n",
      "running train loss =   93525.71953125\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 111613.562500[  100/ 1000]\n",
      "\n",
      "running train loss =   93065.00234375\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89711.617188[  100/ 1000]\n",
      "\n",
      "running train loss =   92573.7234375\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76769.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   92119.61953125\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88987.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   91632.0421875\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97483.460938[  100/ 1000]\n",
      "\n",
      "running train loss =   91165.790625\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 78555.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   90680.68125\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 98935.929688[  100/ 1000]\n",
      "\n",
      "running train loss =   90215.78515625\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87678.703125[  100/ 1000]\n",
      "\n",
      "running train loss =   89738.5640625\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 97786.617188[  100/ 1000]\n",
      "\n",
      "running train loss =   89249.06171875\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 99637.179688[  100/ 1000]\n",
      "\n",
      "running train loss =   88787.6078125\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84014.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   88300.59296875\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79613.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   87822.9453125\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89756.453125[  100/ 1000]\n",
      "\n",
      "running train loss =   87352.80703125\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65035.539062[  100/ 1000]\n",
      "\n",
      "running train loss =   86872.6359375\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 77994.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   86388.7\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69040.570312[  100/ 1000]\n",
      "\n",
      "running train loss =   85911.90390625\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84367.570312[  100/ 1000]\n",
      "\n",
      "running train loss =   85450.57734375\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80817.828125[  100/ 1000]\n",
      "\n",
      "running train loss =   84960.682421875\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74032.242188[  100/ 1000]\n",
      "\n",
      "running train loss =   84481.404296875\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85352.492188[  100/ 1000]\n",
      "\n",
      "running train loss =   84012.19921875\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 69857.382812[  100/ 1000]\n",
      "\n",
      "running train loss =   83536.37734375\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 103423.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   83067.634375\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 87353.281250[  100/ 1000]\n",
      "\n",
      "running train loss =   82578.778515625\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 84889.320312[  100/ 1000]\n",
      "\n",
      "running train loss =   82106.175390625\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63247.089844[  100/ 1000]\n",
      "\n",
      "running train loss =   81626.116796875\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75028.906250[  100/ 1000]\n",
      "\n",
      "running train loss =   81161.58125\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70058.640625[  100/ 1000]\n",
      "\n",
      "running train loss =   80686.67734375\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68244.468750[  100/ 1000]\n",
      "\n",
      "running train loss =   80218.492578125\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81743.718750[  100/ 1000]\n",
      "\n",
      "running train loss =   79758.0421875\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 96844.492188[  100/ 1000]\n",
      "\n",
      "running train loss =   79271.78046875\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 89859.773438[  100/ 1000]\n",
      "\n",
      "running train loss =   78823.36875\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62778.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   78334.97109375\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 66909.562500[  100/ 1000]\n",
      "\n",
      "running train loss =   77860.342578125\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85293.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   77406.500390625\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85152.468750[  100/ 1000]\n",
      "\n",
      "running train loss =   76951.807421875\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68989.296875[  100/ 1000]\n",
      "\n",
      "running train loss =   76469.095703125\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46739.363281[  100/ 1000]\n",
      "\n",
      "running train loss =   76017.928515625\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 85823.601562[  100/ 1000]\n",
      "\n",
      "running train loss =   75552.075390625\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55393.964844[  100/ 1000]\n",
      "\n",
      "running train loss =   75076.070703125\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70025.968750[  100/ 1000]\n",
      "\n",
      "running train loss =   74636.1765625\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 88445.789062[  100/ 1000]\n",
      "\n",
      "running train loss =   74166.39375\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67550.796875[  100/ 1000]\n",
      "\n",
      "running train loss =   73716.1078125\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 53108.191406[  100/ 1000]\n",
      "\n",
      "running train loss =   73249.045703125\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74751.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   72793.738671875\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65344.613281[  100/ 1000]\n",
      "\n",
      "running train loss =   72328.403125\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55011.679688[  100/ 1000]\n",
      "\n",
      "running train loss =   71900.862890625\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 63815.750000[  100/ 1000]\n",
      "\n",
      "running train loss =   71441.731640625\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 81923.039062[  100/ 1000]\n",
      "\n",
      "running train loss =   70990.248828125\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 80197.343750[  100/ 1000]\n",
      "\n",
      "running train loss =   70531.08671875\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 62992.031250[  100/ 1000]\n",
      "\n",
      "running train loss =   70095.640625\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 75669.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   69645.9109375\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74723.718750[  100/ 1000]\n",
      "\n",
      "running train loss =   69194.844921875\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 74767.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   68760.58515625\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 79490.968750[  100/ 1000]\n",
      "\n",
      "running train loss =   68317.653125\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47526.851562[  100/ 1000]\n",
      "\n",
      "running train loss =   67865.152734375\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68648.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   67442.562109375\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 47022.570312[  100/ 1000]\n",
      "\n",
      "running train loss =   66996.631640625\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58929.800781[  100/ 1000]\n",
      "\n",
      "running train loss =   66569.65546875\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52296.156250[  100/ 1000]\n",
      "\n",
      "running train loss =   66132.9546875\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60594.089844[  100/ 1000]\n",
      "\n",
      "running train loss =   65702.94296875\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70548.382812[  100/ 1000]\n",
      "\n",
      "running train loss =   65271.675390625\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65944.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   64838.096875\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 70008.781250[  100/ 1000]\n",
      "\n",
      "running train loss =   64418.052734375\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 67524.820312[  100/ 1000]\n",
      "\n",
      "running train loss =   63985.051953125\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68408.031250[  100/ 1000]\n",
      "\n",
      "running train loss =   63578.355859375\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 65507.824219[  100/ 1000]\n",
      "\n",
      "running train loss =   63150.904296875\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 68894.882812[  100/ 1000]\n",
      "\n",
      "running train loss =   62730.473828125\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52490.058594[  100/ 1000]\n",
      "\n",
      "running train loss =   62312.348828125\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60382.445312[  100/ 1000]\n",
      "\n",
      "running train loss =   61889.4671875\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58409.039062[  100/ 1000]\n",
      "\n",
      "running train loss =   61480.103515625\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56026.039062[  100/ 1000]\n",
      "\n",
      "running train loss =   61082.465234375\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49230.054688[  100/ 1000]\n",
      "\n",
      "running train loss =   60660.066796875\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 76318.203125[  100/ 1000]\n",
      "\n",
      "running train loss =   60251.978125\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49596.246094[  100/ 1000]\n",
      "\n",
      "running train loss =   59838.364453125\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 61413.109375[  100/ 1000]\n",
      "\n",
      "running train loss =   59460.496484375\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 58521.316406[  100/ 1000]\n",
      "\n",
      "running train loss =   59045.459765625\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 72430.523438[  100/ 1000]\n",
      "\n",
      "running train loss =   58640.9578125\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 64693.988281[  100/ 1000]\n",
      "\n",
      "running train loss =   58246.06640625\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54166.488281[  100/ 1000]\n",
      "\n",
      "running train loss =   57855.205078125\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 55554.609375[  100/ 1000]\n",
      "\n",
      "running train loss =   57460.925\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42221.199219[  100/ 1000]\n",
      "\n",
      "running train loss =   57066.2015625\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42948.761719[  100/ 1000]\n",
      "\n",
      "running train loss =   56676.43359375\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 50643.488281[  100/ 1000]\n",
      "\n",
      "running train loss =   56282.42734375\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60326.015625[  100/ 1000]\n",
      "\n",
      "running train loss =   55913.057421875\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57189.851562[  100/ 1000]\n",
      "\n",
      "running train loss =   55509.879296875\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 64933.730469[  100/ 1000]\n",
      "\n",
      "running train loss =   55146.082421875\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52382.406250[  100/ 1000]\n",
      "\n",
      "running train loss =   54758.182421875\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56092.015625[  100/ 1000]\n",
      "\n",
      "running train loss =   54386.987109375\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38472.050781[  100/ 1000]\n",
      "\n",
      "running train loss =   54001.48515625\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 73947.796875[  100/ 1000]\n",
      "\n",
      "running train loss =   53644.673828125\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 57288.605469[  100/ 1000]\n",
      "\n",
      "running train loss =   53261.058984375\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 54627.535156[  100/ 1000]\n",
      "\n",
      "running train loss =   52888.54609375\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 53214.781250[  100/ 1000]\n",
      "\n",
      "running train loss =   52531.060546875\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 53329.890625[  100/ 1000]\n",
      "\n",
      "running train loss =   52164.540625\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41745.167969[  100/ 1000]\n",
      "\n",
      "running train loss =   51808.220703125\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35696.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   51441.395703125\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56263.199219[  100/ 1000]\n",
      "\n",
      "running train loss =   51091.205859375\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44561.011719[  100/ 1000]\n",
      "\n",
      "running train loss =   50731.21328125\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 60025.238281[  100/ 1000]\n",
      "\n",
      "running train loss =   50377.6921875\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44242.691406[  100/ 1000]\n",
      "\n",
      "running train loss =   50032.140625\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44907.351562[  100/ 1000]\n",
      "\n",
      "running train loss =   49685.309375\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39870.453125[  100/ 1000]\n",
      "\n",
      "running train loss =   49333.734375\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46679.050781[  100/ 1000]\n",
      "\n",
      "running train loss =   48994.4125\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41971.843750[  100/ 1000]\n",
      "\n",
      "running train loss =   48653.598046875\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35963.230469[  100/ 1000]\n",
      "\n",
      "running train loss =   48303.36328125\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 56892.945312[  100/ 1000]\n",
      "\n",
      "running train loss =   47971.11015625\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52170.328125[  100/ 1000]\n",
      "\n",
      "running train loss =   47633.232421875\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49094.960938[  100/ 1000]\n",
      "\n",
      "running train loss =   47296.948046875\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43696.351562[  100/ 1000]\n",
      "\n",
      "running train loss =   46972.103125\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34180.898438[  100/ 1000]\n",
      "\n",
      "running train loss =   46638.075\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46743.980469[  100/ 1000]\n",
      "\n",
      "running train loss =   46305.848828125\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40444.148438[  100/ 1000]\n",
      "\n",
      "running train loss =   45989.3921875\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48863.738281[  100/ 1000]\n",
      "\n",
      "running train loss =   45665.523046875\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34271.269531[  100/ 1000]\n",
      "\n",
      "running train loss =   45344.784765625\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36076.738281[  100/ 1000]\n",
      "\n",
      "running train loss =   45024.02265625\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40108.703125[  100/ 1000]\n",
      "\n",
      "running train loss =   44711.1205078125\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42333.656250[  100/ 1000]\n",
      "\n",
      "running train loss =   44398.57421875\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37083.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   44082.4875\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49371.398438[  100/ 1000]\n",
      "\n",
      "running train loss =   43767.2638671875\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 49538.894531[  100/ 1000]\n",
      "\n",
      "running train loss =   43473.9123046875\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42044.445312[  100/ 1000]\n",
      "\n",
      "running train loss =   43167.96875\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36887.199219[  100/ 1000]\n",
      "\n",
      "running train loss =   42856.66171875\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40858.527344[  100/ 1000]\n",
      "\n",
      "running train loss =   42566.0583984375\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41059.484375[  100/ 1000]\n",
      "\n",
      "running train loss =   42268.455859375\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46642.066406[  100/ 1000]\n",
      "\n",
      "running train loss =   41965.871875\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 48510.636719[  100/ 1000]\n",
      "\n",
      "running train loss =   41667.973046875\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 40419.023438[  100/ 1000]\n",
      "\n",
      "running train loss =   41377.9958984375\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35531.214844[  100/ 1000]\n",
      "\n",
      "running train loss =   41087.113671875\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34299.039062[  100/ 1000]\n",
      "\n",
      "running train loss =   40796.034375\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 46178.214844[  100/ 1000]\n",
      "\n",
      "running train loss =   40508.2328125\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 36457.796875[  100/ 1000]\n",
      "\n",
      "running train loss =   40222.3328125\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 44228.300781[  100/ 1000]\n",
      "\n",
      "running train loss =   39946.792578125\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30784.802734[  100/ 1000]\n",
      "\n",
      "running train loss =   39666.1560546875\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31980.818359[  100/ 1000]\n",
      "\n",
      "running train loss =   39392.3330078125\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 45705.640625[  100/ 1000]\n",
      "\n",
      "running train loss =   39111.903515625\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31293.400391[  100/ 1000]\n",
      "\n",
      "running train loss =   38839.854296875\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 43032.558594[  100/ 1000]\n",
      "\n",
      "running train loss =   38569.782421875\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30545.132812[  100/ 1000]\n",
      "\n",
      "running train loss =   38295.223828125\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30765.189453[  100/ 1000]\n",
      "\n",
      "running train loss =   38031.2849609375\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34494.273438[  100/ 1000]\n",
      "\n",
      "running train loss =   37770.260546875\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37742.542969[  100/ 1000]\n",
      "\n",
      "running train loss =   37518.079296875\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32835.433594[  100/ 1000]\n",
      "\n",
      "running train loss =   37251.9787109375\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 41897.964844[  100/ 1000]\n",
      "\n",
      "running train loss =   36993.9607421875\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 52128.039062[  100/ 1000]\n",
      "\n",
      "running train loss =   36730.40078125\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 38356.730469[  100/ 1000]\n",
      "\n",
      "running train loss =   36473.16484375\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39163.531250[  100/ 1000]\n",
      "\n",
      "running train loss =   36223.0423828125\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31729.945312[  100/ 1000]\n",
      "\n",
      "running train loss =   35963.507421875\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34312.339844[  100/ 1000]\n",
      "\n",
      "running train loss =   35722.92734375\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34361.730469[  100/ 1000]\n",
      "\n",
      "running train loss =   35473.6427734375\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 45973.093750[  100/ 1000]\n",
      "\n",
      "running train loss =   35224.71328125\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31391.876953[  100/ 1000]\n",
      "\n",
      "running train loss =   34992.1712890625\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31647.097656[  100/ 1000]\n",
      "\n",
      "running train loss =   34745.63125\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29989.765625[  100/ 1000]\n",
      "\n",
      "running train loss =   34510.3556640625\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29364.304688[  100/ 1000]\n",
      "\n",
      "running train loss =   34270.9869140625\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 42090.355469[  100/ 1000]\n",
      "\n",
      "running train loss =   34032.8720703125\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 39288.054688[  100/ 1000]\n",
      "\n",
      "running train loss =   33812.91484375\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33848.441406[  100/ 1000]\n",
      "\n",
      "running train loss =   33568.76640625\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34481.789062[  100/ 1000]\n",
      "\n",
      "running train loss =   33337.14921875\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37367.671875[  100/ 1000]\n",
      "\n",
      "running train loss =   33114.392578125\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29997.359375[  100/ 1000]\n",
      "\n",
      "running train loss =   32886.7244140625\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27325.009766[  100/ 1000]\n",
      "\n",
      "running train loss =   32663.8072265625\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32730.828125[  100/ 1000]\n",
      "\n",
      "running train loss =   32440.0005859375\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32313.449219[  100/ 1000]\n",
      "\n",
      "running train loss =   32219.6033203125\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35275.281250[  100/ 1000]\n",
      "\n",
      "running train loss =   32002.62265625\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30044.783203[  100/ 1000]\n",
      "\n",
      "running train loss =   31785.084375\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33769.613281[  100/ 1000]\n",
      "\n",
      "running train loss =   31574.61875\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30242.630859[  100/ 1000]\n",
      "\n",
      "running train loss =   31353.6935546875\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 33048.414062[  100/ 1000]\n",
      "\n",
      "running train loss =   31146.0408203125\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 34658.714844[  100/ 1000]\n",
      "\n",
      "running train loss =   30951.0982421875\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30083.851562[  100/ 1000]\n",
      "\n",
      "running train loss =   30732.4451171875\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26826.810547[  100/ 1000]\n",
      "\n",
      "running train loss =   30521.878515625\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28996.421875[  100/ 1000]\n",
      "\n",
      "running train loss =   30311.1171875\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27431.802734[  100/ 1000]\n",
      "\n",
      "running train loss =   30114.1958984375\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27340.234375[  100/ 1000]\n",
      "\n",
      "running train loss =   29918.7353515625\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32256.384766[  100/ 1000]\n",
      "\n",
      "running train loss =   29710.7654296875\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32784.976562[  100/ 1000]\n",
      "\n",
      "running train loss =   29518.8552734375\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19931.546875[  100/ 1000]\n",
      "\n",
      "running train loss =   29314.3158203125\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 35987.328125[  100/ 1000]\n",
      "\n",
      "running train loss =   29126.496875\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26619.570312[  100/ 1000]\n",
      "\n",
      "running train loss =   28932.1060546875\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25082.181641[  100/ 1000]\n",
      "\n",
      "running train loss =   28737.9419921875\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32533.310547[  100/ 1000]\n",
      "\n",
      "running train loss =   28547.8697265625\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27648.982422[  100/ 1000]\n",
      "\n",
      "running train loss =   28353.151171875\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26837.152344[  100/ 1000]\n",
      "\n",
      "running train loss =   28172.8501953125\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23653.232422[  100/ 1000]\n",
      "\n",
      "running train loss =   27982.2984375\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 37735.136719[  100/ 1000]\n",
      "\n",
      "running train loss =   27801.4828125\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24610.117188[  100/ 1000]\n",
      "\n",
      "running train loss =   27616.4259765625\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 27439.509766[  100/ 1000]\n",
      "\n",
      "running train loss =   27435.5314453125\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26944.027344[  100/ 1000]\n",
      "\n",
      "running train loss =   27250.68359375\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25059.300781[  100/ 1000]\n",
      "\n",
      "running train loss =   27078.3771484375\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22605.587891[  100/ 1000]\n",
      "\n",
      "running train loss =   26895.7611328125\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28400.685547[  100/ 1000]\n",
      "\n",
      "running train loss =   26729.4109375\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23040.984375[  100/ 1000]\n",
      "\n",
      "running train loss =   26551.349609375\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24168.800781[  100/ 1000]\n",
      "\n",
      "running train loss =   26379.2330078125\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23670.759766[  100/ 1000]\n",
      "\n",
      "running train loss =   26207.51328125\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26247.855469[  100/ 1000]\n",
      "\n",
      "running train loss =   26044.7740234375\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 31008.238281[  100/ 1000]\n",
      "\n",
      "running train loss =   25868.03984375\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22331.650391[  100/ 1000]\n",
      "\n",
      "running train loss =   25692.54921875\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30104.742188[  100/ 1000]\n",
      "\n",
      "running train loss =   25525.68046875\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30959.865234[  100/ 1000]\n",
      "\n",
      "running train loss =   25366.83203125\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 32098.300781[  100/ 1000]\n",
      "\n",
      "running train loss =   25202.777734375\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 26661.998047[  100/ 1000]\n",
      "\n",
      "running train loss =   25038.213671875\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19036.052734[  100/ 1000]\n",
      "\n",
      "running train loss =   24874.3736328125\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25471.113281[  100/ 1000]\n",
      "\n",
      "running train loss =   24723.3052734375\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 29166.164062[  100/ 1000]\n",
      "\n",
      "running train loss =   24564.50390625\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18813.742188[  100/ 1000]\n",
      "\n",
      "running train loss =   24394.66015625\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24710.587891[  100/ 1000]\n",
      "\n",
      "running train loss =   24245.88046875\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25815.681641[  100/ 1000]\n",
      "\n",
      "running train loss =   24096.306640625\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24402.037109[  100/ 1000]\n",
      "\n",
      "running train loss =   23937.0380859375\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30854.425781[  100/ 1000]\n",
      "\n",
      "running train loss =   23785.6625\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 28448.160156[  100/ 1000]\n",
      "\n",
      "running train loss =   23631.4466796875\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 30135.494141[  100/ 1000]\n",
      "\n",
      "running train loss =   23486.61484375\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21835.613281[  100/ 1000]\n",
      "\n",
      "running train loss =   23336.2912109375\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24298.335938[  100/ 1000]\n",
      "\n",
      "running train loss =   23183.610546875\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18220.929688[  100/ 1000]\n",
      "\n",
      "running train loss =   23038.93935546875\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24911.500000[  100/ 1000]\n",
      "\n",
      "running train loss =   22890.4828125\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25512.181641[  100/ 1000]\n",
      "\n",
      "running train loss =   22751.67861328125\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17488.000000[  100/ 1000]\n",
      "\n",
      "running train loss =   22601.1517578125\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15876.855469[  100/ 1000]\n",
      "\n",
      "running train loss =   22465.0169921875\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20877.117188[  100/ 1000]\n",
      "\n",
      "running train loss =   22322.650390625\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15158.480469[  100/ 1000]\n",
      "\n",
      "running train loss =   22177.1724609375\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 24877.820312[  100/ 1000]\n",
      "\n",
      "running train loss =   22039.2041015625\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19650.792969[  100/ 1000]\n",
      "\n",
      "running train loss =   21908.633203125\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22068.607422[  100/ 1000]\n",
      "\n",
      "running train loss =   21763.991796875\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17009.248047[  100/ 1000]\n",
      "\n",
      "running train loss =   21635.5861328125\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20776.783203[  100/ 1000]\n",
      "\n",
      "running train loss =   21500.22470703125\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20132.460938[  100/ 1000]\n",
      "\n",
      "running train loss =   21359.4162109375\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 22284.482422[  100/ 1000]\n",
      "\n",
      "running train loss =   21226.3833984375\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21656.283203[  100/ 1000]\n",
      "\n",
      "running train loss =   21097.73759765625\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 25304.095703[  100/ 1000]\n",
      "\n",
      "running train loss =   20961.7341796875\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19471.841797[  100/ 1000]\n",
      "\n",
      "running train loss =   20831.41591796875\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18141.292969[  100/ 1000]\n",
      "\n",
      "running train loss =   20704.5443359375\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23967.476562[  100/ 1000]\n",
      "\n",
      "running train loss =   20583.52431640625\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20600.591797[  100/ 1000]\n",
      "\n",
      "running train loss =   20457.225\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14211.292969[  100/ 1000]\n",
      "\n",
      "running train loss =   20318.932421875\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17156.359375[  100/ 1000]\n",
      "\n",
      "running train loss =   20198.3525390625\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14647.820312[  100/ 1000]\n",
      "\n",
      "running train loss =   20074.8814453125\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15820.388672[  100/ 1000]\n",
      "\n",
      "running train loss =   19950.2775390625\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21029.794922[  100/ 1000]\n",
      "\n",
      "running train loss =   19832.11572265625\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17822.171875[  100/ 1000]\n",
      "\n",
      "running train loss =   19703.84921875\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20707.947266[  100/ 1000]\n",
      "\n",
      "running train loss =   19588.55849609375\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16107.166016[  100/ 1000]\n",
      "\n",
      "running train loss =   19467.8828125\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17089.804688[  100/ 1000]\n",
      "\n",
      "running train loss =   19354.1578125\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16113.070312[  100/ 1000]\n",
      "\n",
      "running train loss =   19227.347265625\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15377.004883[  100/ 1000]\n",
      "\n",
      "running train loss =   19112.78974609375\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16528.570312[  100/ 1000]\n",
      "\n",
      "running train loss =   18995.284375\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15731.159180[  100/ 1000]\n",
      "\n",
      "running train loss =   18879.17021484375\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13702.509766[  100/ 1000]\n",
      "\n",
      "running train loss =   18765.183203125\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14145.812500[  100/ 1000]\n",
      "\n",
      "running train loss =   18652.72685546875\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17206.988281[  100/ 1000]\n",
      "\n",
      "running train loss =   18539.03408203125\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16077.188477[  100/ 1000]\n",
      "\n",
      "running train loss =   18430.6265625\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21307.630859[  100/ 1000]\n",
      "\n",
      "running train loss =   18323.22138671875\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17845.507812[  100/ 1000]\n",
      "\n",
      "running train loss =   18214.53154296875\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18032.470703[  100/ 1000]\n",
      "\n",
      "running train loss =   18100.26083984375\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20009.087891[  100/ 1000]\n",
      "\n",
      "running train loss =   17989.4322265625\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16671.800781[  100/ 1000]\n",
      "\n",
      "running train loss =   17885.12275390625\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19134.363281[  100/ 1000]\n",
      "\n",
      "running train loss =   17775.57568359375\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16334.830078[  100/ 1000]\n",
      "\n",
      "running train loss =   17670.8208984375\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18155.890625[  100/ 1000]\n",
      "\n",
      "running train loss =   17566.8927734375\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21356.169922[  100/ 1000]\n",
      "\n",
      "running train loss =   17472.64921875\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17798.634766[  100/ 1000]\n",
      "\n",
      "running train loss =   17357.85478515625\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 21680.724609[  100/ 1000]\n",
      "\n",
      "running train loss =   17255.22216796875\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14499.914062[  100/ 1000]\n",
      "\n",
      "running train loss =   17161.3896484375\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19700.539062[  100/ 1000]\n",
      "\n",
      "running train loss =   17052.668359375\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15420.834961[  100/ 1000]\n",
      "\n",
      "running train loss =   16953.24951171875\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 23563.687500[  100/ 1000]\n",
      "\n",
      "running train loss =   16857.27578125\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19382.785156[  100/ 1000]\n",
      "\n",
      "running train loss =   16765.88125\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11260.124023[  100/ 1000]\n",
      "\n",
      "running train loss =   16658.64208984375\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18176.423828[  100/ 1000]\n",
      "\n",
      "running train loss =   16565.4388671875\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12281.187500[  100/ 1000]\n",
      "\n",
      "running train loss =   16465.441015625\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15117.102539[  100/ 1000]\n",
      "\n",
      "running train loss =   16384.59970703125\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13369.085938[  100/ 1000]\n",
      "\n",
      "running train loss =   16275.71220703125\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17071.951172[  100/ 1000]\n",
      "\n",
      "running train loss =   16184.01650390625\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18493.535156[  100/ 1000]\n",
      "\n",
      "running train loss =   16096.1337890625\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15533.014648[  100/ 1000]\n",
      "\n",
      "running train loss =   15998.2888671875\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20241.726562[  100/ 1000]\n",
      "\n",
      "running train loss =   15911.483203125\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17080.191406[  100/ 1000]\n",
      "\n",
      "running train loss =   15816.02939453125\n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15149.375000[  100/ 1000]\n",
      "\n",
      "running train loss =   15726.926953125\n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 20525.214844[  100/ 1000]\n",
      "\n",
      "running train loss =   15644.5091796875\n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15327.842773[  100/ 1000]\n",
      "\n",
      "running train loss =   15553.39462890625\n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17273.640625[  100/ 1000]\n",
      "\n",
      "running train loss =   15464.2263671875\n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17337.052734[  100/ 1000]\n",
      "\n",
      "running train loss =   15377.00166015625\n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 18025.599609[  100/ 1000]\n",
      "\n",
      "running train loss =   15292.03466796875\n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16673.572266[  100/ 1000]\n",
      "\n",
      "running train loss =   15207.3765625\n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10077.897461[  100/ 1000]\n",
      "\n",
      "running train loss =   15123.82958984375\n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12472.459961[  100/ 1000]\n",
      "\n",
      "running train loss =   15043.06064453125\n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11254.699219[  100/ 1000]\n",
      "\n",
      "running train loss =   14955.81240234375\n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16519.574219[  100/ 1000]\n",
      "\n",
      "running train loss =   14874.9390625\n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9500.272461[  100/ 1000]\n",
      "\n",
      "running train loss =   14791.02138671875\n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12790.302734[  100/ 1000]\n",
      "\n",
      "running train loss =   14715.65859375\n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15919.335938[  100/ 1000]\n",
      "\n",
      "running train loss =   14633.6212890625\n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11471.525391[  100/ 1000]\n",
      "\n",
      "running train loss =   14552.3380859375\n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7515.383301[  100/ 1000]\n",
      "\n",
      "running train loss =   14476.387939453125\n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11472.258789[  100/ 1000]\n",
      "\n",
      "running train loss =   14399.3375\n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14406.428711[  100/ 1000]\n",
      "\n",
      "running train loss =   14321.954296875\n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17100.953125[  100/ 1000]\n",
      "\n",
      "running train loss =   14241.39140625\n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11051.785156[  100/ 1000]\n",
      "\n",
      "running train loss =   14163.84853515625\n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14782.717773[  100/ 1000]\n",
      "\n",
      "running train loss =   14093.06630859375\n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7867.363281[  100/ 1000]\n",
      "\n",
      "running train loss =   14014.273828125\n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14017.255859[  100/ 1000]\n",
      "\n",
      "running train loss =   13946.1392578125\n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12709.838867[  100/ 1000]\n",
      "\n",
      "running train loss =   13869.2732421875\n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14651.347656[  100/ 1000]\n",
      "\n",
      "running train loss =   13795.6546875\n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13381.258789[  100/ 1000]\n",
      "\n",
      "running train loss =   13728.06279296875\n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11097.280273[  100/ 1000]\n",
      "\n",
      "running train loss =   13658.0634765625\n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11035.200195[  100/ 1000]\n",
      "\n",
      "running train loss =   13587.3796875\n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12536.165039[  100/ 1000]\n",
      "\n",
      "running train loss =   13514.522216796875\n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13975.925781[  100/ 1000]\n",
      "\n",
      "running train loss =   13445.97275390625\n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15577.666016[  100/ 1000]\n",
      "\n",
      "running train loss =   13374.341015625\n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16502.005859[  100/ 1000]\n",
      "\n",
      "running train loss =   13306.46962890625\n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12259.539062[  100/ 1000]\n",
      "\n",
      "running train loss =   13246.98173828125\n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15436.787109[  100/ 1000]\n",
      "\n",
      "running train loss =   13170.34365234375\n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14194.893555[  100/ 1000]\n",
      "\n",
      "running train loss =   13103.99833984375\n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13985.072266[  100/ 1000]\n",
      "\n",
      "running train loss =   13041.527734375\n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17536.785156[  100/ 1000]\n",
      "\n",
      "running train loss =   12976.42841796875\n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12878.356445[  100/ 1000]\n",
      "\n",
      "running train loss =   12918.12060546875\n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8945.747070[  100/ 1000]\n",
      "\n",
      "running train loss =   12848.057373046875\n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12045.422852[  100/ 1000]\n",
      "\n",
      "running train loss =   12786.0025390625\n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9627.401367[  100/ 1000]\n",
      "\n",
      "running train loss =   12727.9705078125\n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9354.842773[  100/ 1000]\n",
      "\n",
      "running train loss =   12660.51298828125\n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13034.522461[  100/ 1000]\n",
      "\n",
      "running train loss =   12599.53369140625\n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13537.465820[  100/ 1000]\n",
      "\n",
      "running train loss =   12541.136474609375\n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14640.128906[  100/ 1000]\n",
      "\n",
      "running train loss =   12479.43408203125\n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 19087.664062[  100/ 1000]\n",
      "\n",
      "running train loss =   12415.9048828125\n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12206.610352[  100/ 1000]\n",
      "\n",
      "running train loss =   12358.18515625\n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16195.471680[  100/ 1000]\n",
      "\n",
      "running train loss =   12315.581201171875\n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9223.278320[  100/ 1000]\n",
      "\n",
      "running train loss =   12240.192236328125\n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10224.262695[  100/ 1000]\n",
      "\n",
      "running train loss =   12186.905859375\n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9787.571289[  100/ 1000]\n",
      "\n",
      "running train loss =   12136.813671875\n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11958.542969[  100/ 1000]\n",
      "\n",
      "running train loss =   12072.26455078125\n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10267.633789[  100/ 1000]\n",
      "\n",
      "running train loss =   12015.44736328125\n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12323.490234[  100/ 1000]\n",
      "\n",
      "running train loss =   11961.41142578125\n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10829.615234[  100/ 1000]\n",
      "\n",
      "running train loss =   11902.7470703125\n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11167.446289[  100/ 1000]\n",
      "\n",
      "running train loss =   11856.73984375\n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12250.752930[  100/ 1000]\n",
      "\n",
      "running train loss =   11800.69990234375\n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11883.110352[  100/ 1000]\n",
      "\n",
      "running train loss =   11739.99775390625\n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10933.937500[  100/ 1000]\n",
      "\n",
      "running train loss =   11692.165771484375\n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13151.646484[  100/ 1000]\n",
      "\n",
      "running train loss =   11639.8404296875\n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11233.875000[  100/ 1000]\n",
      "\n",
      "running train loss =   11590.07890625\n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12359.825195[  100/ 1000]\n",
      "\n",
      "running train loss =   11532.77216796875\n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10759.363281[  100/ 1000]\n",
      "\n",
      "running train loss =   11486.242578125\n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12379.016602[  100/ 1000]\n",
      "\n",
      "running train loss =   11434.713330078124\n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11621.590820[  100/ 1000]\n",
      "\n",
      "running train loss =   11382.0400390625\n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11600.717773[  100/ 1000]\n",
      "\n",
      "running train loss =   11334.77724609375\n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11834.457031[  100/ 1000]\n",
      "\n",
      "running train loss =   11288.17099609375\n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8349.342773[  100/ 1000]\n",
      "\n",
      "running train loss =   11239.363427734375\n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13750.333984[  100/ 1000]\n",
      "\n",
      "running train loss =   11195.906787109376\n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 17071.449219[  100/ 1000]\n",
      "\n",
      "running train loss =   11139.528515625\n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10138.356445[  100/ 1000]\n",
      "\n",
      "running train loss =   11094.444873046876\n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13143.160156[  100/ 1000]\n",
      "\n",
      "running train loss =   11048.8419921875\n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 16912.818359[  100/ 1000]\n",
      "\n",
      "running train loss =   11001.6279296875\n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10426.849609[  100/ 1000]\n",
      "\n",
      "running train loss =   10955.342919921875\n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10469.272461[  100/ 1000]\n",
      "\n",
      "running train loss =   10910.087841796874\n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14363.986328[  100/ 1000]\n",
      "\n",
      "running train loss =   10866.54912109375\n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10169.219727[  100/ 1000]\n",
      "\n",
      "running train loss =   10819.83359375\n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6871.028809[  100/ 1000]\n",
      "\n",
      "running train loss =   10779.716650390625\n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13311.931641[  100/ 1000]\n",
      "\n",
      "running train loss =   10731.58876953125\n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9506.003906[  100/ 1000]\n",
      "\n",
      "running train loss =   10699.0720703125\n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10628.240234[  100/ 1000]\n",
      "\n",
      "running train loss =   10645.514111328124\n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9250.757812[  100/ 1000]\n",
      "\n",
      "running train loss =   10604.4091796875\n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8597.056641[  100/ 1000]\n",
      "\n",
      "running train loss =   10564.92421875\n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12110.884766[  100/ 1000]\n",
      "\n",
      "running train loss =   10522.03740234375\n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14029.982422[  100/ 1000]\n",
      "\n",
      "running train loss =   10481.68525390625\n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9002.925781[  100/ 1000]\n",
      "\n",
      "running train loss =   10436.989453125\n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7447.259766[  100/ 1000]\n",
      "\n",
      "running train loss =   10400.040966796874\n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13261.289062[  100/ 1000]\n",
      "\n",
      "running train loss =   10367.600341796875\n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14119.335938[  100/ 1000]\n",
      "\n",
      "running train loss =   10321.943310546874\n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8279.667969[  100/ 1000]\n",
      "\n",
      "running train loss =   10280.628564453125\n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10452.674805[  100/ 1000]\n",
      "\n",
      "running train loss =   10241.439208984375\n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7643.742676[  100/ 1000]\n",
      "\n",
      "running train loss =   10201.18955078125\n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11633.105469[  100/ 1000]\n",
      "\n",
      "running train loss =   10161.694873046876\n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9833.901367[  100/ 1000]\n",
      "\n",
      "running train loss =   10132.3142578125\n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8944.136719[  100/ 1000]\n",
      "\n",
      "running train loss =   10087.87587890625\n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12391.849609[  100/ 1000]\n",
      "\n",
      "running train loss =   10049.736572265625\n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10574.973633[  100/ 1000]\n",
      "\n",
      "running train loss =   10012.62890625\n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14513.892578[  100/ 1000]\n",
      "\n",
      "running train loss =   9980.739111328125\n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10243.106445[  100/ 1000]\n",
      "\n",
      "running train loss =   9939.33720703125\n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12590.042969[  100/ 1000]\n",
      "\n",
      "running train loss =   9915.687646484375\n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10430.953125[  100/ 1000]\n",
      "\n",
      "running train loss =   9867.67890625\n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13008.932617[  100/ 1000]\n",
      "\n",
      "running train loss =   9834.5416015625\n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10072.318359[  100/ 1000]\n",
      "\n",
      "running train loss =   9806.743505859375\n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7843.555664[  100/ 1000]\n",
      "\n",
      "running train loss =   9763.72744140625\n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7740.567383[  100/ 1000]\n",
      "\n",
      "running train loss =   9728.273681640625\n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6585.865723[  100/ 1000]\n",
      "\n",
      "running train loss =   9697.93046875\n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13267.575195[  100/ 1000]\n",
      "\n",
      "running train loss =   9665.9470703125\n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8233.864258[  100/ 1000]\n",
      "\n",
      "running train loss =   9628.92314453125\n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13229.419922[  100/ 1000]\n",
      "\n",
      "running train loss =   9596.43583984375\n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11781.637695[  100/ 1000]\n",
      "\n",
      "running train loss =   9566.83017578125\n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9085.992188[  100/ 1000]\n",
      "\n",
      "running train loss =   9526.609814453124\n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 15742.792969[  100/ 1000]\n",
      "\n",
      "running train loss =   9494.10341796875\n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8572.311523[  100/ 1000]\n",
      "\n",
      "running train loss =   9484.671630859375\n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 14428.589844[  100/ 1000]\n",
      "\n",
      "running train loss =   9429.3080078125\n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9561.744141[  100/ 1000]\n",
      "\n",
      "running train loss =   9397.14404296875\n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8497.934570[  100/ 1000]\n",
      "\n",
      "running train loss =   9369.80751953125\n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11435.192383[  100/ 1000]\n",
      "\n",
      "running train loss =   9336.094775390626\n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10353.681641[  100/ 1000]\n",
      "\n",
      "running train loss =   9303.70302734375\n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8362.834961[  100/ 1000]\n",
      "\n",
      "running train loss =   9272.240771484376\n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10916.268555[  100/ 1000]\n",
      "\n",
      "running train loss =   9245.686474609374\n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9265.297852[  100/ 1000]\n",
      "\n",
      "running train loss =   9214.145849609375\n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11570.879883[  100/ 1000]\n",
      "\n",
      "running train loss =   9181.43154296875\n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6915.183105[  100/ 1000]\n",
      "\n",
      "running train loss =   9152.44970703125\n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7979.354492[  100/ 1000]\n",
      "\n",
      "running train loss =   9123.943896484376\n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11583.835938[  100/ 1000]\n",
      "\n",
      "running train loss =   9095.069140625\n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8467.052734[  100/ 1000]\n",
      "\n",
      "running train loss =   9070.69111328125\n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6977.446289[  100/ 1000]\n",
      "\n",
      "running train loss =   9036.799755859374\n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10497.784180[  100/ 1000]\n",
      "\n",
      "running train loss =   9003.694384765626\n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12793.834961[  100/ 1000]\n",
      "\n",
      "running train loss =   8980.295654296875\n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10360.071289[  100/ 1000]\n",
      "\n",
      "running train loss =   8951.2294921875\n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12980.291016[  100/ 1000]\n",
      "\n",
      "running train loss =   8924.2052734375\n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11497.731445[  100/ 1000]\n",
      "\n",
      "running train loss =   8893.442236328125\n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8896.933594[  100/ 1000]\n",
      "\n",
      "running train loss =   8865.64775390625\n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8189.299805[  100/ 1000]\n",
      "\n",
      "running train loss =   8837.86416015625\n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5633.017578[  100/ 1000]\n",
      "\n",
      "running train loss =   8811.613037109375\n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9050.162109[  100/ 1000]\n",
      "\n",
      "running train loss =   8780.466796875\n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8426.470703[  100/ 1000]\n",
      "\n",
      "running train loss =   8756.643994140624\n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9591.155273[  100/ 1000]\n",
      "\n",
      "running train loss =   8728.270654296875\n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7101.052734[  100/ 1000]\n",
      "\n",
      "running train loss =   8700.8068359375\n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11698.825195[  100/ 1000]\n",
      "\n",
      "running train loss =   8675.25087890625\n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 13918.205078[  100/ 1000]\n",
      "\n",
      "running train loss =   8648.645654296875\n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9718.636719[  100/ 1000]\n",
      "\n",
      "running train loss =   8621.83779296875\n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8926.167969[  100/ 1000]\n",
      "\n",
      "running train loss =   8600.564794921875\n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8191.701172[  100/ 1000]\n",
      "\n",
      "running train loss =   8574.115673828124\n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7162.317383[  100/ 1000]\n",
      "\n",
      "running train loss =   8556.615625\n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8170.586426[  100/ 1000]\n",
      "\n",
      "running train loss =   8520.962646484375\n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8829.981445[  100/ 1000]\n",
      "\n",
      "running train loss =   8507.045703125\n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5758.845215[  100/ 1000]\n",
      "\n",
      "running train loss =   8468.7115234375\n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10525.066406[  100/ 1000]\n",
      "\n",
      "running train loss =   8441.240380859375\n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11896.077148[  100/ 1000]\n",
      "\n",
      "running train loss =   8422.480029296876\n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6762.389160[  100/ 1000]\n",
      "\n",
      "running train loss =   8395.61982421875\n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7949.893066[  100/ 1000]\n",
      "\n",
      "running train loss =   8369.59638671875\n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8641.060547[  100/ 1000]\n",
      "\n",
      "running train loss =   8347.197900390625\n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10522.410156[  100/ 1000]\n",
      "\n",
      "running train loss =   8324.340185546875\n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8505.645508[  100/ 1000]\n",
      "\n",
      "running train loss =   8296.77919921875\n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7298.008789[  100/ 1000]\n",
      "\n",
      "running train loss =   8271.407373046875\n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6167.087891[  100/ 1000]\n",
      "\n",
      "running train loss =   8247.552465820312\n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7147.250488[  100/ 1000]\n",
      "\n",
      "running train loss =   8223.045751953125\n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5699.015137[  100/ 1000]\n",
      "\n",
      "running train loss =   8202.4705078125\n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7387.958008[  100/ 1000]\n",
      "\n",
      "running train loss =   8177.917578125\n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7541.865234[  100/ 1000]\n",
      "\n",
      "running train loss =   8151.70361328125\n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9697.197266[  100/ 1000]\n",
      "\n",
      "running train loss =   8131.99541015625\n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8654.666016[  100/ 1000]\n",
      "\n",
      "running train loss =   8108.838916015625\n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8151.051270[  100/ 1000]\n",
      "\n",
      "running train loss =   8084.632275390625\n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9364.364258[  100/ 1000]\n",
      "\n",
      "running train loss =   8070.715234375\n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6496.654297[  100/ 1000]\n",
      "\n",
      "running train loss =   8039.428515625\n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9083.503906[  100/ 1000]\n",
      "\n",
      "running train loss =   8014.509033203125\n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6247.346680[  100/ 1000]\n",
      "\n",
      "running train loss =   7994.37861328125\n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9978.429688[  100/ 1000]\n",
      "\n",
      "running train loss =   7972.736669921875\n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8314.482422[  100/ 1000]\n",
      "\n",
      "running train loss =   7953.01494140625\n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8616.933594[  100/ 1000]\n",
      "\n",
      "running train loss =   7928.45703125\n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10724.495117[  100/ 1000]\n",
      "\n",
      "running train loss =   7904.372509765625\n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8006.163574[  100/ 1000]\n",
      "\n",
      "running train loss =   7879.67548828125\n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6705.396484[  100/ 1000]\n",
      "\n",
      "running train loss =   7861.141357421875\n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6783.838867[  100/ 1000]\n",
      "\n",
      "running train loss =   7836.38994140625\n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9523.400391[  100/ 1000]\n",
      "\n",
      "running train loss =   7826.435498046875\n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7523.213867[  100/ 1000]\n",
      "\n",
      "running train loss =   7790.9671875\n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5749.950684[  100/ 1000]\n",
      "\n",
      "running train loss =   7771.437939453125\n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8054.773926[  100/ 1000]\n",
      "\n",
      "running train loss =   7749.694287109375\n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6433.220215[  100/ 1000]\n",
      "\n",
      "running train loss =   7742.0744140625\n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6420.377441[  100/ 1000]\n",
      "\n",
      "running train loss =   7720.21259765625\n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6298.470703[  100/ 1000]\n",
      "\n",
      "running train loss =   7689.23056640625\n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11809.191406[  100/ 1000]\n",
      "\n",
      "running train loss =   7669.952685546875\n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7659.413574[  100/ 1000]\n",
      "\n",
      "running train loss =   7650.410498046875\n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6652.439453[  100/ 1000]\n",
      "\n",
      "running train loss =   7623.993994140625\n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7433.275391[  100/ 1000]\n",
      "\n",
      "running train loss =   7607.606201171875\n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6889.680176[  100/ 1000]\n",
      "\n",
      "running train loss =   7583.2667236328125\n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6919.658691[  100/ 1000]\n",
      "\n",
      "running train loss =   7559.168798828125\n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7809.284180[  100/ 1000]\n",
      "\n",
      "running train loss =   7542.602099609375\n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12581.402344[  100/ 1000]\n",
      "\n",
      "running train loss =   7520.55537109375\n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9153.028320[  100/ 1000]\n",
      "\n",
      "running train loss =   7498.437353515625\n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 11102.949219[  100/ 1000]\n",
      "\n",
      "running train loss =   7480.5201171875\n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8272.917969[  100/ 1000]\n",
      "\n",
      "running train loss =   7458.69443359375\n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8546.124023[  100/ 1000]\n",
      "\n",
      "running train loss =   7439.682421875\n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4360.987305[  100/ 1000]\n",
      "\n",
      "running train loss =   7424.9188232421875\n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7535.494141[  100/ 1000]\n",
      "\n",
      "running train loss =   7402.719384765625\n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5289.417969[  100/ 1000]\n",
      "\n",
      "running train loss =   7378.221875\n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6249.732910[  100/ 1000]\n",
      "\n",
      "running train loss =   7359.713671875\n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5596.080078[  100/ 1000]\n",
      "\n",
      "running train loss =   7349.022802734375\n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7146.861328[  100/ 1000]\n",
      "\n",
      "running train loss =   7324.386083984375\n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7981.142578[  100/ 1000]\n",
      "\n",
      "running train loss =   7301.023681640625\n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7003.732910[  100/ 1000]\n",
      "\n",
      "running train loss =   7282.524072265625\n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7400.921875[  100/ 1000]\n",
      "\n",
      "running train loss =   7267.00439453125\n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6143.010742[  100/ 1000]\n",
      "\n",
      "running train loss =   7240.0919921875\n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4497.619629[  100/ 1000]\n",
      "\n",
      "running train loss =   7220.914990234375\n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6896.841309[  100/ 1000]\n",
      "\n",
      "running train loss =   7202.776904296875\n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5611.822266[  100/ 1000]\n",
      "\n",
      "running train loss =   7184.047998046875\n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6073.199219[  100/ 1000]\n",
      "\n",
      "running train loss =   7169.192138671875\n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9106.258789[  100/ 1000]\n",
      "\n",
      "running train loss =   7145.1140625\n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4507.696777[  100/ 1000]\n",
      "\n",
      "running train loss =   7128.989697265625\n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4742.774414[  100/ 1000]\n",
      "\n",
      "running train loss =   7107.69794921875\n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5852.868652[  100/ 1000]\n",
      "\n",
      "running train loss =   7087.357421875\n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5665.629883[  100/ 1000]\n",
      "\n",
      "running train loss =   7075.969775390625\n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7135.353027[  100/ 1000]\n",
      "\n",
      "running train loss =   7054.985009765625\n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 12025.912109[  100/ 1000]\n",
      "\n",
      "running train loss =   7033.440185546875\n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7131.208008[  100/ 1000]\n",
      "\n",
      "running train loss =   7017.2623046875\n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8764.090820[  100/ 1000]\n",
      "\n",
      "running train loss =   6994.09326171875\n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6077.938965[  100/ 1000]\n",
      "\n",
      "running train loss =   6979.247900390625\n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6008.767090[  100/ 1000]\n",
      "\n",
      "running train loss =   6960.741796875\n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 9615.710938[  100/ 1000]\n",
      "\n",
      "running train loss =   6942.3296875\n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6107.373535[  100/ 1000]\n",
      "\n",
      "running train loss =   6938.673681640625\n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6866.815430[  100/ 1000]\n",
      "\n",
      "running train loss =   6911.274169921875\n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8125.799316[  100/ 1000]\n",
      "\n",
      "running train loss =   6885.641625976563\n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6810.008789[  100/ 1000]\n",
      "\n",
      "running train loss =   6865.41669921875\n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5969.916992[  100/ 1000]\n",
      "\n",
      "running train loss =   6848.484375\n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6826.901367[  100/ 1000]\n",
      "\n",
      "running train loss =   6835.09560546875\n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6273.581055[  100/ 1000]\n",
      "\n",
      "running train loss =   6828.42705078125\n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7530.759766[  100/ 1000]\n",
      "\n",
      "running train loss =   6814.50830078125\n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7111.437500[  100/ 1000]\n",
      "\n",
      "running train loss =   6786.867919921875\n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5575.203125[  100/ 1000]\n",
      "\n",
      "running train loss =   6760.00283203125\n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7556.372559[  100/ 1000]\n",
      "\n",
      "running train loss =   6746.9525390625\n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6637.941895[  100/ 1000]\n",
      "\n",
      "running train loss =   6723.54189453125\n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7179.609375[  100/ 1000]\n",
      "\n",
      "running train loss =   6704.987548828125\n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5751.048340[  100/ 1000]\n",
      "\n",
      "running train loss =   6689.180078125\n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6072.013672[  100/ 1000]\n",
      "\n",
      "running train loss =   6673.577392578125\n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4773.500977[  100/ 1000]\n",
      "\n",
      "running train loss =   6651.3923828125\n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5317.780762[  100/ 1000]\n",
      "\n",
      "running train loss =   6637.256982421875\n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6361.015137[  100/ 1000]\n",
      "\n",
      "running train loss =   6627.41533203125\n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4751.938965[  100/ 1000]\n",
      "\n",
      "running train loss =   6605.07314453125\n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7578.973145[  100/ 1000]\n",
      "\n",
      "running train loss =   6591.635961914062\n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5781.715820[  100/ 1000]\n",
      "\n",
      "running train loss =   6580.521435546875\n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6685.060547[  100/ 1000]\n",
      "\n",
      "running train loss =   6551.416064453125\n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7537.713867[  100/ 1000]\n",
      "\n",
      "running train loss =   6549.55078125\n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5558.890625[  100/ 1000]\n",
      "\n",
      "running train loss =   6526.443798828125\n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6906.926270[  100/ 1000]\n",
      "\n",
      "running train loss =   6500.2322998046875\n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6314.768555[  100/ 1000]\n",
      "\n",
      "running train loss =   6482.74775390625\n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8337.400391[  100/ 1000]\n",
      "\n",
      "running train loss =   6465.675756835937\n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5307.894531[  100/ 1000]\n",
      "\n",
      "running train loss =   6450.864306640625\n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10393.459961[  100/ 1000]\n",
      "\n",
      "running train loss =   6455.5140625\n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4309.044434[  100/ 1000]\n",
      "\n",
      "running train loss =   6420.183349609375\n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7214.628906[  100/ 1000]\n",
      "\n",
      "running train loss =   6405.24853515625\n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4985.054199[  100/ 1000]\n",
      "\n",
      "running train loss =   6390.312646484375\n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4462.620605[  100/ 1000]\n",
      "\n",
      "running train loss =   6368.075390625\n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4922.413574[  100/ 1000]\n",
      "\n",
      "running train loss =   6362.002099609375\n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5438.144531[  100/ 1000]\n",
      "\n",
      "running train loss =   6330.267529296875\n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4550.157227[  100/ 1000]\n",
      "\n",
      "running train loss =   6318.9931640625\n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5697.388672[  100/ 1000]\n",
      "\n",
      "running train loss =   6302.893115234375\n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4574.707031[  100/ 1000]\n",
      "\n",
      "running train loss =   6287.846435546875\n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3993.907471[  100/ 1000]\n",
      "\n",
      "running train loss =   6272.796362304687\n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 10139.060547[  100/ 1000]\n",
      "\n",
      "running train loss =   6253.2455078125\n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5069.174316[  100/ 1000]\n",
      "\n",
      "running train loss =   6236.805615234375\n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7936.027344[  100/ 1000]\n",
      "\n",
      "running train loss =   6222.355126953125\n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7083.752441[  100/ 1000]\n",
      "\n",
      "running train loss =   6204.763720703125\n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4800.033203[  100/ 1000]\n",
      "\n",
      "running train loss =   6189.589013671875\n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5119.370117[  100/ 1000]\n",
      "\n",
      "running train loss =   6183.163427734375\n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3505.837891[  100/ 1000]\n",
      "\n",
      "running train loss =   6157.53115234375\n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6090.294922[  100/ 1000]\n",
      "\n",
      "running train loss =   6147.562548828125\n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5716.302734[  100/ 1000]\n",
      "\n",
      "running train loss =   6132.851171875\n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6916.685059[  100/ 1000]\n",
      "\n",
      "running train loss =   6111.416796875\n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8849.804688[  100/ 1000]\n",
      "\n",
      "running train loss =   6095.83125\n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7514.041992[  100/ 1000]\n",
      "\n",
      "running train loss =   6082.224609375\n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7601.831055[  100/ 1000]\n",
      "\n",
      "running train loss =   6061.1\n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6529.974609[  100/ 1000]\n",
      "\n",
      "running train loss =   6047.948999023438\n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5174.524414[  100/ 1000]\n",
      "\n",
      "running train loss =   6036.289453125\n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5869.440430[  100/ 1000]\n",
      "\n",
      "running train loss =   6014.16640625\n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4653.486816[  100/ 1000]\n",
      "\n",
      "running train loss =   6005.642626953125\n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4029.244629[  100/ 1000]\n",
      "\n",
      "running train loss =   5986.743212890625\n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6096.685059[  100/ 1000]\n",
      "\n",
      "running train loss =   5976.2645263671875\n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4638.492676[  100/ 1000]\n",
      "\n",
      "running train loss =   5953.098291015625\n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5729.147461[  100/ 1000]\n",
      "\n",
      "running train loss =   5940.47333984375\n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4561.889160[  100/ 1000]\n",
      "\n",
      "running train loss =   5926.135205078125\n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4248.260742[  100/ 1000]\n",
      "\n",
      "running train loss =   5914.479833984375\n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6315.914551[  100/ 1000]\n",
      "\n",
      "running train loss =   5897.355688476562\n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3896.041016[  100/ 1000]\n",
      "\n",
      "running train loss =   5887.132470703125\n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3911.350586[  100/ 1000]\n",
      "\n",
      "running train loss =   5870.64013671875\n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7949.405762[  100/ 1000]\n",
      "\n",
      "running train loss =   5852.6544921875\n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5929.755859[  100/ 1000]\n",
      "\n",
      "running train loss =   5834.71865234375\n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5575.549805[  100/ 1000]\n",
      "\n",
      "running train loss =   5820.670068359375\n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7959.846680[  100/ 1000]\n",
      "\n",
      "running train loss =   5807.168310546875\n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5456.080078[  100/ 1000]\n",
      "\n",
      "running train loss =   5794.6121826171875\n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4327.662598[  100/ 1000]\n",
      "\n",
      "running train loss =   5779.460034179688\n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5573.304199[  100/ 1000]\n",
      "\n",
      "running train loss =   5771.0490234375\n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4106.337891[  100/ 1000]\n",
      "\n",
      "running train loss =   5747.63017578125\n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4883.641113[  100/ 1000]\n",
      "\n",
      "running train loss =   5743.084936523437\n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5972.818848[  100/ 1000]\n",
      "\n",
      "running train loss =   5712.956494140625\n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4211.259766[  100/ 1000]\n",
      "\n",
      "running train loss =   5709.56572265625\n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7550.152344[  100/ 1000]\n",
      "\n",
      "running train loss =   5690.540673828125\n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8320.963867[  100/ 1000]\n",
      "\n",
      "running train loss =   5674.252026367188\n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5979.324219[  100/ 1000]\n",
      "\n",
      "running train loss =   5663.294702148438\n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6066.564941[  100/ 1000]\n",
      "\n",
      "running train loss =   5646.85126953125\n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5342.393555[  100/ 1000]\n",
      "\n",
      "running train loss =   5635.13095703125\n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5547.494141[  100/ 1000]\n",
      "\n",
      "running train loss =   5624.1029296875\n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4230.829590[  100/ 1000]\n",
      "\n",
      "running train loss =   5609.578344726562\n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5049.422852[  100/ 1000]\n",
      "\n",
      "running train loss =   5592.655224609375\n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8747.855469[  100/ 1000]\n",
      "\n",
      "running train loss =   5584.448559570313\n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4124.983887[  100/ 1000]\n",
      "\n",
      "running train loss =   5564.22236328125\n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5969.083984[  100/ 1000]\n",
      "\n",
      "running train loss =   5546.20908203125\n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5654.654297[  100/ 1000]\n",
      "\n",
      "running train loss =   5537.7779296875\n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8071.581055[  100/ 1000]\n",
      "\n",
      "running train loss =   5522.215405273438\n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5375.916016[  100/ 1000]\n",
      "\n",
      "running train loss =   5507.50634765625\n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5473.865234[  100/ 1000]\n",
      "\n",
      "running train loss =   5495.187231445312\n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6614.691406[  100/ 1000]\n",
      "\n",
      "running train loss =   5480.071875\n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5597.235840[  100/ 1000]\n",
      "\n",
      "running train loss =   5463.479711914062\n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4234.762207[  100/ 1000]\n",
      "\n",
      "running train loss =   5451.895678710937\n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5619.084961[  100/ 1000]\n",
      "\n",
      "running train loss =   5446.354077148438\n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4176.360840[  100/ 1000]\n",
      "\n",
      "running train loss =   5421.76591796875\n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4526.649414[  100/ 1000]\n",
      "\n",
      "running train loss =   5413.867163085937\n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7718.151367[  100/ 1000]\n",
      "\n",
      "running train loss =   5400.3228515625\n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6502.164551[  100/ 1000]\n",
      "\n",
      "running train loss =   5384.042456054687\n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4884.681152[  100/ 1000]\n",
      "\n",
      "running train loss =   5373.29140625\n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3590.850098[  100/ 1000]\n",
      "\n",
      "running train loss =   5367.8812255859375\n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3696.738770[  100/ 1000]\n",
      "\n",
      "running train loss =   5342.767578125\n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4629.462891[  100/ 1000]\n",
      "\n",
      "running train loss =   5334.200952148438\n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7531.897949[  100/ 1000]\n",
      "\n",
      "running train loss =   5318.060522460937\n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5547.515137[  100/ 1000]\n",
      "\n",
      "running train loss =   5306.712646484375\n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5076.698730[  100/ 1000]\n",
      "\n",
      "running train loss =   5291.916821289063\n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3985.215576[  100/ 1000]\n",
      "\n",
      "running train loss =   5280.548779296875\n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4200.851562[  100/ 1000]\n",
      "\n",
      "running train loss =   5268.651391601563\n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5244.790039[  100/ 1000]\n",
      "\n",
      "running train loss =   5273.4486083984375\n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4223.249512[  100/ 1000]\n",
      "\n",
      "running train loss =   5237.643383789063\n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4384.065918[  100/ 1000]\n",
      "\n",
      "running train loss =   5231.641650390625\n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4264.539062[  100/ 1000]\n",
      "\n",
      "running train loss =   5214.321484375\n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3728.658447[  100/ 1000]\n",
      "\n",
      "running train loss =   5202.128466796875\n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4395.368164[  100/ 1000]\n",
      "\n",
      "running train loss =   5191.004663085938\n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4359.708496[  100/ 1000]\n",
      "\n",
      "running train loss =   5179.522216796875\n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4734.790527[  100/ 1000]\n",
      "\n",
      "running train loss =   5165.46611328125\n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4398.267578[  100/ 1000]\n",
      "\n",
      "running train loss =   5150.651953125\n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4740.458008[  100/ 1000]\n",
      "\n",
      "running train loss =   5140.606713867188\n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5483.612305[  100/ 1000]\n",
      "\n",
      "running train loss =   5127.51640625\n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2969.637695[  100/ 1000]\n",
      "\n",
      "running train loss =   5112.998510742187\n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3994.501953[  100/ 1000]\n",
      "\n",
      "running train loss =   5103.687329101563\n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4496.427734[  100/ 1000]\n",
      "\n",
      "running train loss =   5089.466015625\n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6076.768555[  100/ 1000]\n",
      "\n",
      "running train loss =   5076.9774169921875\n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5121.381836[  100/ 1000]\n",
      "\n",
      "running train loss =   5068.2943359375\n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3951.041260[  100/ 1000]\n",
      "\n",
      "running train loss =   5051.566845703125\n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4154.665039[  100/ 1000]\n",
      "\n",
      "running train loss =   5040.029052734375\n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5024.549316[  100/ 1000]\n",
      "\n",
      "running train loss =   5028.51279296875\n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4916.347168[  100/ 1000]\n",
      "\n",
      "running train loss =   5027.0987548828125\n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6369.504883[  100/ 1000]\n",
      "\n",
      "running train loss =   5011.436669921875\n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4272.022461[  100/ 1000]\n",
      "\n",
      "running train loss =   5007.135595703125\n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2873.024414[  100/ 1000]\n",
      "\n",
      "running train loss =   4979.5404052734375\n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4267.007324[  100/ 1000]\n",
      "\n",
      "running train loss =   4968.334326171875\n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5358.454590[  100/ 1000]\n",
      "\n",
      "running train loss =   4953.428881835937\n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4476.248535[  100/ 1000]\n",
      "\n",
      "running train loss =   4946.545166015625\n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4687.804199[  100/ 1000]\n",
      "\n",
      "running train loss =   4942.712744140625\n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5444.035156[  100/ 1000]\n",
      "\n",
      "running train loss =   4919.500219726562\n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4319.764160[  100/ 1000]\n",
      "\n",
      "running train loss =   4910.2322998046875\n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4541.860840[  100/ 1000]\n",
      "\n",
      "running train loss =   4910.267749023437\n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5879.704590[  100/ 1000]\n",
      "\n",
      "running train loss =   4904.832861328125\n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5314.367676[  100/ 1000]\n",
      "\n",
      "running train loss =   4876.322338867188\n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5877.404785[  100/ 1000]\n",
      "\n",
      "running train loss =   4864.058349609375\n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3754.983643[  100/ 1000]\n",
      "\n",
      "running train loss =   4853.6177978515625\n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4156.402344[  100/ 1000]\n",
      "\n",
      "running train loss =   4836.88623046875\n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4552.760742[  100/ 1000]\n",
      "\n",
      "running train loss =   4831.317211914063\n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5077.841797[  100/ 1000]\n",
      "\n",
      "running train loss =   4815.37509765625\n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4636.845703[  100/ 1000]\n",
      "\n",
      "running train loss =   4802.463354492187\n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4517.856934[  100/ 1000]\n",
      "\n",
      "running train loss =   4796.30234375\n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5084.620605[  100/ 1000]\n",
      "\n",
      "running train loss =   4780.204760742187\n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5749.758789[  100/ 1000]\n",
      "\n",
      "running train loss =   4771.215478515625\n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3590.014893[  100/ 1000]\n",
      "\n",
      "running train loss =   4758.1833984375\n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3699.827881[  100/ 1000]\n",
      "\n",
      "running train loss =   4746.726000976562\n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3968.219482[  100/ 1000]\n",
      "\n",
      "running train loss =   4735.8111572265625\n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3407.347412[  100/ 1000]\n",
      "\n",
      "running train loss =   4724.937280273438\n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5957.643066[  100/ 1000]\n",
      "\n",
      "running train loss =   4716.480786132813\n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 8229.348633[  100/ 1000]\n",
      "\n",
      "running train loss =   4703.927294921875\n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3802.916992[  100/ 1000]\n",
      "\n",
      "running train loss =   4695.903637695313\n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3968.345703[  100/ 1000]\n",
      "\n",
      "running train loss =   4684.127197265625\n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4140.056152[  100/ 1000]\n",
      "\n",
      "running train loss =   4668.06865234375\n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4525.157227[  100/ 1000]\n",
      "\n",
      "running train loss =   4658.180200195313\n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6613.031250[  100/ 1000]\n",
      "\n",
      "running train loss =   4649.176318359375\n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3706.274902[  100/ 1000]\n",
      "\n",
      "running train loss =   4637.4044921875\n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6091.398926[  100/ 1000]\n",
      "\n",
      "running train loss =   4633.742724609375\n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3710.921631[  100/ 1000]\n",
      "\n",
      "running train loss =   4616.841552734375\n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5220.803711[  100/ 1000]\n",
      "\n",
      "running train loss =   4612.446850585938\n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3557.835938[  100/ 1000]\n",
      "\n",
      "running train loss =   4597.761987304688\n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4592.383301[  100/ 1000]\n",
      "\n",
      "running train loss =   4584.615307617188\n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5472.271484[  100/ 1000]\n",
      "\n",
      "running train loss =   4570.646557617188\n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3267.538086[  100/ 1000]\n",
      "\n",
      "running train loss =   4566.706201171875\n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3714.128174[  100/ 1000]\n",
      "\n",
      "running train loss =   4550.625561523438\n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4854.694824[  100/ 1000]\n",
      "\n",
      "running train loss =   4546.445385742188\n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4408.389160[  100/ 1000]\n",
      "\n",
      "running train loss =   4532.029077148438\n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3147.819092[  100/ 1000]\n",
      "\n",
      "running train loss =   4521.255712890625\n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3504.954590[  100/ 1000]\n",
      "\n",
      "running train loss =   4517.0984375\n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4825.214355[  100/ 1000]\n",
      "\n",
      "running train loss =   4503.959375\n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5604.601074[  100/ 1000]\n",
      "\n",
      "running train loss =   4490.3404296875\n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6777.778320[  100/ 1000]\n",
      "\n",
      "running train loss =   4480.150708007813\n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4724.090820[  100/ 1000]\n",
      "\n",
      "running train loss =   4506.213940429688\n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4852.809570[  100/ 1000]\n",
      "\n",
      "running train loss =   4468.405688476562\n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2724.082764[  100/ 1000]\n",
      "\n",
      "running train loss =   4457.859057617187\n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3612.497559[  100/ 1000]\n",
      "\n",
      "running train loss =   4442.357861328125\n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5845.709961[  100/ 1000]\n",
      "\n",
      "running train loss =   4432.562060546875\n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3833.035889[  100/ 1000]\n",
      "\n",
      "running train loss =   4419.243505859375\n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5264.805664[  100/ 1000]\n",
      "\n",
      "running train loss =   4411.42119140625\n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3026.330078[  100/ 1000]\n",
      "\n",
      "running train loss =   4398.652758789062\n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4840.817383[  100/ 1000]\n",
      "\n",
      "running train loss =   4387.5505859375\n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4654.272461[  100/ 1000]\n",
      "\n",
      "running train loss =   4379.057446289063\n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6035.915527[  100/ 1000]\n",
      "\n",
      "running train loss =   4369.198071289063\n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4430.902344[  100/ 1000]\n",
      "\n",
      "running train loss =   4363.254150390625\n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4666.077637[  100/ 1000]\n",
      "\n",
      "running train loss =   4353.391137695313\n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4297.988770[  100/ 1000]\n",
      "\n",
      "running train loss =   4345.9573974609375\n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5388.673340[  100/ 1000]\n",
      "\n",
      "running train loss =   4328.50263671875\n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3880.577393[  100/ 1000]\n",
      "\n",
      "running train loss =   4319.213305664062\n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6301.884277[  100/ 1000]\n",
      "\n",
      "running train loss =   4310.955981445312\n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3997.348633[  100/ 1000]\n",
      "\n",
      "running train loss =   4298.88720703125\n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5395.196289[  100/ 1000]\n",
      "\n",
      "running train loss =   4290.383227539062\n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5035.724609[  100/ 1000]\n",
      "\n",
      "running train loss =   4282.4422607421875\n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 7700.142578[  100/ 1000]\n",
      "\n",
      "running train loss =   4271.310522460937\n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3039.861816[  100/ 1000]\n",
      "\n",
      "running train loss =   4263.161962890625\n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3451.312744[  100/ 1000]\n",
      "\n",
      "running train loss =   4256.145751953125\n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3097.156494[  100/ 1000]\n",
      "\n",
      "running train loss =   4242.367431640625\n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3185.426025[  100/ 1000]\n",
      "\n",
      "running train loss =   4233.1036376953125\n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4846.947266[  100/ 1000]\n",
      "\n",
      "running train loss =   4225.686059570313\n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2776.787598[  100/ 1000]\n",
      "\n",
      "running train loss =   4213.648193359375\n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3538.202881[  100/ 1000]\n",
      "\n",
      "running train loss =   4210.312084960938\n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5839.842285[  100/ 1000]\n",
      "\n",
      "running train loss =   4201.033666992187\n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3530.410645[  100/ 1000]\n",
      "\n",
      "running train loss =   4188.281274414063\n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4477.981934[  100/ 1000]\n",
      "\n",
      "running train loss =   4181.196459960937\n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5748.908203[  100/ 1000]\n",
      "\n",
      "running train loss =   4178.664624023438\n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2672.393799[  100/ 1000]\n",
      "\n",
      "running train loss =   4159.934375\n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2871.957764[  100/ 1000]\n",
      "\n",
      "running train loss =   4152.99443359375\n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2932.818848[  100/ 1000]\n",
      "\n",
      "running train loss =   4151.0833740234375\n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3220.606201[  100/ 1000]\n",
      "\n",
      "running train loss =   4132.920263671875\n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5028.981445[  100/ 1000]\n",
      "\n",
      "running train loss =   4129.113427734375\n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2997.812256[  100/ 1000]\n",
      "\n",
      "running train loss =   4115.3896484375\n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3795.407715[  100/ 1000]\n",
      "\n",
      "running train loss =   4106.781298828125\n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4237.549316[  100/ 1000]\n",
      "\n",
      "running train loss =   4097.376538085938\n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6511.767090[  100/ 1000]\n",
      "\n",
      "running train loss =   4089.977685546875\n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3349.198975[  100/ 1000]\n",
      "\n",
      "running train loss =   4078.2653076171873\n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5426.312500[  100/ 1000]\n",
      "\n",
      "running train loss =   4069.3906982421877\n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3475.048828[  100/ 1000]\n",
      "\n",
      "running train loss =   4060.786181640625\n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4458.920410[  100/ 1000]\n",
      "\n",
      "running train loss =   4052.6947509765623\n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4874.130371[  100/ 1000]\n",
      "\n",
      "running train loss =   4045.2613037109377\n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2667.462891[  100/ 1000]\n",
      "\n",
      "running train loss =   4034.542041015625\n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5490.355469[  100/ 1000]\n",
      "\n",
      "running train loss =   4026.396337890625\n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3758.464355[  100/ 1000]\n",
      "\n",
      "running train loss =   4022.07490234375\n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5996.129883[  100/ 1000]\n",
      "\n",
      "running train loss =   4011.085400390625\n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2600.465332[  100/ 1000]\n",
      "\n",
      "running train loss =   4005.1351318359375\n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2874.131348[  100/ 1000]\n",
      "\n",
      "running train loss =   3995.0156494140624\n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4896.779785[  100/ 1000]\n",
      "\n",
      "running train loss =   3982.421923828125\n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5015.041504[  100/ 1000]\n",
      "\n",
      "running train loss =   3976.906494140625\n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3418.962402[  100/ 1000]\n",
      "\n",
      "running train loss =   3967.0890625\n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4885.549316[  100/ 1000]\n",
      "\n",
      "running train loss =   3961.767919921875\n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4484.556152[  100/ 1000]\n",
      "\n",
      "running train loss =   3950.1712646484375\n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2286.782715[  100/ 1000]\n",
      "\n",
      "running train loss =   3943.73564453125\n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3305.504150[  100/ 1000]\n",
      "\n",
      "running train loss =   3935.869970703125\n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4189.044922[  100/ 1000]\n",
      "\n",
      "running train loss =   3926.66181640625\n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2817.742432[  100/ 1000]\n",
      "\n",
      "running train loss =   3920.483447265625\n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4295.841797[  100/ 1000]\n",
      "\n",
      "running train loss =   3920.7906494140625\n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3423.416992[  100/ 1000]\n",
      "\n",
      "running train loss =   3902.8356689453126\n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2641.368652[  100/ 1000]\n",
      "\n",
      "running train loss =   3893.76708984375\n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4831.728516[  100/ 1000]\n",
      "\n",
      "running train loss =   3888.92568359375\n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2629.864258[  100/ 1000]\n",
      "\n",
      "running train loss =   3883.5066650390627\n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3742.622559[  100/ 1000]\n",
      "\n",
      "running train loss =   3869.106689453125\n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4277.996582[  100/ 1000]\n",
      "\n",
      "running train loss =   3862.239892578125\n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2667.854492[  100/ 1000]\n",
      "\n",
      "running train loss =   3854.6961181640627\n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3128.428955[  100/ 1000]\n",
      "\n",
      "running train loss =   3846.0515380859374\n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5834.081055[  100/ 1000]\n",
      "\n",
      "running train loss =   3849.039892578125\n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3622.658447[  100/ 1000]\n",
      "\n",
      "running train loss =   3837.2763427734376\n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4868.817383[  100/ 1000]\n",
      "\n",
      "running train loss =   3822.5345703125\n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4533.930176[  100/ 1000]\n",
      "\n",
      "running train loss =   3819.075573730469\n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3119.691162[  100/ 1000]\n",
      "\n",
      "running train loss =   3804.59150390625\n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5530.251465[  100/ 1000]\n",
      "\n",
      "running train loss =   3806.2624755859374\n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3373.306885[  100/ 1000]\n",
      "\n",
      "running train loss =   3791.23203125\n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4368.952148[  100/ 1000]\n",
      "\n",
      "running train loss =   3788.9237060546875\n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4323.734375[  100/ 1000]\n",
      "\n",
      "running train loss =   3789.9392578125\n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3357.994385[  100/ 1000]\n",
      "\n",
      "running train loss =   3778.336279296875\n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2812.593750[  100/ 1000]\n",
      "\n",
      "running train loss =   3770.3266357421876\n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4363.291016[  100/ 1000]\n",
      "\n",
      "running train loss =   3753.6459106445313\n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2644.777100[  100/ 1000]\n",
      "\n",
      "running train loss =   3751.5939208984373\n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4852.505859[  100/ 1000]\n",
      "\n",
      "running train loss =   3739.3306396484377\n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2601.927246[  100/ 1000]\n",
      "\n",
      "running train loss =   3735.3008544921877\n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3874.821289[  100/ 1000]\n",
      "\n",
      "running train loss =   3723.9124755859375\n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4418.627441[  100/ 1000]\n",
      "\n",
      "running train loss =   3717.9868896484377\n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 6147.011230[  100/ 1000]\n",
      "\n",
      "running train loss =   3706.219580078125\n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2639.235352[  100/ 1000]\n",
      "\n",
      "running train loss =   3702.6531005859374\n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3638.875244[  100/ 1000]\n",
      "\n",
      "running train loss =   3695.8349609375\n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3355.566650[  100/ 1000]\n",
      "\n",
      "running train loss =   3685.5124267578126\n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3274.983643[  100/ 1000]\n",
      "\n",
      "running train loss =   3683.0672119140627\n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2415.824951[  100/ 1000]\n",
      "\n",
      "running train loss =   3670.6868896484375\n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2405.703857[  100/ 1000]\n",
      "\n",
      "running train loss =   3662.869775390625\n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2753.302734[  100/ 1000]\n",
      "\n",
      "running train loss =   3656.4623779296876\n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 4710.583008[  100/ 1000]\n",
      "\n",
      "running train loss =   3650.543408203125\n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2705.979980[  100/ 1000]\n",
      "\n",
      "running train loss =   3644.10771484375\n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2943.194092[  100/ 1000]\n",
      "\n",
      "running train loss =   3634.82275390625\n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 5143.395508[  100/ 1000]\n",
      "\n",
      "running train loss =   3629.6154541015626\n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3169.372559[  100/ 1000]\n",
      "\n",
      "running train loss =   3624.770751953125\n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2588.445312[  100/ 1000]\n",
      "\n",
      "running train loss =   3617.0263671875\n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2632.081543[  100/ 1000]\n",
      "\n",
      "running train loss =   3607.118017578125\n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 2693.209473[  100/ 1000]\n",
      "\n",
      "running train loss =   3601.093505859375\n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Processing batch n. 1 ----> running loss: 3270.614990[  100/ 1000]\n",
      "\n",
      "running train loss =   3592.7448974609374\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_values = [100,200,500,1000]\n",
    "\n",
    "mse = []\n",
    "\n",
    "for n in n_values:\n",
    "\n",
    "    #clear and initiate new datasets, with specific number of training examples:\n",
    "    train_dataset_new = 0\n",
    "    train_dataloader_new = 0  \n",
    "    #train_dataset_new = torch.utils.data.TensorDataset(dataset[\"X_train\"][:n], N_y_train[:n])  \n",
    "    train_dataset_new = torch.utils.data.TensorDataset(dataset[\"X_train\"][:n], dataset[\"y_train\"][:n])  \n",
    "    train_dataloader_new = torch.utils.data.DataLoader(train_dataset_new, batch_size= 100, shuffle= True)\n",
    "    res = 0\n",
    "    \n",
    "\n",
    "    #Define just one learning rate\n",
    "\n",
    "    # define the neural network model\n",
    "    d = dataset[\"X_train\"].shape[1]\n",
    "    p = 1000 \n",
    "    \n",
    "    model = NeuralNetwork(d,p)\n",
    "    \n",
    "    \n",
    "    # define the hyper-parameters of the network training\n",
    "    epochs = 1000\n",
    "    \n",
    "    learning_rate = 10e-5\n",
    "    \n",
    "    # train the network\n",
    "    \n",
    "    train_loop(train_dataloader_new, model, epochs = epochs, learning_rate = learning_rate,datatype = \"train\")\n",
    "\n",
    "    #compute mean squared error of the test set\n",
    "\n",
    "    res = pd.read_csv(\"./metrics_over_epochs.csv\")\n",
    "\n",
    "    mse_aux = np.mean(res[\"train_loss\"])\n",
    "\n",
    "    mse.append(mse_aux)\n",
    "        \n",
    "#plot\n",
    "\n",
    "\n",
    "plt.loglog(n_values,mse)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('$\\epsilon_T$')\n",
    "plt.title(' unnormalized mean squared error on the test set for n ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ε_T = 92785015.35705 *n^-0.90487\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Curve Fitting\n",
    "\n",
    "from pandas import read_csv\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# define the true objective function\n",
    "def objective(x, a, b):\n",
    " return a*(x** (-b))\n",
    " \n",
    "# curve fit\n",
    "popt, _ = curve_fit(objective, n_values, mse)\n",
    "# summarize the parameter values\n",
    "a, b = popt\n",
    "print(' ε_T = %.5f *n^%.5f' % (a, -b))\n",
    "# plot input vs output\n",
    "pyplot.scatter(n_values, mse,label = \"data points\")\n",
    "# define a sequence of inputs between the smallest and largest known inputs\n",
    "x_line = np.arange(min(n_values), max(n_values), 1)\n",
    "# calculate the output for the range\n",
    "y_line = objective(x_line, a, b)\n",
    "# create a line plot for the mapping function\n",
    "pyplot.plot(x_line, y_line, '--', color='red',label = \"fit\")\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('$\\epsilon_T$')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The test error that this model would achieve with n = 4000 training points is 168605941279.45822\n"
     ]
    }
   ],
   "source": [
    "#compute εT (n = 4000)\n",
    "\n",
    "epsilon_T = a*(4000**b)\n",
    "\n",
    "print(f\" The test error that this model would achieve with n = 4000 training points is {epsilon_T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ml4phys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86a5618298748035e24d3cf1c1a18481c6efa11bcb52205e6a42241e54161982"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
